[{"text": "Agada Joseph Oche \u2217 $^{1}$, Ademola Glory Folashade \u2020 $^{1}$, Tirthankar Ghosal$^{2}$, and Arpan Biswas3", "metadata": {"uuid": "1738d2e4-1a01-4004-925a-9b9361ec0da4", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 0, "section_path": ["A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems:"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 41}}, {"text": "$^{1}$Bredesen Center for Interdisciplinary Research, University of Tennessee, Knoxville, USA, 37996 $^{2}$National Center for Computational Sciences, Oak Ridge National Laboratory, Oak Ridge, USA, 37830", "metadata": {"uuid": "dfec9d10-686d-48e2-8fc3-12be11a7ccf7", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 1, "section_path": ["A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems:"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 48}}, {"text": "$^{3}$University of Tennessee-Oak Ridge Innovation Institute, University of Tennessee, Knoxville, USA, 37996", "metadata": {"uuid": "bb0ef019-a2a7-490f-8bfc-68df3bf7286b", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 2, "section_path": ["A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems:"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 24}}, {"text": "July 28, 2025", "metadata": {"uuid": "654d0ce5-367a-4501-a3b0-028a29532cf5", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 3, "section_path": ["A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems:"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 7}}, {"text": "Retrieval-Augmented Generation (RAG) represents a major advancement in natural language processing (NLP), combining large language models (LLMs) with information retrieval systems to enhance factual grounding, accuracy, and contextual relevance. This paper presents a comprehensive systematic review of RAG, tracing its evolution from early developments in open-domain question answering to recent state-of-the-art implementations across diverse applications. The review begins by outlining the motivations behind RAG, particularly its ability to mitigate hallucinations and outdated knowledge in parametric models. Core technical components-retrieval mechanisms, sequence-to-sequence generation models, and fusion strategies-are examined in detail. A year-by-year analysis highlights key milestones and research trends, providing insight into RAG's rapid growth. The paper further explores the deployment of RAG in enterprise systems, addressing practical challenges related to retrieval of proprietary data, security, and scalability. A comparative evaluation of RAG implementations is conducted, benchmarking performance on retrieval accuracy, generation fluency, latency, and computational efficiency. Persistent challenges such as retrieval quality, privacy concerns, and integration overhead are critically assessed. Finally, the review highlights emerging solutions, including hybrid retrieval approaches, privacy-preserving techniques, optimized fusion strategies, and agent RAG architectures. These innovations point toward a future of more reliable, efficient, and context-aware knowledge-intensive NLP systems.", "metadata": {"uuid": "61e49117-90e7-468d-837e-af9e3a77e284", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 4, "section_path": ["A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems:"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 267}}, {"text": "Keywords: Retrieval Augmented Generation (RAG), Large Language Model (LLM), Generative AI, Natural Language Model (NLP)", "metadata": {"uuid": "59342ae2-a5f3-4451-8556-fc6f75ea4872", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 5, "section_path": ["A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems:"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 28}}, {"text": "$^{\u2217}$Corresponding author: joe88data1@gmail.com", "metadata": {"uuid": "0508f7e9-c5b8-411e-8a21-a4a7e170bf31", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 6, "section_path": ["A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems:"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 15}}, {"text": "$^{\u2020}$gloryademola112@gmail.com", "metadata": {"uuid": "d0e839a0-ff94-4abf-82f6-1c860269fae4", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 7, "section_path": ["A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems:"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 11}}, {"text": "Since its formal introduction in the seminal work of [52] in 2020, Retrieval-Augmented Generation (RAG) has witnessed rapid advancements, marked by a significant surge in research interest and scholarly publications. This paper offers a unique and comprehensive review of the key developments and contributions in the field to date. The remainder of this introduction outlines the background and motivation for this review, defines its scope and objectives, and provides an overview of the paper's organization.", "metadata": {"uuid": "ef267312-b0d4-495b-bc18-754143680f14", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 8, "section_path": ["1 Introduction"], "section_refs": ["#/texts/13"], "page_no": 2, "tokens": 92}}, {"text": "Large-scale pre-trained language models have demonstrated an ability to store vast amounts of factual knowledge in their parameters, but they struggle with accessing up-to-date information and providing verifiable sources. This limitation has motivated techniques that augment generative models with information retrieval. Retrieval-Augmented Generation (RAG) emerged as a solution to this problem, combining a neural retriever with a sequence-to-sequence generator to ground outputs in external documents [52]. The seminal work of [52] introduced RAG for knowledge-intensive tasks, showing that a generative model (built on a BART encoder-decoder) could retrieve relevant Wikipedia passages and incorporate them into its responses, thereby achieving state-of-the-art performance on open-domain question answering. RAG is built upon prior efforts in which retrieval was used to enhance question answering and language modeling [48, 26, 45]. Unlike earlier extractive approaches, RAG produces free-form answers while still leveraging non-parametric memory, offering the best of both worlds: improved factual accuracy and the ability to cite sources. This capability is especially important to mitigate hallucinations (i.e., believable but incorrect outputs) and to allow knowledge updates without retraining the model [52, 33].", "metadata": {"uuid": "bc44f715-f2f3-49d9-aed5-cc0d0466313a", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 9, "section_path": ["1 Introduction", "1.1 Background and Motivation"], "section_refs": ["#/texts/13", "#/texts/15"], "page_no": 2, "tokens": 242}}, {"text": "Since its introduction, RAG has gained significant attention in both research and industry. A growing body of literature has extended RAG with improved retrievers and generators, and the approach has been applied to a wide range of domains. By 2023, the RAG paradigm underpinned hundreds of research publications and numerous commercial systems [21, 60]. In academia, researchers have scaled up retrieval-augmented models and refined their architectures-examples include leveraging", "metadata": {"uuid": "f79ca739-8764-4940-9997-b1d91e75d39d", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 10, "section_path": ["1 Introduction", "1.1 Background and Motivation"], "section_refs": ["#/texts/13", "#/texts/15"], "page_no": 2, "tokens": 91}}, {"text": "larger pre-trained models with retrieval in the loop [e.g., 35, 10]. In parallel, industry adoption of RAG has been swift: leading tech companies have integrated retrieval-augmented generators into search engines, virtual assistants, and enterprise question-answering applications [33, 60]. RAG now powers applications from open-domain QA and customer support chatbots to tools that automatically generate answers with supporting evidence. This broad adoption underscores the significance of RAG as a foundation for making generative AI more reliable and knowledge-aware. This paper provides a unique perspective on to review of literature in RAG by providing detailed yearly research progress in RAG, developing new perspectives, and evaluating trends.", "metadata": {"uuid": "c8e80379-32fd-464f-b141-6dacf5cbacb1", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 11, "section_path": ["1 Introduction", "1.1 Background and Motivation"], "section_refs": ["#/texts/13", "#/texts/15"], "page_no": 2, "tokens": 140}}, {"text": "The objective of this systematic review is to provide a comprehensive overview of the development of RAG and its expanding role in information access. We aim to answer several key research questions: (1) How has RAG been progressed every year since its inception, and what are the major technical milestones in its research and deployment? (2) What challenges and solutions have emerged for integrating RAG with proprietary or private data sources, and what gaps remain (e.g., in security and privacy)? (3) How has RAG been used in accelerating material discovery and characterization (4) How are RAG systems categorized and how does this categorization affect their performance?. By addressing these questions, the review seeks to chart the evolution of RAG, evaluate its current capabilities and limitations, and identify areas for future work.", "metadata": {"uuid": "bb43fc0e-e480-4ed5-862f-5f911e225dba", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 12, "section_path": ["1 Introduction", "1.2 Scope and Objectives"], "section_refs": ["#/texts/13", "#/texts/20"], "page_no": 2, "tokens": 160}}, {"text": "Over the past few years, progress in RAG has been marked by continuous innovation and new applications. We chronicle the advancement year-by-year, highlighting important academic contributions and industry developments that have shaped the field. Special attention is given to the integration of RAG with proprietary data-an area of growing interest as organizations apply RAG to internal knowledge bases. This involves examining techniques for efficient retrieval on private corpora and the handling of sensitive information, as well as open issues around data privacy [101]. Recent systems have also demonstrated that users can interact with an RAG-powered agent to obtain information directly from the web or a document corpus, rather than through traditional ranked search results [62, 81]. This paradigm blur s the line between search engine and dialogue agent, opening questions about usability,", "metadata": {"uuid": "60178eb4-b1f3-4abb-8f7a-3570b62b0983", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 13, "section_path": ["1 Introduction", "1.2 Scope and Objectives"], "section_refs": ["#/texts/13", "#/texts/20"], "page_no": 2, "tokens": 157}}, {"text": "accuracy, and trust in such interfaces. Overall, the review considers this and also consolidates knowledge on how RAG techniques have matured and what objectives remain for future research and development.", "metadata": {"uuid": "05202217-14a3-4a7d-8ae2-d959ddf0dca3", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 14, "section_path": ["1 Introduction", "1.2 Scope and Objectives"], "section_refs": ["#/texts/13", "#/texts/20"], "page_no": 3, "tokens": 36}}, {"text": "The remainder of this paper is structured as follows. Section 2 (Methodology) explains the review methodology, including the literature search strategy, inclusion/exclusion criteria, and approach to data synthesis. Section 3 (Foundations of RAG) provides a technical overview of retrieval-augmented generation, describing its core components (retrievers, indexes, generators) and the baseline architectures introduced by seminal works. Section 4 (Year-by-Year Progress) presents a chronological synthesis of RAG developments from 2017 onward, highlighting key research milestones. Section 5 (RAG for Proprietary data and Industry Implementation) examines enterprise implementation of RAG on proprietary data by key industry players. Section 6 (RAG Systems Evaluation) benchmarks different RAG implementations and variants, summarizing their performance across standard datasets and tasks. Section 7 (Challenges of RAG Systems) , Section 8 (Discussion and Future Direction) and Section 9 (Conclusion) finally provide current research gaps and potential future directions to expand the applications to various domain problems.", "metadata": {"uuid": "f84e8fa7-2f63-4ccd-af65-abf83aee07bb", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 15, "section_path": ["1 Introduction", "1.3 Paper Organization"], "section_refs": ["#/texts/13", "#/texts/24"], "page_no": 3, "tokens": 209}}, {"text": "This section details the systematic review methodology employed to survey RAG papers. It comprises three main steps: (1) designing a Search Strategy to capture a wide range of relevant works, (2) defining Inclusion and Exclusion Criteria to refine the initial corpus, and (3) implementing a Data Extraction and Synthesis process to analyze and consolidate findings.", "metadata": {"uuid": "1406e9fa-2e9b-4d03-adf9-3e2e1b8b109a", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 16, "section_path": ["2 Methodology"], "section_refs": ["#/texts/26"], "page_no": 3, "tokens": 69}}, {"text": "To ensure comprehensive coverage, we searched both academic and industry-focused literature on RAG. Multiple digital libraries were queried, including ACL Anthology, IEEE Xplore, ACM Digital Library, and Google Scholar. We included documents published from 2017 up to the end of mid 2025, covering early", "metadata": {"uuid": "f7474cbe-736d-4530-8204-49d1b1f573e1", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 17, "section_path": ["2 Methodology", "2.1 Search Strategy"], "section_refs": ["#/texts/26", "#/texts/28"], "page_no": 3, "tokens": 60}}, {"text": "\"retrieve-and-generate\" approaches and more recent RAG-specific techniques.", "metadata": {"uuid": "cb50eebe-f633-4819-bbd0-de84da3ebb51", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 18, "section_path": ["2 Methodology", "2.1 Search Strategy"], "section_refs": ["#/texts/26", "#/texts/28"], "page_no": 3, "tokens": 15}}, {"text": "Keywords and Databases. We used a set of pre-defined keywords, such as \" retrieval-augmented generation (RAG),\" \" dense retrieval ,\" \" hybrid retrieval LLM ,\" \" RAG proprietary data ,\" and \" LLM web search .\" These queries captured works ranging from open-domain QA to secure enterprise implementations. Each keyword search was executed on the above-listed databases, resulting in a pool of references that included journal articles, conference papers, technical reports, and white papers.", "metadata": {"uuid": "ca354b9e-62f7-456d-abb6-d2251a0c01fb", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 19, "section_path": ["2 Methodology", "2.1 Search Strategy"], "section_refs": ["#/texts/26", "#/texts/28"], "page_no": 3, "tokens": 95}}, {"text": "Initial Screening. A comprehensive list of potentially relevant works was formed by merging all search results and removing duplicates. Abstracts and titles were checked to confirm alignment with the RAG focus. If a work concentrated solely on retrieval or generation in isolation, without discussing how these components integrate, it was set aside for possible exclusion.", "metadata": {"uuid": "d3fd1783-d807-4d0d-821e-bdf080e0224f", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 20, "section_path": ["2 Methodology", "2.1 Search Strategy"], "section_refs": ["#/texts/26", "#/texts/28"], "page_no": 3, "tokens": 63}}, {"text": "We next applied a formal screening process to determine which references genuinely contributed insights into RAG. The criteria below guided our decisions:", "metadata": {"uuid": "9569cff7-4825-4217-adc7-963b3430feed", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 21, "section_path": ["2 Methodology", "2.2 Inclusion and Exclusion Criteria"], "section_refs": ["#/texts/26", "#/texts/34"], "page_no": 3, "tokens": 25}}, {"text": "Based on these criteria, the initial corpus was refined into a finalized set of documents deemed pertinent to the state-of-the-art in RAG.", "metadata": {"uuid": "3e2511e6-bcdb-4aa0-a065-8334e9e5f399", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 22, "section_path": ["2 Methodology", "2.2 Inclusion and Exclusion Criteria", "2.2.2 Exclusion Criteria"], "section_refs": ["#/texts/26", "#/texts/34", "#/texts/42"], "page_no": 4, "tokens": 28}}, {"text": "For each included publication, we collected key information, such as basic bibliographic details, the retrieval method (e.g., dense vs. sparse), the generator architecture (e.g., T5, BART, GPT), and the evaluated tasks or datasets. This allowed us to systematically compare different RAG implementations and their reported performance. We also looked for and extracted information on the challenges facing RAG implementation. The survey is not limited to peer-reviewed journal articles and conference proceedings, preprints, technical reports, and industry white papers were also reviewed. The review covers the application of RAG systems in all domains", "metadata": {"uuid": "9b8f2f22-25bc-45fb-a091-3bb16a4271aa", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 23, "section_path": ["2 Methodology", "2.3 Data Extraction"], "section_refs": ["#/texts/26", "#/texts/48"], "page_no": 4, "tokens": 121}}, {"text": "Synthesis Process. All extracted details/data were gathered in a central repository, allowing cross-study comparisons. We grouped research outputs by year of publication to track the chronological evolution of RAG, highlighting seminal breakthroughs and subsequent expansions. In line with systematic review principles, we combined both qualitative (themes, research directions) and quantitative (performance figures, latency measures) observations.", "metadata": {"uuid": "435fd2d5-c8f9-4c0a-b4ea-95b35bf29856", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 24, "section_path": ["2 Methodology", "2.3 Data Extraction"], "section_refs": ["#/texts/26", "#/texts/48"], "page_no": 4, "tokens": 72}}, {"text": "Ensuring Reliability. Disagreements during the review were resolved through discussion or by consulting a third reviewer. This final step ensured consistent application of the inclusion/exclusion", "metadata": {"uuid": "58d484b9-f57c-49ac-984d-8290acef7225", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 25, "section_path": ["2 Methodology", "2.3 Data Extraction"], "section_refs": ["#/texts/26", "#/texts/48"], "page_no": 4, "tokens": 32}}, {"text": "criteria and reliable data extraction. The data collected then served as the foundation for our analysis in subsequent sections, including discussions on year-by-year progress, enterprise applications,and proposed solutions.", "metadata": {"uuid": "62af945b-b4a0-4e98-b655-f160f522830b", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 26, "section_path": ["2 Methodology", "2.3 Data Extraction"], "section_refs": ["#/texts/26", "#/texts/48"], "page_no": 4, "tokens": 35}}, {"text": "Retrieval-Augmented Generation (RAG) : RAG is a framework that combines a neural text retrieval module with a text generation module to improve the quality of generated responses in knowledge-intensive tasks. Formally, a RAG model augments a sequence-to-sequence (seq2seq) generator with access to an external text corpus (non-parametric memory) via a retriever [52, 45]. Given an input query x , the retriever R selects a small subset of relevant documents Z = { z$_{1}$, z$_{2}$, . . . , z$_{K}$ } from a large corpus C (with K \u226a |C| ) [45]. The generator then conditions on both the query x and the retrieved documents Z to produce an output y (such as an answer or a descriptive text). Formally, the RAG model can be viewed as a latent variable generative model that defines a probability distribution over outputs y by marginalizing over the retrieved documents z$_{i}$ :", "metadata": {"uuid": "ca3e00eb-d488-490b-a7ff-9148540a9cad", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 27, "section_path": ["3 Foundations of RAG", "3.1 Definition and Key Concepts"], "section_refs": ["#/texts/54", "#/texts/55"], "page_no": 4, "tokens": 208}}, {"text": "P ( y \\, | \\, x ) \\, = \\, \\sum _ { i = 1 } ^ { K } P _ { \\text {ret} } ( z _ { i } \\, | \\, x ) \\, P _ { \\text {gen} } ( y \\, | \\, x , z _ { i } ) \\, , \\quad ( 1 )", "metadata": {"uuid": "61487578-a796-456a-9ea2-1864dc13c890", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 28, "section_path": ["3 Foundations of RAG", "3.1 Definition and Key Concepts"], "section_refs": ["#/texts/54", "#/texts/55"], "page_no": 4, "tokens": 85}}, {"text": "where P$_{ret}$ ( z$_{i}$ | x ) is the probability of retrieving document z$_{i}$ given query x (the retriever's output distribution), and P$_{gen}$ ( y | x, z$_{i}$ ) is the generator's conditional probability of producing y given x and a particular retrieved document z$_{i}$ . In practice, P$_{ret}$ ( z$_{i}$ | x ) is typically non-zero only for the topK retrieved items, providing a tractable approximation to the full sum over the corpus [52]. The retriever R itself can be defined as a function R ( x, C ) \u2192 Z that takes a query and returns a small subset Z of corpus C (with | Z | = K \u226a |C| ) likely to contain information relevant to x [45]. By design, RAG models maintain two kinds of memory : a parametric memory (the knowledge encoded in the generator's weights) and a non-parametric memory (the external text corpus accessed via retrieval) [52]. A standard RAG architecture is illustrated in Figure 1 below. A key distinction between RAG and pure large language model (LLM) generation is the use of this external non-parametric knowledge source at inference time. Traditional LLM-based generation relies solely on the model's internal parameters for knowledge, which can", "metadata": {"uuid": "a1aa9cf4-27c6-4644-8a5d-120e61388fc7", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 29, "section_path": ["3 Foundations of RAG", "3.1 Definition and Key Concepts"], "section_refs": ["#/texts/54", "#/texts/55"], "page_no": 4, "tokens": 281}}, {"text": "lead to hallucinations and factual inaccuracies when the model's training data does not adequately cover the query's topic [52]. In contrast, RAG explicitly grounds the generation of retrieved documents that serve as up-to-date evidence, enabling the model to generate content supported by those documents. This retrieval step means that RAG's outputs can be more accurate and factually correct compared to generation from a standalone LLM, especially for knowledge-intensive queries. Empirically, [52] demonstrates that a RAG model generates more specific and factual responses than a parametric-only", "metadata": {"uuid": "2e42851f-c9a8-4887-bc85-2ad5cd154ef9", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 30, "section_path": ["3 Foundations of RAG", "3.1 Definition and Key Concepts"], "section_refs": ["#/texts/54", "#/texts/55"], "page_no": 5, "tokens": 111}}, {"text": "generator, since the retrieved text provides verified information that the generator can incorporate. Another benefit is that the knowledge in a RAG system can be easily updated by modifying the document index (or corpus) without retraining the generator, addressing the stiffness of LLMs that have fixed knowledge up to their training cutoffdate. In summary, RAG introduces a modular architecture where a retrieval component supplies relevant context \"just in time\" for the generator, marrying the strengths of Information Retrieval (IR) with those of large-scale generation.", "metadata": {"uuid": "7c16e7fb-86ca-47c2-9b36-7479149f985c", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 31, "section_path": ["3 Foundations of RAG", "3.1 Definition and Key Concepts"], "section_refs": ["#/texts/54", "#/texts/55"], "page_no": 5, "tokens": 104}}, {"text": "Figure 1: Illustration of a RAG Architecture.", "metadata": {"uuid": "d8a3f0f2-8066-4456-9461-c5876e104e20", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 32, "section_path": ["3 Foundations of RAG", "3.1 Definition and Key Concepts"], "section_refs": ["#/texts/54", "#/texts/55"], "page_no": 5, "tokens": 11}}, {"text": "Chunking, Embedding, and (Re)ranking :A typical RAG pipeline consists of four stages: chunking, embedding, (re)ranking, and generation. First, chunking is applied to the knowledge source: large documents are segmented into smaller, self-contained pieces (e.g., paragraphs or passages) for indexing. Using fine-grained text chunks as retrieval units improves the chance that a query will surface a highly relevant fragment, rather than an entire lengthy document [52]. For example, open-domain QA systems split Wikipedia articles into passage chunks to enable pinpoint retrieval of answer-containing segments [45, 52]. The chunk size is typically tuned to balance context completeness and specificity chunks must be large enough to contain useful context, yet small enough to match queries narrowly and fit within model context windows. Next, each chunk is embedded into a high-dimensional vector", "metadata": {"uuid": "91510b75-59ce-417d-a072-f5a3769a32a8", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 33, "section_path": ["3 Foundations of RAG", "3.1 Definition and Key Concepts"], "section_refs": ["#/texts/54", "#/texts/55"], "page_no": 5, "tokens": 172}}, {"text": "representation that encodes its semantic content. This is usually done with a transformer-based bi-encoder that produces dense vector embeddings of text [45]. The embeddings serve as keys in a vector index (or vector database) that supports efficient nearest-neighbor search. At query time, the user's query is likewise embedded into the same vector space, and the system performs similarity search to retrieve the most relevant chunk vectors. In contrast to sparse keyword search, dense embeddings enable semantic matching: a query about \"financial earnings\" can retrieve a chunk about \"quarterly revenue\" even if exact words differ [45]. Modern RAG implementations often combine dense retrieval with lightweight filtering or hybrid search (e.g., BM25 + embeddings) to improve recall for difficult queries. The result of the initial retrieval stage is a candidate set of topk chunks that are potentially relevant to", "metadata": {"uuid": "8f755bbd-8baa-47f6-906b-697f7ef8f983", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 34, "section_path": ["3 Foundations of RAG", "3.1 Definition and Key Concepts"], "section_refs": ["#/texts/54", "#/texts/55"], "page_no": 5, "tokens": 169}}, {"text": "the query. To further improve precision, an optional re-ranking step is applied on the retrieved candidates before generation. The topk chunks from the first stage may contain some irrelevant or only tangentially related items, since embedding similarity is a coarse proxy for relevance. A re-ranker model (typically a cross-encoder transformer that jointly encodes query and document) evaluates each retrieved chunk in the context of the query and produces a refined relevance score [63]. By re-scoring and sorting the candidates, the re-ranker ensures that the most pertinent chunks (for example, those actually containing the answer to a question) are ranked highest. This two-stage retrieval process - a fast dense retriever followed by a more accurate but expensive re-ranker - has been shown to significantly boost retrieval performance on knowledge-intensive benchmarks. For instance, neural cross-attention re-rankers achieve substantially higher accuracy than single-stage retrievers alone [63]. In practice, re-ranking is crucial in high-stakes applications (e.g., legal or medical QA), where one must maximize the likelihood that the top context passages truly address the user's query. After re-ranking, the top N (e.g. 3-5) chunks are selected as the final context passages for the generative model. In the generation stage, the LLM produces an answer or response conditioned on the retrieved external chunks.", "metadata": {"uuid": "e2ae98b9-5800-4efd-8701-a9c4020fa8c7", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 35, "section_path": ["3 Foundations of RAG", "3.1 Definition and Key Concepts"], "section_refs": ["#/texts/54", "#/texts/55"], "page_no": 6, "tokens": 274}}, {"text": "Typically, a sequence-to-sequence model (such as T5 or BART) is used so that the retrieved text can be prepended or incorporated into the model's input along with the user query [52, 73]. During training, the model learns to copy or attend to the relevant facts from the retrieved documents and integrate them into a coherent output. This approach allows the generator to cite up-to-date, specific information beyond its parametric knowledge. For example, [52] show that a RAG model (BART-based) can accurately answer open-domain questions by retrieving and conditioning on Wikipedia text, dramatically reducing hallucinations compared to a standalone LLM. The generated output can also include references to source documents, providing traceability for the facts used. Retrieval augmentation thus serves as a \"live memory\" for the LLM: it supplies factual grounding from an external knowledge base while the language model creates fluent and contextually relevant text. Notably, recent large-scale studies have demonstrated that even very large models benefit from retrieval augmentation. For instance, the RETRO model augments a 7.5-billion-parameter transformer with a database of trillions of tokens, yielding improved perplexity and factual accuracy by looking up passages during", "metadata": {"uuid": "e2ae98b9-5800-4efd-8701-a9c4020fa8c7", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 36, "section_path": ["3 Foundations of RAG", "3.1 Definition and Key Concepts"], "section_refs": ["#/texts/54", "#/texts/55"], "page_no": 6, "tokens": 246}}, {"text": "generation [10]. In summary, chunking, embedding, re-ranking, and generation work in concert in RAG systems to leverage external knowledge - the retrieval components identify and prioritize relevant information, and the generation component uses that information to produce answers that are both informative and grounded in source data. This modular design has become a foundation for building more reliable and explainable AI assistants in knowledge-intensive domains.", "metadata": {"uuid": "1856c4d7-4347-4456-a2b6-584e7643d102", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 37, "section_path": ["3 Foundations of RAG", "3.1 Definition and Key Concepts"], "section_refs": ["#/texts/54", "#/texts/55"], "page_no": 6, "tokens": 79}}, {"text": "A RAG system is composed of two primary components - a retriever module and a generator module - along with a strategy for fusing their outputs. We break down these components and the underlying mechanics as follows.", "metadata": {"uuid": "b58fdd35-4bba-4bb3-bf5c-63501002b8e5", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 38, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 6, "tokens": 42}}, {"text": "The retriever's job is to efficiently identify which pieces of text in a large corpus are relevant to the input query x . Modern RAG implementations typically use dense retrievers like Dense Passage Retrieval (DPR) [45] in lieu of traditional keyword search. In DPR, a bi-encoder architecture is employed: a question encoder E$_{q}$ ( x ) maps the query x to a d -dimensional vector, and a passage encoder E$_{p}$ ( d ) maps each candidate document (or passage) d in the corpus to a d -dimensional vector in the same space. Relevance is assessed via a similarity score, usually the dot product of these vectors:", "metadata": {"uuid": "ece027c5-4d4c-481f-a5ee-19376b5b2edc", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 39, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 6, "tokens": 139}}, {"text": "\\sin ( x , d ) \\, = \\, E _ { q } ( x ) ^ { \\top } \\, E _ { p } ( d ) \\, .", "metadata": {"uuid": "830ed41a-0a1f-4508-947f-d7693bd70529", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 40, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 6, "tokens": 38}}, {"text": "At query time, the retriever computes v$_{q}$ = E$_{q}$ ( x ) and then finds the topK documents whose vector E$_{p}$ ( d ) has highest inner product with v$_{q}$ . This search for maximum inner product can be implemented efficiently using Approximate Nearest Neighbor techniques (e.g., FAISS) to handle millions of documents in sub-linear time. The output of the retriever is the set Z = { z$_{1}$, . . . , z$_{K}$ } of top-ranked documents and potentially their similarity scores. One can view the retriever as defining a probability distribution P$_{ret}$ ( z | x ) over documents z in the corpus, such that:", "metadata": {"uuid": "5e65056c-c2df-4ef3-b1d6-771fe1830d93", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 41, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 6, "tokens": 153}}, {"text": "P _ { \\text {ret} } ( z \\, | \\, x ) \\, \\infty \\, \\exp \\left ( E _ { q } ( x ) ^ { \\top } E _ { p } ( z ) \\right ) , \\quad ( 2 )", "metadata": {"uuid": "a61aafbe-0410-4be4-9df3-48eb6a51ffb1", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 42, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 6, "tokens": 61}}, {"text": "with the normalization \u2211 d \u2208C exp( E$_{q}$ ( x ) $^{\u22a4}$E$_{p}$ ( d )) (in practice approximated by summing over retrieved candidates rather than all of C ). In other words, the retriever assigns higher probability (or score)", "metadata": {"uuid": "2692580d-54c6-455a-85cb-8e2ca3f8c261", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 43, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 6, "tokens": 63}}, {"text": "to documents whose embedding is most similar to the query's embedding. The DPR retriever is usually first trained on pairs of questions and relevant passages to ensure E$_{q}$ and E$_{p}$ produce representations that maximize dot-products for true Q-A pairs and minimize them for irrelevant pairs. The training objective is often a contrastive loss : for a given question q with a gold relevant passage p + and a set of negative passages { p - 1 , . . . , p - N } , the encoder is trained to maximize sim( q, p $^{+}$) while minimizing sim( q, p $^{-}$) for negatives. This can be formulated as a cross-entropy loss treating the positive passage as the correct class among one positive and N negatives:", "metadata": {"uuid": "04e5faff-e543-4f57-ab98-3837bf9c8426", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 44, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 7, "tokens": 157}}, {"text": "\\mathcal { L } _ { \\text {ret} } ( q , p ^ { + } ) = - \\log \\frac { \\exp ( \\text {sim} ( q , p ^ { + } ) ) } { \\exp ( \\text {sim} ( q , p ^ { + } ) ) + \\sum _ { j = 1 } ^ { N } \\exp ( \\text {sim} ( q , p ^ { - } _ { j } ) ) }", "metadata": {"uuid": "0bd615ea-edac-4627-b366-0f889c985c66", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 45, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 7, "tokens": 105}}, {"text": "which encourages E$_{q}$ and E$_{p}$ to embed true pairs closer together than any negative pair [45]. After training, the retriever can generalize to new queries: it embeds the query and efficiently finds the nearest neighbor passages in the index. This retrieval step is crucial because it narrows down the evidence from potentially billions of tokens to a manageable subset that the generator will actually consider.", "metadata": {"uuid": "94c1c9e7-ab57-4efa-ba80-6a729209d3f0", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 46, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 7, "tokens": 82}}, {"text": "Generator Module (Conditional Seq2Seq Model). The generator in a RAG pipeline is typically a sequence-to-sequence language model that produces the final answer or output text, given the input query and retrieved documents. Formally, the generator defines a conditional distribution P$_{gen}$ ( y | x, Z ) over output sequences, where Z = { z$_{1}$, . . . , z$_{K}$ } are the retrieved passages. The generator is often initialized from a pre-trained transformer-based seq2seq model (such as BART or T5) to leverage rich language generation capabilities [52]. During generation, the model is provided with the question (or prompt) as well as the content of the retrieved documents. There are multiple ways to feed the retrieved context to the generator:", "metadata": {"uuid": "5fd0d9ee-5586-415f-a9e3-905cc7fcef8e", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 47, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 7, "tokens": 162}}, {"text": "Because the generator is conditioning on retrieved evidence, it tends to produce outputs that are supported by that evidence. For example, if the query asks for a specific factual answer, the generator can copy or rephrase the needed information from one of the retrieved passages. This is in contrast to a vanilla language model which would attempt to rely on parametric knowledge (which might be outdated or incomplete). In sum, the generator module in RAG is responsible for fluent and coherent text generation, but crucially it is grounded by the retrieval context, which guides it toward accurate content.", "metadata": {"uuid": "d3d548f7-3f0f-47e0-a2aa-13bd5296f7b7", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 48, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 7, "tokens": 113}}, {"text": "Fusion Mechanisms and Answer Aggregation. A critical aspect of RAG systems is how to fuse information from multiple retrieved documents z$_{1}$, . . . , z$_{K}$ when producing the final answer. Different fusion strategies have been explored: Marginalization (Probabilistic Fusion): As described, RAG treats the retrieved documents as latent variables and marginalizes over them [52]. This means the model doesn't commit to one retrieved source up front; instead, it considers each in turn and combines their contributions by summing probabilities. Concretely, if y is an output sequence, a RAG model might compute its probability by P ( y | x ) = \u2211 K i $_{=1}$P$_{ret}$ ( z$_{i}$ | x ) P$_{gen}$ ( y | x, z$_{i}$ ) (Eq. 1). During training, this encourages the model to distribute probability mass across any", "metadata": {"uuid": "071cce0e-d0df-4481-af74-850c661834f9", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 49, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 7, "tokens": 195}}, {"text": "document that could yield the correct answer, reinforcing multiple evidence paths. At inference, one can approximately marginalize by taking the most likely y under this mixture model. -Direct concatenation (Early Fusion): As mentioned, another approach is to simply feed all topK retrieved texts into the generator at once (often referred to as \"Fusion-in-Decoder\" when implemented in a decoder-attention context). In this setup, the generator effectively performs its own internal fusion by attending over a combined context. This approach has the advantage that the generator can directly cross-attend to multiple documents and integrate their content, but it may require a more powerful model to handle very long concatenated inputs. It also does not explicitly model the per-document probabilities P ( z$_{i}$ | x ). -Weighted Aggregation: Some systems introduce an attention or weighting mechanism over retrieved documents. For instance, the generator's decoder might assign different attention weights to different passages at each decoding step, effectively learning which source is most useful for generating the next token. This can be seen as a soft fusion: rather than hard marginalization or simple concatenation, the model dynamically blends information. In practice, approaches like [52] found that marginalization (which is a form of weighting by the retriever's scores) works well, especially when the retriever is accurate.", "metadata": {"uuid": "01d5274a-7239-457d-93c5-519a0a9cd343", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 50, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 8, "tokens": 267}}, {"text": "Other works have since experimented with learnable fusion weights or iterative retrieval-generation cycles, but the core idea is the same: the model must reconcile possibly conflicting or complementary information from multiple documents to produce a single, coherent answer.", "metadata": {"uuid": "01d5274a-7239-457d-93c5-519a0a9cd343", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 51, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 8, "tokens": 44}}, {"text": "The choice of fusion affects the system's ability to handle conflicting evidence and the credit assignment during training (i.e., which document gets \"credit\" for a correct answer). RAG's probabilistic fusion provides a principled way to train the retriever and generator together by marginalizing, whereas direct concatenation treats the problem in a single forward pass of a generator (often fine for tasks where evidence is mostly additive or when using very large generators). Fusion strategies continue to be an active area of research, but they all serve the goal of effectively utilizing multiple retrieved pieces of text to improve answer completeness and correctness.", "metadata": {"uuid": "bd44a943-55e9-414e-b5db-dca502957d6a", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 52, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 8, "tokens": 121}}, {"text": "Training and Optimization. Training a RAG model involves objectives for both the retriever and the generator, which can be combined in an end-to-end manner. A common training approach is", "metadata": {"uuid": "9db467f3-54a7-4693-a007-43b56aadc420", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 53, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 8, "tokens": 36}}, {"text": "as follows: first, pre-train or initialize the retriever on a relevance task and initialize the generator on a language modeling or seq2seq task (often using a pre-trained model checkpoint). Then, perform joint fine-tuning on the target task (e.g., a QA dataset or a knowledge-intensive dialogue dataset) by maximizing the likelihood of the correct output y \u2217 given the input x and allowing gradients to flow into both the generator and retriever. The training objective for the whole RAG system can be written as the expected negative log-likelihood:", "metadata": {"uuid": "37c5197e-4ad4-4c46-80ba-a4ad8d43f537", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 54, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 8, "tokens": 112}}, {"text": "\\mathcal { L } _ { R A G } = - \\log P ( y ^ { * } \\, | \\, x ) \\, ,", "metadata": {"uuid": "86a5a3b6-2d0b-44c6-a358-ba3ee3717801", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 55, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 8, "tokens": 33}}, {"text": "where P ( y \u2217 | x ) is computed as in Eq. 1. Because P ( y $^{\u2217}$| x ) is a sum over documents, the gradient will encourage whichever retrieved documents z$_{i}$ that helped predict y \u2217 (by giving high P$_{gen}$ ( y $^{\u2217}$| x, z$_{i}$ ) to have their retrieval probability P$_{ret}$ ( z$_{i}$ | x ) increased. In effect, the model learns to adjust the retriever to fetch better supporting documents and adjust the generator to rely on them appropriately. This joint training is typically done with standard backpropagation; since the retriever's selection operation is not differentiable for all documents, one uses the topK approximation (only those contribute to the loss) and treats the retrieval probabilities for those as soft variables. [52] report that initializing the retriever with DPR and then fine-tuning end-to-end yields the best results, as opposed to training from scratch. Notably, the retriever is trained indirectly here: it does not receive explicit labels of which document is correct, but the generator's success or failure on producing y \u2217 provides a supervision signal. This is sometimes called \"self-supervised\" retriever training or \"feedback\" training.", "metadata": {"uuid": "bc90fe3e-b8d1-45ba-a201-b9b39acd86f0", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 56, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 8, "tokens": 267}}, {"text": "In addition to end-to-end training, various optimization tricks may be used: e.g., using a small learning rate for the retriever if it's already strong, or alternating between retriever-focused and generator-focused updates. In some cases, researchers have also explored contrastive learning at the generation level (to reduce ambiguity between retrieved passages) or reward-based objectives if the task is not a straightforward next-word prediction. However, the most common training objective for RAG is the simple maximum likelihood training of the seq2seq model, augmented by the latent document marginalization. The result is a system where both components are tuned to the end task: the retriever learns to bring useful evidence, and the generator learns to incorporate that evidence into the output. This joint optimization is a major advantage of RAG over non-integrated pipelines, as it", "metadata": {"uuid": "c43f6e9e-c151-4911-ad7a-78ce5c701003", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 57, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 8, "tokens": 166}}, {"text": "aligns the retriever's objective with generating correct final answers (not just retrieving vaguely related documents).", "metadata": {"uuid": "41479822-10c1-4eb3-ad8c-e779f1a62e6d", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 58, "section_path": ["3 Foundations of RAG", "3.2 Technical Components of RAG"], "section_refs": ["#/texts/54", "#/texts/68"], "page_no": 9, "tokens": 20}}, {"text": "The evolution of RAG builds upon earlier developments in open-domain question answering (QA) and neural information retrieval. Traditional open-domain QA systems were typically pipeline-based, consisting of a retrieval step followed by a reading or extraction step [12]. For example, [12] introduced the DrQA system, which first used a TF-IDF or BM25 retriever to select Wikipedia articles and then fed those to a machine reader model to extract answers. This established the value of retrieving relevant text from a large corpus as an essential first step in answering open-domain questions. However, in such pipeline approaches the retriever was not integrated into the learning of the reader, and the system could not adjust retrieval based on the end task's needs. Subsequent research sought to bridge this gap by jointly learning retrieval and answering. Notably, the concept of using learned dense representations for retrieval emerged as a powerful alternative to traditional sparse retrieval. Early milestones in this direction include latent retrieval models: [48] proposed the ORQA model, which treats retrieval as a latent variable problem and pre-trains a neural retriever on an unsupervised \"inverse cloze task\" before jointly fine-tuning it with a reader on QA. Around the same time, [26] introduced REALM, a retrieval-augmented language model pre-training method that incorporated a differentiable retriever into the pre-training of a masked language model.", "metadata": {"uuid": "f26a7622-bea8-4dff-8478-b8d57e93a3bb", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 59, "section_path": ["3 Foundations of RAG", "3.3 Historical Context"], "section_refs": ["#/texts/54", "#/texts/94"], "page_no": 9, "tokens": 279}}, {"text": "REALM demonstrated that pre-training a model to retrieve and reason over Wikipedia could significantly improve open-domain QA, highlighting the benefit of coupling a language model with a learned retrieval mechanism [26]. These efforts were focused primarily on question answering (often extractive), but they laid important groundwork for retrieval-augmented generation by showing that retrieval and neural text generation can be trained in tandem.", "metadata": {"uuid": "f26a7622-bea8-4dff-8478-b8d57e93a3bb", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 60, "section_path": ["3 Foundations of RAG", "3.3 Historical Context"], "section_refs": ["#/texts/54", "#/texts/94"], "page_no": 9, "tokens": 75}}, {"text": "In parallel, the idea of combining external knowledge with neural networks has roots in earlier memory-augmented models . For instance, Memory Networks [95] and subsequent variants allowed neural networks to read from an external memory of facts and use that information to answer questions or generate responses. These models (e.g., [95, 87]) demonstrated the feasibility", "metadata": {"uuid": "e866a481-0234-4231-a367-1accd758f82e", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 61, "section_path": ["3 Foundations of RAG", "3.3 Historical Context"], "section_refs": ["#/texts/54", "#/texts/94"], "page_no": 9, "tokens": 70}}, {"text": "of non-parametric memory for reasoning, albeit on smaller-scale knowledge bases or synthetic tasks. While memory network architectures were often task-specific and required the memory to be relatively small or structured, they presaged the RAG approach by emphasizing that not all knowledge needs to be baked into model parameters-some can be looked up as needed. Another line of work in dialogue systems also integrated retrieval into generation: the Wizard of Wikipedia project [18] is a prime example, where a conversational agent retrieves relevant Wikipedia sentences and conditions a generative dialogue model on those sentences to produce knowledgeable responses. This retrieval-based dialogue system (published in 2019) demonstrated improved factuality and depth in conversational responses, reflecting the general trend that augmenting generators with retrieved context yields more informative and correct outputs.", "metadata": {"uuid": "a12ef6f3-f9b3-448f-8f31-c5ed0402d8e2", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 62, "section_path": ["3 Foundations of RAG", "3.3 Historical Context"], "section_refs": ["#/texts/54", "#/texts/94"], "page_no": 9, "tokens": 155}}, {"text": "These developments converged in 2020 with the formalization of Retrieval-Augmented Generation by [52], who unified the retriever-reader architecture with seq2seq generation in an end-to-end framework. The RAG model of [52] was a culmination of insights from open-domain QA and neural IR: it used a dense passage retriever [45] to fetch text chunks from Wikipedia and a powerful seq2seq generator (BART) to produce answers or summaries, training both components jointly. By marginalizing over multiple retrieved documents (as in Eq. 1), the RAG system could leverage several pieces of evidence and was shown to outperform both parametric-only models and earlier retrieve-and-read pipelines on knowledge-intensive tasks like open QA [52]. The introduction of RAG in 2020 is considered a key milestone because it generalized retrieval-augmented architectures beyond QA to any generative task requiring external knowledge. It also spurred a new line of research into knowledge-enhanced text generation , influencing subsequent models that further refined retrieval modules, document ranking, and fusion techniques for even better performance.", "metadata": {"uuid": "12ccc1e7-0735-4aca-b34a-8f4ab2f272cf", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 63, "section_path": ["3 Foundations of RAG", "3.3 Historical Context"], "section_refs": ["#/texts/54", "#/texts/94"], "page_no": 9, "tokens": 219}}, {"text": "Execution Flow of a RAG System. To summarize the interactions of these components, we outline the step-by-step execution flow of a typical RAG system processing a query:", "metadata": {"uuid": "62154af8-e61b-4b92-9a52-650a027d9983", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 64, "section_path": ["3 Foundations of RAG", "3.3 Historical Context"], "section_refs": ["#/texts/54", "#/texts/94"], "page_no": 9, "tokens": 34}}, {"text": "embedding space.", "metadata": {"uuid": "29212e0d-2db4-4a14-8fbb-1cc6b80b6f79", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 65, "section_path": ["3 Foundations of RAG", "3.3 Historical Context"], "section_refs": ["#/texts/54", "#/texts/94"], "page_no": 10, "tokens": 3}}, {"text": "Before the term retrieval-augmented generation (RAG) was coined, researchers explored methods to combine information retrieval with neural models for question answering (QA) and text generation. Early open-domain QA systems typically employed a retrieve-and-read pipeline: a search module", "metadata": {"uuid": "088e93ff-06d9-4516-8b78-3416c33356a5", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 66, "section_path": ["3 Foundations of RAG", "4.1 Initial Proposals and Early Research (2017-2019)"], "section_refs": ["#/texts/54", "#/texts/107"], "page_no": 10, "tokens": 51}}, {"text": "outputs were considered (e.g., one per retrieved document), the model marginalizes or otherwise aggregates them to produce the final answer. Often the single most likely sequence y is selected as the output. The final answer y is then returned by the system as the response to the query. Optionally, the system might also output the passages it used (providing provenance or justification, which is a useful feature of RAG systems).", "metadata": {"uuid": "d69bf5e9-1cc5-4fdd-bb87-821bb8a4ec06", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 67, "section_path": ["3 Foundations of RAG", "4.1 Initial Proposals and Early Research (2017-2019)"], "section_refs": ["#/texts/54", "#/texts/107"], "page_no": 10, "tokens": 85}}, {"text": "This flow involves interleaving retrieval and generation in a seamless way. Notably, steps 1-2 (retrieval) drastically reduce the problem space by focusing on a handful of documents out of potentially millions, and steps 3-5 ensure that the information in those documents is synthesized into a fluent answer. The entire process is typically very fast at inference: encoding the query and searching the index can be done in tens of milliseconds with efficient vector databases, and generation is on the order of the length of the output (with modern transformers generating dozens of tokens per second). Therefore, RAG systems can scale to handle user queries in real-time applications, all while maintaining higher accuracy by leveraging updated and explicit knowledge. The end-to-end design means that if the output is incorrect, the system can be improved by either enhancing the retriever (fetch more relevant docs) or the generator (better use of docs), or both, which aligns with the modular evaluation and training typical in IR+NLP pipeline but now integrated into a single model.", "metadata": {"uuid": "3bcedd30-d5ba-4ff7-a6f7-7c2b358313c7", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 68, "section_path": ["3 Foundations of RAG", "4.1 Initial Proposals and Early Research (2017-2019)"], "section_refs": ["#/texts/54", "#/texts/107"], "page_no": 10, "tokens": 208}}, {"text": "RAG, as it is known today, was proposed in [52], but before then, the retrieval and read pipeline, which operates like RAG. This section reviews the evolution of RAG from the pre RAG erra till date. The anual year-by-year progress of RAG is illustrated in Figure 2.", "metadata": {"uuid": "5a1f26b9-3812-45fb-8e2b-b09a48fde994", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 69, "section_path": ["4 Year-by-Year Progress in RAG"], "section_refs": ["#/texts/111"], "page_no": 10, "tokens": 65}}, {"text": "locates relevant documents, then a neural reader model extracts or generates answers [12]. For instance, the 2017 DrQA framework answered questions using Wikipedia as a knowledge source by pairing a TF-IDF-based document retriever with an RNN-based reader trained to extract answer spans. Although [12] demonstrated strong performance on open-domain trivia tasks, these systems were piecemeal in nature: the retrieval and", "metadata": {"uuid": "28d52cbd-11e5-4c36-abee-cccd03a16c2e", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 70, "section_path": ["4 Year-by-Year Progress in RAG"], "section_refs": ["#/texts/111"], "page_no": 10, "tokens": 83}}, {"text": "Figure 2: Evolution of a RAG Architecture.", "metadata": {"uuid": "cd01b887-e197-468e-9a26-12b021ea519f", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 71, "section_path": ["4 Year-by-Year Progress in RAG"], "section_refs": ["#/texts/111"], "page_no": 11, "tokens": 11}}, {"text": "generation components were not trained jointly, and the end-to-end approach was limited to extractive answers. In 2018, work shifted toward tighter integration between retrieval and reading. [92] proposed R 3 ( Reinforced Reader-Ranker ), adding a neural ranker to score retrieved passages by answer likelihood. The system then learned ranker-reader synergy via reinforcement learning, boosting open-domain QA accuracy by better filtering relevant evidence. Meanwhile, neural IR methods gained traction. [48] introduced Latent Retrieval in their Open-Retrieval QA (ORQA) framework, training a dense retriever and a reader end-to-end with only question-answer supervision. Specifically, the retriever embeddings were pretrained on an inverse cloze task and then adapted to select evidence documents that help the QA model answer correctly. This concept of dense retrieval , which outperformed sparse BM25 by up to 19% in exact-match QA scores, would become the foundation of subsequent RAG models. Still, these early systems were limited mostly to extractive QA, without a unified end-to-end training for generative outputs.", "metadata": {"uuid": "08197f51-9b6c-47c1-a569-8771e9aeba9a", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 72, "section_path": ["4 Year-by-Year Progress in RAG"], "section_refs": ["#/texts/111"], "page_no": 11, "tokens": 218}}, {"text": "2020 - Birth of RAG. The year 2020 marked a turning point with the official formalization of retrieval-augmented generation . [52] coined the term \"RAG\" and demonstrated its power on knowledge-intensive tasks. RAG explicitly splits knowledge across (i) a neural retriever and (ii) a neural generator , each playing a distinct role. Given a query x , RAG retrieves topk relevant passages { z$_{1}$,...,z$_{k}$ } from a large text corpus (via a learned dense index) and then conditions a seq2seq model on both x and { z$_{i}$ } . Mathematically,", "metadata": {"uuid": "249a8743-1151-4cf2-8de8-1642a4318495", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 73, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 11, "tokens": 137}}, {"text": "P ( y \\, | \\, x ) \\, = \\, \\sum _ { i = 1 } ^ { k } P _ { \\theta } ( y \\, | \\, x , z _ { i } ) \\, P _ { \\phi } ( z _ { i } \\, | \\, x ) ,", "metadata": {"uuid": "8633d0e2-faa5-428f-8495-d9487f00fb2c", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 74, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 11, "tokens": 71}}, {"text": "where P$_{\u03c6}$ ( z$_{i}$ | x ) is the retriever's distribution over documents (often realized via a softmax on", "metadata": {"uuid": "a6b0ad2d-e22c-4cbc-8849-ef715310c8aa", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 75, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 11, "tokens": 31}}, {"text": "inner product scores), and P$_{\u03b8}$ ( y | x,z$_{i}$ ) is the conditional probability of the generator. [52] used a BART-based generator and a Dense Passage Retriever [45], outperforming older retrieve-and-read pipelines by leveraging both parametric and non-parametric memory. Simultaneously, [26] introduced REALM : retrieval-augmented pretraining ,", "metadata": {"uuid": "0e8296a3-3681-4557-8d46-eee7ab0156d4", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 76, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 11, "tokens": 81}}, {"text": "which integrated a differentiable retriever into a language model to predict masked tokens with retrieved evidence. REALM achieved significant QA gains over conventional LMs, validating that external knowledge injection helps both at pretraining and fine-tuning. [45] also released their Dense Passage Retrieval (DPR) approach, establishing a simple but effective dual-encoder architecture. By 2020's end, retrieval-augmented strategies had become key to state-of-the-art QA, and generative models demonstrated improved factuality by referencing retrieved text. Some particular progress made in 2020 are discussed below:", "metadata": {"uuid": "0cc8018d-a0ff-453b-83cc-a168cb24698e", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 77, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 12, "tokens": 117}}, {"text": "Dense neural retrieval. Classical TF-IDF/BM25 retrieval limited earlier pipeline QA systems. [44] introduced Dense Passage Retrieval (DPR) , training dual BERT encoders to embed questions and passages into a shared space. DPR improved top 20 recall by 9-19 pp over BM25 and became the de facto index for RAG models.", "metadata": {"uuid": "625b7b56-dba6-45b5-aa44-a5e355645dcc", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 78, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 12, "tokens": 72}}, {"text": "End-to-end retriever-generator architectures. Building on DPR, [51] proposed the eponymous RAG model: a BART generator conditions on k passages retrieved (and jointly trained) via DPR. Two variants-RAG-Sequence and RAG-Token-achieved state-of-the-art exact-match scores on Natural Questions and TriviaQA, while producing more specific, better-grounded answers than closed-book T5 of comparable size. Concurrently, the Fusion-in-Decoder (FiD) architecture of [34] showed that a T5 decoder attending over dozens of retrieved passages could push QA accuracy even higher, underscoring that modern seq2seq models can synthesise evidence from many documents.", "metadata": {"uuid": "48a15f1c-a7e6-40d2-96ff-554b3a5959a4", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 79, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 12, "tokens": 140}}, {"text": "Retrieval-aware pre-training. Rather than bolt retrieval on during fine-tuning, REALM [25] trained a BERT-style model to retrieve Wikipedia passages to fill masked tokens, optimising retriever and LM jointly. REALM outperformed larger closed-book models by up to 16 pp in open QA. MARGE [49] extended the idea: the model reconstructs a target document from related texts it retrieves, yielding strong zero-shot results in multilingual summarisation and translation.", "metadata": {"uuid": "348df156-a7a9-4e96-a3d4-6e287c2dfe9b", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 80, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 12, "tokens": 96}}, {"text": "Applications beyond QA. RAG principles generalized quickly. In knowledge-grounded dialogue, bridging a prior (inference-time) and posterior (training-time) retriever improved response", "metadata": {"uuid": "31796960-fcf0-403c-8360-573289bce4c7", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 81, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 12, "tokens": 34}}, {"text": "relevance [15]. Fact-checking systems retrieved evidence then generated verdicts, while early experiments hinted at factuality gains in abstractive summarisation. The KILT benchmark [68] unified eleven knowledge-intensive tasks with a shared Wikipedia snapshot and evaluation that scores both answer correctness and evidence retrieval. A single RAG baseline proved competitive across QA, dialogue, fact verification, and slot-filling, highlighting RAG's domain-agnostic promise.", "metadata": {"uuid": "15daaf79-56d2-4277-bdf4-01dc49b1f255", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 82, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 12, "tokens": 88}}, {"text": "Impact and open questions. By year-end 2020, RAG systems of only a few hundred million parameters were surpassing 11-billion-parameter closed-book LMs [52, 75], demonstrating the efficiency of hybrid parametric + non-parametric memory. Challenges that remained-in retrieval latency at scale, multi-hop reasoning, and tighter faithfulness evaluation-set the agenda for subsequent work. Nonetheless, the 2020 breakthroughs firmly established retrieval-augmented generation as a core methodology for building knowledgeable, transparent, and updatable NLP systems.", "metadata": {"uuid": "d1282bee-7edc-42da-94ef-4f46550cf0c3", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 83, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 12, "tokens": 108}}, {"text": "2021 - Improving Generation Quality. Work in 2021 centered on refining how retrieval and generation interact, leading to Fusion-in-Decoder (FiD) by [34]. Rather than marginalize over topk passages as in RAG, FiD concatenates them and feeds them all into a T5-based seq2seq, allowing the decoder to attend to multiple documents simultaneously. This architecture demonstrated that large generative models can effectively combine evidence from many passages, achieving further gains on open QA benchmarks. Meanwhile, new tasks beyond QA surfaced, such as fact-checking, knowledge-grounded dialogue, and entity-rich tasks [68], all relying on retrieval to supply correct and up-to-date information. By 2021, RAG had expanded its reach to a wider range of knowledge-intensive NLP domains. Some major progress made in 2021 are discussed below:", "metadata": {"uuid": "94c2b62a-4e02-463b-884c-eb9d10d5b221", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 84, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 12, "tokens": 172}}, {"text": "Open-domain QA was an early focus of RAG research. [34] introduced the Fusion-in-Decoder (FiD) architecture, which encodes each retrieved passage independently and fuses them in the decoder. FiD achieved state-of-the-art results on benchmarks such as Natural Questions and TriviaQA, showing that performance improves as more evidence passages are. In parallel, [76] developed an end-to-end training scheme (EMDR2) for multi-document QA. By treating retrieval as a latent variable and iteratively training the retriever and reader with an", "metadata": {"uuid": "c1306975-193b-4221-843c-5c9ecaf82013", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 85, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 12, "tokens": 111}}, {"text": "EM-like algorithm, they improved answer accuracy across datasets, establishing new state-of-the-art results without supervised retrieval labelsyielded. Together, these works demonstrated that advanced RAG architectures and training strategies yield substantial QA gains in 2021.", "metadata": {"uuid": "01522e4f-982b-4904-8760-82c7e51f2e19", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 86, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 13, "tokens": 48}}, {"text": "RAG techniques also permeated knowledge-grounded dialogue system. The KILT benchmark introduced by [69] unified several knowledge-intensive tasks (QA, fact-checking, dialogue, etc.) on a common Wikipedia knowledge source. KILT showed that a shared dense retriever and generative model can serve as a strong generalist system: it outperformed task-specific baselines in fact-checking and open-domain QA, and was competitive on dialogue and entity linking:contentReference[:4]index=4. This suggests that RAG-style models (a retriever plus a seq2seq generator) can effectively support multi-turn, knowledge-grounded conversation. While Wizard-of-Wikipedia (2019) had earlier demonstrated retrieval in dialogue, the KILT results in 2021 underscored that unified RAG pipelines are robust across dialogue and QA domains.", "metadata": {"uuid": "7c823f01-1baa-47ff-bbea-5b86196f5c63", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 87, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 13, "tokens": 169}}, {"text": "In summarization and content generation, RAG has been applied to integrate relevant context. One example is code summarization: [67] proposed REDCODER, a framework that retrieves relevant code snippets or summaries to augment a code generation model. By searching a codebase for similar examples, REDCODER improved both code generation and summarization quality on Java and Python benchmarks. More broadly, retrieval-augmented summarization was noted to help produce more accurate and up-to-date summaries by grounding language models in external documents (e.g. news or scientific articles), though in 2021 most demonstrations were in specialized domains like code or clinical summarization. Fact-checking similarly benefited: retrieving evidence from text (e.g. news or scientific literature) provides the basis for verifying model outputs. KILT explicitly includes fact-checking (the FEVER dataset) in its suite, and found that a general RAG approach is competitive on evidence retrieval for claims.", "metadata": {"uuid": "9cdde932-e554-4a35-8d33-b92daf5b1e4e", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 88, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 13, "tokens": 189}}, {"text": "Several methodological improvements emerged in 2021. Beyond FiD and EMDR2, researchers explored how to structure RAG models. [51] distinguished two RAG variants: one conditions on a fixed set of retrieved passages for the entire output, while the other can fetch different passages at each decoding", "metadata": {"uuid": "ff133a58-403b-49b7-b91f-d8c35efef2ce", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 89, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 13, "tokens": 59}}, {"text": "step. [34] exemplified the former by concatenating encoded passages in the decoder. but in 2021 the focus was on multi-passage integration. Additionally, enhancements to retrievers were important: dense retrievers like DPR [44] underpin many 2021 RAG systems, often pretrained on open-domain QA. Training retrievers jointly with generators, as in integrated topic or dialogue context into retrieval for conversational settings. In all cases, the synergy of retrieval and generation architectures was a key theme.", "metadata": {"uuid": "80d51bc1-283f-4555-806c-3e2c6f4d71ca", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 90, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 13, "tokens": 101}}, {"text": "By the end of 2021, standardized benchmarks and code releases helped consolidate RAG progress. The KILT benchmark in particular brought together 11 tasks across multiple knowledge-intensive domains. On this benchmark, the combination of a pretrained retriever and a large generative model achieved strong performance across QA, dialogue, and even slot filling, highlighting the versatility of RAG pipelines. In open QA, the Natural Questions, TriviaQA, and EfficientQA datasets continued to serve as metrics for RAG-based methods; FiD and other models raised the bar on these tasks in 2021. Summaries of RAG results noted major gains over prior baselines in both accuracy and factuality.", "metadata": {"uuid": "fd96a861-d17c-422d-b8f6-108abfe754cb", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 91, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 13, "tokens": 136}}, {"text": "2022 - Scaling & Specialization. In 2022, [10] introduced RETRO ( Retrieval-Enhanced Transformer ), revealing that a moderately sized 7.5B-parameter model could match GPT-3 (175B params) if it retrieves relevant text chunks from a huge corpus (2T tokens), thereby proving that retrieval can substitute for scale. RETRO thus illustrated the efficiency advantage of external knowledge over solely increasing model parameters. In parallel, [35] presented Atlas , a retrieval-augmented language model aiming for few-shot learning on knowledge-intensive tasks. Atlas combined a T5-based generator with an advanced dense retriever, pre-trained on massive unlabeled text. Demonstrating strong results in few-shot scenarios (only 64 examples needed for respectable performance), Atlas highlighted the synergy between retrieval and pre-trained seq2seq in reducing reliance on large labeled datasets.", "metadata": {"uuid": "50fadb70-832e-42f1-b9ad-bdc0e5e3b65a", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 92, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 13, "tokens": 174}}, {"text": "[52] and DPR [44] also proved that conditioning generation on retrieved passages boosts factual accuracy. DeepMind's RETRO model [9] attaches a nearest-neighbour retriever to every decoding window of a 7-billion-parameter transformer; with a 2-trillion-token index, RETRO matches GPT-3", "metadata": {"uuid": "0703f19c-6d44-4d9a-89c6-7f6bd67cc129", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 93, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 13, "tokens": 64}}, {"text": "performance while using 25 \u00d7 fewer parameters, underscoring retrieval's efficiency gains.", "metadata": {"uuid": "e2eca1b1-4c0b-464e-b38a-f5a37be88e89", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 94, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 14, "tokens": 17}}, {"text": "Google's ATLAS [35] unifies retrieval and generation during pre-training . Using the unsupervised Contriever dense retriever [37] and a Fusion-in-Decoder reader, ATLAS achieves new state-of-the-art results on Natural Questions and TriviaQA and, in few-shot mode, outperforms much larger PaLM-540B by 3 EM on NQ with only 64 examples. These results highlight that high-quality retrieval plus multi-document fusion can outperform sheer parameter count.", "metadata": {"uuid": "14f61eb1-3560-4a14-9b56-d4b8607d69aa", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 95, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 14, "tokens": 99}}, {"text": "Beyond QA, RAG became core to knowledge-grounded dialogue . Meta's BlenderBot 3 [80] couples a 175B LLaMA-style generator with live internet search and long-term memory, reducing hallucinations and increasing user engagement in open-domain conversation. BlenderBot 3's deployment study shows that users prefer retrieval-grounded responses and that continual online learning can keep the retriever index current. Retrieval quality itself improved through unsupervised contrastive training. Contriever [37] dispenses with labelled (question, passage) pairs, yet surpasses BM25 and even DPR on BEIR benchmarks, making high-recall indexes available for any corpus. Such retrievers power ATLAS and other 2022 RAG systems, demonstrating that scalable training data is no longer a bottleneck.", "metadata": {"uuid": "1c90cd65-22fa-482c-959f-72125764ba22", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 96, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 14, "tokens": 159}}, {"text": "Finally, 2022 work extended RAG to fact-checking, summarization, and few-shot learning . ATLAS gains 5F1 on FEVER, indicating that retrieved evidence helps verdict generation. Early studies in retrieval-augmented summarization show improved factual consistency by grounding summaries in external documents. Few-shot evaluations reveal that retrieval narrows the data gap: ATLAS and RETRO deliver strong accuracy with under 100 task examples, whereas closed-book baselines require orders of magnitude more data.", "metadata": {"uuid": "0612be9d-3460-44a6-a87f-b0c67b58d0ad", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 97, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 14, "tokens": 100}}, {"text": "Outlook By the end of 2022, RAG had broadened from open-domain QA into a general recipe for knowledge-intensive NLP. Parameter-efficient hybrids like RETRO and ATLAS challenge the notion that bigger models alone yield better knowledge; instead, high-quality retrieval and multi-document reasoning emerge as key levers. Open challenges include faster retrieval over trillion-token corpora,", "metadata": {"uuid": "0ddf359f-501f-4db5-b52c-b09138ccfd09", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 98, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 14, "tokens": 74}}, {"text": "differentiable multi-hop reasoning, and robust evaluation of evidential faithfulness, but 2022 firmly established retrieval-augmented generation as a premier path toward up-to-date, factual, and data-efficient language models.", "metadata": {"uuid": "3bd2068d-0417-43a4-9158-8f20ee9cdd26", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 99, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 14, "tokens": 45}}, {"text": "2023 -RAG Meets LLMs. By 2023, mainstream LLM-based applications (e.g., ChatGPT with plugins, Bing Chat, enterprise chatbots) widely incorporated retrieval [27]. This retrieve-then-generate paradigm was used to mitigate hallucinations and update factual knowledge post-training. The debate emerged as to whether long-context LLMs (with tens of thousands of tokens) could negate the need for retrieval systems. Studies like [54] showed that while large context windows can absorb more text, RAG remains more cost-efficient and better at exposing citations. Hybrid approaches also arose: letting a model choose between retrieving or just using a long context. Overall, RAG became a cornerstone for credible LLM deployments needing up-to-date knowledge and interpretability. SOme of the areas that received attention in 2023 are discussed below:", "metadata": {"uuid": "42aafb25-bbc8-4e13-81f6-ed98286fbf01", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 100, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 14, "tokens": 172}}, {"text": "Scale and few-shot learning: Atlas[36], an 11B-parameter retrieval-augmented model, achieved 42.4% accuracy on Natural Questions with only 64 training examples, outperforming a 540B closed-book model by 3%. Atlas also set new few-shot records on TriviaQA and FEVER (gains of +3-5%), matching 540B-scale performance on multi-task benchmarks. Crucially, Atlas's dense document index can be easily updated with new text, demonstrating updatable knowledge.", "metadata": {"uuid": "1698bd87-2848-49d1-b7ef-0ff3764ae20e", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 101, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 14, "tokens": 106}}, {"text": "Adaptive retrieval: Self-RAG[5] trains a single language model to generate special \"reflection\" tokens that trigger on-demand retrieval and self-critique. In experiments, 7B and 13B Self-RAG models substantially outperformed ChatGPT and a RAG-augmented Llama-2-chat baseline on open-domain QA, reasoning, and fact-verification tasks, yielding much higher factual accuracy and citation precision.", "metadata": {"uuid": "7d4d443b-271d-4a53-9475-8bc0430a42ce", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 102, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 14, "tokens": 86}}, {"text": "Knowledge-grounded dialogue: Retrieval augments dialogue systems to improve consistency and informativeness. Kumari et al.[47] incorporate retrieved persona and context snippets in long conversation modeling, showing that adding relevant knowledge improves response quality. Similarly, Kang et al.[43] propose SURGE , which retrieves relevant subgraphs from a knowledge graph and uses them to bias the response generation. SURGE produces more coherent, factual responses grounded in the retrieved", "metadata": {"uuid": "3e2850af-9d42-4890-80d8-82cc7dbc95b1", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 103, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 14, "tokens": 86}}, {"text": "knowledge.", "metadata": {"uuid": "4552ee9b-65e5-42d1-9215-1a26e597689d", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 104, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 15, "tokens": 2}}, {"text": "Summarization and explanation: RAG has been applied to summarization and explanation tasks. By retrieving source documents or evidence passages, RAG-augmented summarizers produce more accurate and detailed summaries than closed-book models. Likewise, in fact-checking pipelines, retrieving evidence before verification leads to more reliable verdicts and explanations. These applications extend RAG's grounding advantages beyond QA to a broader range of generative tasks.", "metadata": {"uuid": "7849fbe3-4e70-4518-80d9-0fab1b8899d9", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 105, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 15, "tokens": 84}}, {"text": "Benchmarks and evaluation: Knowledge-intensive benchmarks track RAG progress. OpenQA tasks (Natural Questions, TriviaQA, HotpotQA) and the KILT benchmark suite (including QA, fact-checking, slot filling, etc.) are standard evaluation sets. In 2023, RAG models dominated many KILT tasks and few-shot QA challenges. New evaluation tools also emerged: for example, the RAGAS framework provides reference-free metrics for RAG pipelines, and the RAGTruth corpus (Niu et al. 2024) enables fine-grained analysis of hallucinations in RAG outputs. 2023 also witnessed a major spike in publications and adoption of RAG. Some other works are on Active RAG [38], Improving domain adaptation of RAG models for open question answering [85], content filtering for RAG [94], and RAG with self-memory [16].", "metadata": {"uuid": "94da0b7a-81a4-4740-a9fd-9966f5ae06ce", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 106, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 15, "tokens": 180}}, {"text": "2024 - Recent Advances. In 2024, research on RAG continues to push on secure retrieval frameworks, multi-hop reasoning, and domain specialization. Several groups explore differentiable retrievers that can be tuned in an end-to-end pipeline, while others investigate merging large-context attention with retrieval indexing. Meanwhile, new techniques aim to reduce the chance the generator ignores retrieved data or merges contradictory documents incorrectly. RAG-based chat systems in healthcare, finance, and law now incorporate advanced fact-checking modules to ensure that only vetted external sources influence the final output. Some notable works in 2024 are Evaluating Retrieval Quality in RAG [78], Benchmarking RAG for Medicine [97], Benchmarking LLM in RAG [13], Review of RAG for AI Generated Content [106], Unifying Context Ranking with RAG [100], Searching for the Best Practice in RAG [93], Finetuning Vs RAG for Less Popular Knowledge [86], Integrating RAG wit LLM in Nephrology [61], RAG for Copyright Protection [23], RAG for Textual Graph Understanding and Question Answering [31], Interactive AI with RAG for NExt", "metadata": {"uuid": "15599943-5202-419d-84b6-067fb7018368", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 107, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 15, "tokens": 237}}, {"text": "Generation Networking [103], Web Application for RAG: Implementation and Testing [72], Overcoming Challenges for Long Input in RAG [39], and Adapting Language Model for Domain Specific RAG [104]. Some specifics in RAG development as at 2024 are discussed below:", "metadata": {"uuid": "42b91596-7919-43ef-8c40-727a37012b3b", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 108, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 15, "tokens": 58}}, {"text": "Infrastructure and standardized evaluation: The community recognised a need for common tooling and shared tasks. Ragnarok introduced a reusable end-to-end framework and provided industrial baselines for the inaugural TREC 2024 RAG Track [70]. Beyond code, evaluation methodology itself became a research focus: ARAGOG proposed automatic grading of RAG outputs that correlates with human judgements, analysing retrieval precision and answer similarity across advanced pipelines [20]. These efforts mark a shift from anecdotal demos to systematic, reproducible assessment.", "metadata": {"uuid": "6fdc97c7-dca7-46ac-906f-cbf348f85e37", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 109, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 15, "tokens": 101}}, {"text": "Adaptive retrieval and self-reflection: Building on ideas such as Self-RAG, several 2024 works taught models to decide when-and how much-to retrieve . SAM-RAG dynamically filters documents and verifies both evidence and final answers in multimodal contexts, improving accuracy without unnecessary retrieval calls [102]. For complex visual-question-answering, OmniSearch plans multi-hop retrieval chains on the fly, demonstrating large gains on the new Dyn-VQA benchmark [53]. These results confirm that retrieval policies, not just retriever quality, matter for difficult queries.", "metadata": {"uuid": "f55a235a-8247-4862-93f6-4ad1c49c5444", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 110, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 15, "tokens": 108}}, {"text": "Multimodal RAG breaks out: Where earlier RAG research was text-only, 2024 saw a surge in multimodal extensions. SAM-RAG and OmniSearch both combine text and image evidence, while concurrent frameworks (e.g. mR $^{2}$AG and M3DocRAG) introduce retrieval-reflection loops or structured vision-language indexes. Surveys published this year chart the design space and highlight open issues such as cross-modal alignment and vision-aware ranking [102, 53].", "metadata": {"uuid": "c6d93b4a-7862-4090-83d3-dfd5f8d3d6bc", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 111, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 15, "tokens": 100}}, {"text": "Progress in dense retrieval: High-recall retrievers remain the backbone of every RAG system. 2024 research emphasised unsupervised or instruction-tuned retrievers that avoid costly labelled data, building on contrastive pre-training techniques and LLM-augmented embedding models. These retrievers power the top submissions in the TREC track and underpin production deployments discussed in industrial white papers.", "metadata": {"uuid": "c1af9eca-18d4-4eeb-85cf-40a3fce7c54b", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 112, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 15, "tokens": 78}}, {"text": "Rapid growth and forward outlook: A bibliometric snapshot counted more than 1,200 RAG-related papers on arXiv in 2024 alone [106],", "metadata": {"uuid": "20ed4bbc-b202-47be-bc68-10eeaa31d3ed", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 113, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 15, "tokens": 33}}, {"text": "compared with fewer than 100 the previous year, underscoring the field's rapid maturation. Looking ahead, challenges include ultra-fast retrieval over trillion-token corpora, faithfulness verification for multi-hop reasoning, and energy-efficient multimodal indexing. Nevertheless, 2024 firmly established RAG as the default strategy for grounding large language (and vision-language) models in up-to-date, attributable knowledge.", "metadata": {"uuid": "6fdddfaf-6589-4e4f-aca1-0906c4348c31", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 114, "section_path": ["4 Year-by-Year Progress in RAG", "4.2 Major Milestones (2020-2024)"], "section_refs": ["#/texts/111", "#/texts/117"], "page_no": 16, "tokens": 80}}, {"text": "The community is exploring how to marry graph knowledge with text retrieval. A comprehensive survey formalised the GraphRAG paradigm and mapped design choices for graph-aware retrievers and generators [29]. A companion study compared vanilla RAG and GraphRAG across QA and summarisation, showing complementary strengths and proposing hybrid fusion strategies [28].", "metadata": {"uuid": "cf9f6414-578e-4660-ac92-52c3248b8ff2", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 115, "section_path": ["4 Year-by-Year Progress in RAG", "4.3 2025 - The Current Direction"], "section_refs": ["#/texts/111", "#/texts/165"], "page_no": 16, "tokens": 65}}, {"text": "Security and robustness. SafeRAG introduced the first security benchmark for RAG pipelines, cataloguing four attack classes (silver noise, context conflict, soft-ad, DoS) and demonstrating that 14 representative systems fail even simple manipulations [55]. Findings sparked interest in provenance tracking and adversarially trained retrievers.", "metadata": {"uuid": "7bd0008a-2309-4d76-9d23-03bb0dd0891f", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 116, "section_path": ["4 Year-by-Year Progress in RAG", "4.3 2025 - The Current Direction"], "section_refs": ["#/texts/111", "#/texts/165"], "page_no": 16, "tokens": 64}}, {"text": "Agentic and selective retrieval. A survey on Agentic RAG synthesised emerging patterns-reflection, planning, tool use-and argued that autonomous agents can orchestrate multi-hop retrieval more effectively than static pipelines [83]. Concrete instantiations followed:", "metadata": {"uuid": "c29377fc-2366-48d2-8bfc-2be20acc72cc", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 117, "section_path": ["4 Year-by-Year Progress in RAG", "4.3 2025 - The Current Direction"], "section_refs": ["#/texts/111", "#/texts/165"], "page_no": 16, "tokens": 49}}, {"text": "Conversational evaluation at scale. IBM's mtRAG benchmark is filling a gap in multi-turn assessment: 110 human-written conversations (7.7 turns each) across four domains, plus synthetic variants, revealed that state-of-the-art systems struggle with unanswerable and follow-up questions citep katsis2025mtrag. Alongside automatic judges such as RL-F and RB-LLM, mtRAG enables holistic measurement of retrieval, generation, and turn-level faithfulness.", "metadata": {"uuid": "783e417f-b792-411f-aa6b-c8969ec25c91", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 118, "section_path": ["4 Year-by-Year Progress in RAG", "4.3 2025 - The Current Direction"], "section_refs": ["#/texts/111", "#/texts/165"], "page_no": 16, "tokens": 97}}, {"text": "Emerging directions. Workshops and surveys highlighted three open fronts. First, agentic planning -letting models reason over retrieval actions-promises better long-horizon reasoning but raises cost and safety questions. Second, structured retrieval (graphs, tables, multimodal stores) demands new embedding spaces and fusion operators. Third, secure and privacy-preserving RAG is gaining urgency, with SafeRAG prompting work on attack-aware retrievers and watermarking of retrieved evidence.", "metadata": {"uuid": "ee419f64-9b75-48df-abf5-d21d6430827e", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 119, "section_path": ["4 Year-by-Year Progress in RAG", "4.3 2025 - The Current Direction"], "section_refs": ["#/texts/111", "#/texts/165"], "page_no": 16, "tokens": 91}}, {"text": "Outlook Mid-2025 results suggest that modest-sized, security-hardened, agent-controlled RAG systems can rival much larger closed-book LMs while offering provenance. Key challenges ahead include trusted evidence ranking at trillion-document scale, automatic detection of retrieval-based attacks, and seamless integration of non-text modalities. Nevertheless, 2025 firmly positions RAG not merely as a booster of accuracy but as an essential framework for reliable , updatable , and auditable language agents.", "metadata": {"uuid": "ef09dbbd-7129-48ff-9afb-1a061c3ede7a", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 120, "section_path": ["4 Year-by-Year Progress in RAG", "4.3 2025 - The Current Direction"], "section_refs": ["#/texts/111", "#/texts/165"], "page_no": 16, "tokens": 94}}, {"text": "Organizations increasingly apply RAG to private internal knowledge , using a secure pipeline to retrieve from proprietary documents and feed them to a generative model. This often involves on-premise or VPC-hosted vector databases, ensuring that queries and document embeddings never leave the corporate firewall [60, 33]. Enterprises store their text in an index, and at inference time, a local embedding model transforms a user query into a dense vector to find topk relevant chunks. These chunks are appended to the user query as context for an LLM (e.g., GPT-4, or a self-hosted model). The separation of knowledge in a database reduces the risk that proprietary data leaks into the model's parameters, but it does not fully guarantee privacy-the generator might still reveal sensitive details in the output. To mitigate these risks, some enterprise solutions incorporate access control layers that filter out documents the user lacks permission to see. Others experiment with secure enclaves or homomorphic encryption to blind the retrieval operation [105]. However, performance overhead is non-trivial. Another strategy is to fine-tune an LLM on domain data. Although fine-tuning helps the model internalize domain nuances, it can potentially memorize private text. RAG better suits data freshness : the knowledge store can be updated without retraining.", "metadata": {"uuid": "b044e8f8-4bc0-4747-a11e-cd79e803cf81", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 121, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.1 Current Approaches"], "section_refs": ["#/texts/174", "#/texts/175"], "page_no": 16, "tokens": 258}}, {"text": "In recent years, several organizations have explored the deployment of Retrieval-Augmented Generation (RAG) systems to leverage proprietary data effectively. Notable case studies include:", "metadata": {"uuid": "80fe5d85-15e6-452e-90cf-2b67ed2f5e28", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 122, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.2 Industry Implementation Examples"], "section_refs": ["#/texts/174", "#/texts/178"], "page_no": 17, "tokens": 32}}, {"text": "PGA Tour's Use of RAG for Enhanced Information Retrieval The PGA Tour implemented a RAG system to improve information retrieval related to golf events and player statistics. By integrating their extensive proprietary data into the RAG framework, they enabled more accurate and contextually relevant responses to user queries. This approach addressed previous limitations where general AI models lacked specific domain knowledge, leading to inaccuracies in responses. The RAG system allowed the PGA Tour to provide precise information, enhancing user engagement and satisfaction [11].", "metadata": {"uuid": "ee0cda55-bf1f-420c-8ddc-0d2dbd09bfdf", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 123, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.2 Industry Implementation Examples"], "section_refs": ["#/texts/174", "#/texts/178"], "page_no": 17, "tokens": 98}}, {"text": "Bayer's Application of RAG in Agricultural Data Management Bayer utilized RAG to manage and retrieve proprietary agricultural data, aiming to provide farmers with accurate and timely information. By incorporating their extensive datasets into the RAG system, Bayer enhanced the accessibility and usability of critical agricultural information. This integration facilitated better decision-making processes for farmers, leading to improved crop management and productivity [11].", "metadata": {"uuid": "3d752bf4-7947-4f3e-b056-3b952b1381b9", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 124, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.2 Industry Implementation Examples"], "section_refs": ["#/texts/174", "#/texts/178"], "page_no": 17, "tokens": 75}}, {"text": "Rocket Companies' Integration of RAG for Mortgage Processing Rocket Companies explored the use of RAG to streamline mortgage processing by integrating proprietary financial data. The RAG system enabled more efficient retrieval of relevant information, reducing processing times and improving customer experiences. By leveraging their internal data within the RAG framework, Rocket Companies enhanced the accuracy and speed of their services, demonstrating the potential of RAG in financial applications [11].", "metadata": {"uuid": "f53611b5-465f-46af-a318-4d18a5a153ff", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 125, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.2 Industry Implementation Examples"], "section_refs": ["#/texts/174", "#/texts/178"], "page_no": 17, "tokens": 82}}, {"text": "Shorenstein Properties' Implementation of RAG for Data Organization Shorenstein Properties adopted RAG to automate file tagging and organize proprietary data more efficiently. By integrating their internal documents into the RAG system, they improved data accessibility and management. This implementation showcased RAG's capability to handle complex data organization tasks, leading to increased operational efficiency within the company [11].", "metadata": {"uuid": "f90a3e9c-9c17-48db-aba8-45e7b53f6ce2", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 126, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.2 Industry Implementation Examples"], "section_refs": ["#/texts/174", "#/texts/178"], "page_no": 17, "tokens": 73}}, {"text": "Cohere's Advancement of RAG for Source Citation Cohere advanced RAG technology to ensure AI systems cite their sources, facilitating human verification of the information produced. By integrating external texts such as company documents or news websites, Cohere's RAG system reduced errors known as hallucinations and provided access to current information. This development highlighted RAG's potential in enhancing the reliability and transparency of AI-generated content [50].", "metadata": {"uuid": "470200ab-2cd0-4be0-bc6f-27ec5af62a96", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 127, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.2 Industry Implementation Examples"], "section_refs": ["#/texts/174", "#/texts/178"], "page_no": 17, "tokens": 84}}, {"text": "NVIDIA's Deployment of RAG in Enterprise AI Solutions NVIDIA incorporated RAG into their enterprise AI solutions to connect large language models with specific information, such as proprietary customer data and authoritative research. This integration enabled more accurate and relevant responses to user queries, enhancing productivity and protecting against AI hallucinations. NVIDIA's deployment demonstrated RAG's applicability in various industries, including customer service and data management [65].", "metadata": {"uuid": "5ea28b8d-f633-4751-a519-3117dcfb724a", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 128, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.2 Industry Implementation Examples"], "section_refs": ["#/texts/174", "#/texts/178"], "page_no": 17, "tokens": 80}}, {"text": "IBM's Utilization of RAG for Domain-Specific AI Applications IBM employed RAG to equip models with specific information, such as proprietary customer data and authoritative research documents. This approach allowed their AI systems to incorporate up-to-date information into generated responses, improving accuracy and relevance in domain-specific applications. IBM's utilization of RAG showcased its effectiveness in enhancing AI capabilities across different sectors [32].", "metadata": {"uuid": "d0ef6f95-f640-42e8-aa50-fd70c257e634", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 129, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.2 Industry Implementation Examples"], "section_refs": ["#/texts/174", "#/texts/178"], "page_no": 17, "tokens": 78}}, {"text": "These case studies illustrate the diverse applications of RAG in leveraging proprietary data across various industries. By integrating internal datasets into RAG systems, organizations have enhanced information retrieval, improved decision-making processes, and increased operational efficiency. However, challenges such as data privacy, intellectual property concerns, and computational overhead remain areas for further research and development.", "metadata": {"uuid": "d1d9a35d-c990-438d-ae64-ef823df60bd0", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 130, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.2 Industry Implementation Examples"], "section_refs": ["#/texts/174", "#/texts/178"], "page_no": 17, "tokens": 66}}, {"text": "Accuracy vs. Latency Trade-offs: Models optimized for retrieval accuracy (e.g., FiD, WebGPT) typically have higher latency due to multiple document processing. Industry applications prioritize real-time performance (e.g., NVIDIA RAG aims for sub-second latency).", "metadata": {"uuid": "4fb55c07-28ed-4295-8d54-033cbe0a8d10", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 131, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.3 Emergent Trends and Patterns in Industry Implementation"], "section_refs": ["#/texts/174", "#/texts/188"], "page_no": 17, "tokens": 52}}, {"text": "Scaling Beyond Wikipedia: Early RAG models focused on Wikipedia-scale retrieval, while industry applications integrate proprietary databases and real-time web data. WebGPT and IBM Watsonx RAG retrieve from dynamic knowledge bases, offering more up-to-date responses.", "metadata": {"uuid": "c55a964c-e5b4-457a-a9d4-ca0b5a324a8f", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 132, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.3 Emergent Trends and Patterns in Industry Implementation"], "section_refs": ["#/texts/174", "#/texts/188"], "page_no": 18, "tokens": 47}}, {"text": "Reducing Hallucination: There is a shift toward factual consistency. WebGPT and IBM Watsonx RAG enforce citation mechanisms to enhance factual accuracy.", "metadata": {"uuid": "defbb3a2-ad48-41ec-af5e-4b3733b92647", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 133, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.3 Emergent Trends and Patterns in Industry Implementation"], "section_refs": ["#/texts/174", "#/texts/188"], "page_no": 18, "tokens": 30}}, {"text": "Model Size vs. Retrieval Efficiency: Smaller models augmented with retrieval (e.g., Atlas, NVIDIA RAG) can outperform much larger models without retrieval (e.g., GPT-3 175B), demonstrating that knowledge retrieval reduces the need for extreme model scaling.", "metadata": {"uuid": "a7834394-373e-47f4-8847-756ba25ea1a0", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 134, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.3 Emergent Trends and Patterns in Industry Implementation"], "section_refs": ["#/texts/174", "#/texts/188"], "page_no": 18, "tokens": 52}}, {"text": "Enterprise Adaptation: Academic models optimize for benchmark performance, whereas enterprise RAG systems prioritize real-world reliability, integration with databases, and data privacy compliance.", "metadata": {"uuid": "608aa34b-8168-4862-abcb-e9f60ec72ace", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 135, "section_path": ["5 RAG for Proprietary Data -Industry Implementation", "5.3 Emergent Trends and Patterns in Industry Implementation"], "section_refs": ["#/texts/174", "#/texts/188"], "page_no": 18, "tokens": 30}}, {"text": "The evaluation of RAG systems is multifaceted, as performance depends not only on the generative model but also on the quality of the retrieval pipeline. A robust evaluation framework must assess retrieval accuracy, answer quality, factuality, latency, and scalability. This section provides a structured overview of RAG evaluation criteria, benchmarks, and the impact of architectural design choices. Summary of this section is in Table Table 1", "metadata": {"uuid": "684d872c-6c0e-4c3f-85e9-3de106f1f930", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 136, "section_path": ["6 Evaluation of RAG Systems"], "section_refs": ["#/texts/195"], "page_no": 18, "tokens": 82}}, {"text": "RAG performance is commonly assessed along the following dimensions:", "metadata": {"uuid": "70a3b0a9-cde8-4f86-902e-142c42852e02", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 137, "section_path": ["6 Evaluation of RAG Systems", "6.1 Evaluation Dimensions and Metrics"], "section_refs": ["#/texts/195", "#/texts/197"], "page_no": 18, "tokens": 11}}, {"text": "Retrieval Accuracy. Key metrics include:", "metadata": {"uuid": "af860e2f-fa41-4b08-8682-66b67d04a3d6", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 138, "section_path": ["6 Evaluation of RAG Systems", "6.1 Evaluation Dimensions and Metrics"], "section_refs": ["#/texts/195", "#/texts/197"], "page_no": 18, "tokens": 8}}, {"text": "Generation Quality. Evaluated using:", "metadata": {"uuid": "11bca7a0-7f0c-4dea-b93e-707bbfe21e91", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 139, "section_path": ["6 Evaluation of RAG Systems", "6.1 Evaluation Dimensions and Metrics"], "section_refs": ["#/texts/195", "#/texts/197"], "page_no": 18, "tokens": 7}}, {"text": "Efficiency and Latency. These include:", "metadata": {"uuid": "d2c8f7da-c000-4163-954c-33d1ae44d3a7", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 140, "section_path": ["6 Evaluation of RAG Systems", "6.1 Evaluation Dimensions and Metrics"], "section_refs": ["#/texts/195", "#/texts/197"], "page_no": 18, "tokens": 8}}, {"text": "Scalability. As corpus size grows, the system's ability to maintain retrieval quality and generation fidelity is tested. Evaluation considers:", "metadata": {"uuid": "483100a3-bf26-4dc1-b044-78642f858b10", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 141, "section_path": ["6 Evaluation of RAG Systems", "6.1 Evaluation Dimensions and Metrics"], "section_refs": ["#/texts/195", "#/texts/197"], "page_no": 18, "tokens": 26}}, {"text": "Several benchmarks are widely used to evaluate RAG systems:", "metadata": {"uuid": "3035307c-1ae5-426e-86e0-58f2cd0a3226", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 142, "section_path": ["6 Evaluation of RAG Systems", "6.2 Benchmarks and Datasets"], "section_refs": ["#/texts/195", "#/texts/213"], "page_no": 18, "tokens": 11}}, {"text": "RAGAS (Retrieval-Augmented Generation Assessment System) is an evaluation framework specifically designed for assessing and improving the factuality and grounding of RAG systems. Unlike conventional metrics that measure superficial linguistic overlap, RAGAS emphasizes the alignment between generated content and retrieved documents, providing explicit signals regarding factual correctness", "metadata": {"uuid": "538e3be2-bb60-4359-96b0-cc1542aea48b", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 143, "section_path": ["6 Evaluation of RAG Systems", "6.3 Retrieval-Augmented Generation Assessment System"], "section_refs": ["#/texts/195", "#/texts/222"], "page_no": 19, "tokens": 60}}, {"text": "and attribution quality. By systematically measuring how well the generated outputs are supported by the retrieved evidence, RAGAS helps identify and penalize hallucinations-instances where the model generates plausible but unsupported statements. Consequently, employing RAGAS during model training or iterative fine-tuning guides RAG systems toward producing outputs firmly grounded in verifiable sources, substantially improving factual accuracy and reducing the incidence of hallucinated information.", "metadata": {"uuid": "a14ed502-7797-4dcb-b687-702a41f23e26", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 144, "section_path": ["6 Evaluation of RAG Systems", "6.3 Retrieval-Augmented Generation Assessment System"], "section_refs": ["#/texts/195", "#/texts/222"], "page_no": 19, "tokens": 81}}, {"text": "Table 1: Evaluation Dimensions, Metrics, Benchmarks, and Tools for RAG Systems", "metadata": {"uuid": "b65ec6b6-3271-467c-9c56-3f51a506f737", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 145, "section_path": ["6 Evaluation of RAG Systems", "6.3 Retrieval-Augmented Generation Assessment System"], "section_refs": ["#/texts/195", "#/texts/222"], "page_no": 19, "tokens": 18}}, {"text": "Empirical studies demonstrate that architectural components in RAG-chunking, embedding, and re-ranking-directly affect performance across tasks and benchmarks.", "metadata": {"uuid": "1ac32a12-817f-4ec3-857f-6979c4c10e5f", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 146, "section_path": ["6 Evaluation of RAG Systems", "6.4 Impact of Architecture on Evaluation"], "section_refs": ["#/texts/195", "#/texts/226"], "page_no": 19, "tokens": 28}}, {"text": "Fixed-size chunks, as used in early RAG [52], are prone to semantic fragmentation. Semantic chunking (e.g., ChunkRAG ) improves retrieval precision and answer accuracy by aligning chunk boundaries with discourse structure [84]. The choice of embedding model (e.g., DPR [45], HyDE [19]) influences Recall@ k and downstream generation quality. Similarly, re-ranking strategies (e.g., MonoT5) can boost EM and F1 scores by prioritizing more relevant passages.", "metadata": {"uuid": "233cecea-44da-476c-a37f-d134879a8a31", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 147, "section_path": ["6 Evaluation of RAG Systems", "6.4 Impact of Architecture on Evaluation"], "section_refs": ["#/texts/195", "#/texts/226"], "page_no": 19, "tokens": 100}}, {"text": "In high-performing RAG systems, retrieval fidelity correlates strongly with factual answer quality. Thus, optimizing these architectural stages is essential for achieving competitive results on QA and generation benchmarks.", "metadata": {"uuid": "f251e84f-6d93-4409-8f44-a69f1c3a22d0", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 148, "section_path": ["6 Evaluation of RAG Systems", "6.4 Impact of Architecture on Evaluation"], "section_refs": ["#/texts/195", "#/texts/226"], "page_no": 19, "tokens": 35}}, {"text": "This section discusses the challenges of RAG, cases of manifestation of such challenges in the selected domain of RAG application, and outlines existing solutions and the way forward.", "metadata": {"uuid": "0160c620-50e0-495b-87f0-e6186c94f3f4", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 149, "section_path": ["7 Challenges of RAG"], "section_refs": ["#/texts/231"], "page_no": 20, "tokens": 33}}, {"text": "The quality of retrieved documents significantly impacts the accuracy of RAG-generated answers. High recall and precision are critical since poor retrieval leads directly to incorrect or irrelevant answers [ 61 ]. Traditional methods like BM25 are limited, often missing relevant texts or returning noisy results [ 12 ]. Modern neural retrievers with dense embeddings improve performance but still face issues like vocabulary mismatches, ambiguous queries, and domain-specific terminology. Specialized domain tuning, such as using legal embeddings or medical synonym expansion, can help, but maintenance of these tailored retrievers remains challenging. Determining the optimal number of retrieved passages ( k ) is also complex. Too few passages limit evidence; too many overwhelm the model and introduce irrelevant context. Approaches like ranking retrieved passages or iterative query reformulation can improve retrieval precision, but add complexity and latency [ 34 , 21 ].", "metadata": {"uuid": "911ef2f7-a284-47fa-8628-198764006b11", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 150, "section_path": ["7 Challenges of RAG", "7.1 Technical Challenges in RAG", "7.1.1 Retrieval Quality and Relevance"], "section_refs": ["#/texts/231", "#/texts/233", "#/texts/234"], "page_no": 20, "tokens": 167}}, {"text": "RAG inherently increases computational complexity and latency compared to standalone LLMs due to retrieval overhead, vector searches, and expanded context processing. Techniques like approximate nearest neighbor indices (e.g., FAISS, HNSW), caching, model distillation, or lightweight retrievers can reduce latency at the expense of accuracy. Integrating retrieval efficiently with large language models (LLMs) and ensuring rapid responses in real-time scenarios (e.g., customer support) remains a significant challenge [ 3 ]. Interestingly, using retrieval can allow smaller models to match the performance of larger models without retrieval (e.g., RETRO, Atlas), reducing model size requirements but shifting complexity to maintaining external knowledge bases and infrastructure.", "metadata": {"uuid": "130d6d4b-e677-4838-abc1-8f7eac88e292", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 151, "section_path": ["7 Challenges of RAG", "7.1 Technical Challenges in RAG", "7.1.2 Latency and Efficiency"], "section_refs": ["#/texts/231", "#/texts/233", "#/texts/236"], "page_no": 20, "tokens": 139}}, {"text": "Integrating retrieved evidence effectively with LLMs is subtle. Models may ignore retrieved evidence, especially when internal model knowledge conflicts with external retrieved information, leading to a \"tug-of-war\" effect [ 41 ]. Multiple retrieved documents might create confusion or confirmation bias if they contradict each other. Limited input lengths in transformer-based LLMs exacerbate these integration challenges by forcing truncation or summarization, potentially omitting essential context. Fine-tuning models specifically for retrieval-augmented tasks often yields better integration than simple zero-shot prompting but introduces complexity, especially when using non-differentiable or API-based models that do not support custom training.", "metadata": {"uuid": "5ad206ee-9bc1-425b-96aa-b84e2f8efb53", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 152, "section_path": ["7 Challenges of RAG", "7.1 Technical Challenges in RAG", "7.1.3 Integration with Large Language Models"], "section_refs": ["#/texts/231", "#/texts/233", "#/texts/238"], "page_no": 20, "tokens": 129}}, {"text": "Deploying RAG at scale requires substantial engineering to maintain large knowledge corpora and efficient retrieval indices. Systems must handle millions or billions of documents, demanding significant computational resources, efficient indexing, distributed computing infrastructure, and cost management strategies [ 21 ]. Efficient indexing methods, caching, and multi-tier retrieval approaches (such as cascaded retrieval) become essential at scale, especially in large deployments like web search engines.", "metadata": {"uuid": "f41170f5-7e14-4baf-812c-0f6c8eea6116", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 153, "section_path": ["7 Challenges of RAG", "7.2 System-Level Challenges", "7.2.1 Scalability and Infrastructure"], "section_refs": ["#/texts/231", "#/texts/240", "#/texts/241"], "page_no": 20, "tokens": 81}}, {"text": "One motivation for RAG is providing current information. However, continuously updating external knowledge bases and retrieval indices is challenging. Domains requiring real-time updates (e.g., finance, healthcare) demand sophisticated data pipelines for incremental updates, possibly frequent re-encoding of documents, and synchronization of retrieval indices. Delays in updates or inconsistencies between the LLM's internal knowledge and newly retrieved data can produce outdated or contradictory answers [ 61 ].", "metadata": {"uuid": "92583d44-cff2-4475-833f-3c03a17532f2", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 154, "section_path": ["7 Challenges of RAG", "7.2 System-Level Challenges", "7.2.2 Freshness and Knowledge Updates"], "section_refs": ["#/texts/231", "#/texts/240", "#/texts/243"], "page_no": 20, "tokens": 86}}, {"text": "While RAG reduces LLM hallucinations, it does not eliminate them completely. Models may fabricate or misattribute information if retrieval provides incomplete or partially contradictory context.", "metadata": {"uuid": "7755b1a5-8367-4c34-94e9-419bdde8bbcc", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 155, "section_path": ["7 Challenges of RAG", "7.2 System-Level Challenges", "7.2.3 Hallucination and Reliability"], "section_refs": ["#/texts/231", "#/texts/240", "#/texts/245"], "page_no": 20, "tokens": 32}}, {"text": "Legal domain studies found that RAG significantly reduces hallucinations, but still generates errors at concerning rates [ 56 ]. Hallucinations also occur in citation generation, with models occasionally inventing nonexistent references. Strategies such as verifying outputs against retrieved sources or calibrating model confidence are needed, but no approach completely prevents hallucination.", "metadata": {"uuid": "9ff08056-85a4-4718-8934-81a07d0f3414", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 156, "section_path": ["7 Challenges of RAG", "7.2 System-Level Challenges", "7.2.3 Hallucination and Reliability"], "section_refs": ["#/texts/231", "#/texts/240", "#/texts/245"], "page_no": 21, "tokens": 64}}, {"text": "RAG systems comprise multiple components-retrievers\u00bfclearly attributing evidence, providing explanations rerankers, indexes, and LLMs-resulting in increased complexity and potential points of failure. Maintenance includes synchronizing knowledge updates, managing access controls, orchestrating prompts, and handling multi-turn dialogues. Robust evaluation methods must assess end-to-end performance, retrieval quality, and faithfulness of model outputs to the evidence [ 3 ].", "metadata": {"uuid": "20ddfd1d-3063-4558-92c1-9fe9f52406bd", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 157, "section_path": ["7 Challenges of RAG", "7.2 System-Level Challenges", "7.2.4 Complex Pipeline and Maintenance"], "section_refs": ["#/texts/231", "#/texts/240", "#/texts/249"], "page_no": 21, "tokens": 86}}, {"text": "RAG inherits biases from both underlying language models and retrieved external data. Biases may manifest through selective use or amplification of biased retrieved evidence, especially in historically biased domains like legal or medical information [ 7 , 99 ]. Ensuring fairness involves curating inclusive datasets, using diverse retrieval results, and possibly prompting LLMs explicitly toward balanced responses.", "metadata": {"uuid": "86334376-6329-440b-ba26-23ea0e62ea45", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 158, "section_path": ["7 Challenges of RAG", "7.3 Ethical and Societal Concerns", "7.3.1 Bias and Fairness"], "section_refs": ["#/texts/231", "#/texts/251", "#/texts/252"], "page_no": 21, "tokens": 70}}, {"text": "The quality and reliability of retrieved sources directly impact trustworthiness. If misinformation is retrieved, the model can inadvertently disseminate false information. Even credible sources can become outdated or contextually misapplied, leading users to overly trust synthesized AI responses. Transparency in citing sources, maintaining high-quality data, and incorporating verification methods are crucial safeguards.", "metadata": {"uuid": "ea0c3981-53d8-45ea-a568-9cc7ba206053", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 159, "section_path": ["7 Challenges of RAG", "7.3 Ethical and Societal Concerns", "7.3.2 Trustworthiness and Misinformation"], "section_refs": ["#/texts/231", "#/texts/251", "#/texts/254"], "page_no": 21, "tokens": 66}}, {"text": "RAG systems handling sensitive data, especially in enterprise or healthcare contexts, raise serious privacy concerns. Risks include accidental exposure of confidential information and vulnerabilities to prompt injection attacks. Implementing rigorous access control, complying with regulations (e.g., GDPR,", "metadata": {"uuid": "2cfc0393-4ad9-4e1c-8480-11d0d3febf23", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 160, "section_path": ["7 Challenges of RAG", "7.3 Ethical and Societal Concerns", "7.3.3 Privacy and Security"], "section_refs": ["#/texts/231", "#/texts/251", "#/texts/256"], "page_no": 21, "tokens": 48}}, {"text": "HIPAA), transparent data usage policies, and security testing are essential to protect privacy and prevent breaches.", "metadata": {"uuid": "0930a32a-a463-46a2-902c-c4d43c2c6f2a", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 161, "section_path": ["7 Challenges of RAG", "7.3 Ethical and Societal Concerns", "7.3.3 Privacy and Security"], "section_refs": ["#/texts/231", "#/texts/251", "#/texts/256"], "page_no": 21, "tokens": 20}}, {"text": "RAG's use of sourced retrieval provides an advantage for accountability, allowing users to trace AI-generated responses back to evidence. However, inaccurate citations or improper synthesis can mislead users. Ethical deployment involves can misleading users.", "metadata": {"uuid": "d2f84e4a-ed5c-4bd3-942f-9472fa7d3893", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 162, "section_path": ["7 Challenges of RAG", "7.3 Ethical and Societal Concerns", "7.3.4 Accountability and Transparency"], "section_refs": ["#/texts/231", "#/texts/251", "#/texts/260"], "page_no": 21, "tokens": 43}}, {"text": "To concretely illustrate the discussed challenges, the following section examines Retrieval-Augmented Generation (RAG) applications across three distinct domains: legal, medical, and customer support. Each domain has unique requirements influencing the design and deployment of RAG systems.", "metadata": {"uuid": "ba7fac2c-2654-43f3-a9a4-068b660cfdab", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 163, "section_path": ["7 Challenges of RAG", "7.4 Application Domains and Case Studies"], "section_refs": ["#/texts/231", "#/texts/262"], "page_no": 21, "tokens": 49}}, {"text": "Legal practice inherently involves extensive information retrieval from statutes, regulations, and case precedents, making it particularly suited for RAG applications. Commercial legal AI systems such as those provided by Westlaw and LexisNexis leverage RAG for tasks including legal research and case analysis. Accuracy and reliability are paramount; errors such as citing nonexistent cases can lead to severe professional consequences, underscored by incidents involving fabricated case citations generated by general-purpose LLMs such as ChatGPT. RAG attempts to mitigate this by ensuring assertions trace directly to verified sources.", "metadata": {"uuid": "1290995c-5538-460b-83de-f71334c308ab", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 164, "section_path": ["7 Challenges of RAG", "7.4 Application Domains and Case Studies", "7.4.1 Legal Domain"], "section_refs": ["#/texts/231", "#/texts/262", "#/texts/264"], "page_no": 21, "tokens": 110}}, {"text": "Key challenges include:", "metadata": {"uuid": "0005f95a-0e02-49e5-83a3-1466409c433c", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 165, "section_path": ["7 Challenges of RAG", "7.4 Application Domains and Case Studies", "7.4.1 Legal Domain"], "section_refs": ["#/texts/231", "#/texts/262", "#/texts/264"], "page_no": 21, "tokens": 4}}, {"text": "Domain-specific challenges include:", "metadata": {"uuid": "fd145029-9728-4759-a071-52ee33bcb856", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 166, "section_path": ["7 Challenges of RAG", "7.4 Application Domains and Case Studies", "7.4.1 Legal Domain"], "section_refs": ["#/texts/231", "#/texts/262", "#/texts/264"], "page_no": 22, "tokens": 5}}, {"text": "from rigorously vetted medical literature, clinical guidelines, and patient databases, necessitating careful curation and precise summarization to avoid misinterpretation or oversimplification.", "metadata": {"uuid": "41aa15f7-fa09-4738-bbdf-069337f6a487", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 167, "section_path": ["7 Challenges of RAG", "7.4 Application Domains and Case Studies", "7.4.1 Legal Domain"], "section_refs": ["#/texts/231", "#/texts/262", "#/texts/264"], "page_no": 22, "tokens": 33}}, {"text": "The medical domain emphasizes the necessity for high factual accuracy, ethical transparency, and rigorous validation processes comparable to clinical trials. Successful deployment hinges on clear delineation between supportive advisory roles and direct patient intervention [64].", "metadata": {"uuid": "81763118-171b-4a20-951c-7c02e9efc2dd", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 168, "section_path": ["7 Challenges of RAG", "7.4 Application Domains and Case Studies", "7.4.1 Legal Domain"], "section_refs": ["#/texts/231", "#/texts/262", "#/texts/264"], "page_no": 22, "tokens": 41}}, {"text": "RAG systems increasingly automate customer support tasks by retrieving information from internal knowledge bases, FAQs, and troubleshooting guides. While typically lower stakes compared to legal or medical domains, customer support RAG applications encounter distinct practical considerations:", "metadata": {"uuid": "b0c96749-9d6a-41b0-b6f7-b29db55f4219", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 169, "section_path": ["7 Challenges of RAG", "7.4 Application Domains and Case Studies", "7.4.3 Customer Support and Knowledge Bases"], "section_refs": ["#/texts/231", "#/texts/262", "#/texts/278"], "page_no": 23, "tokens": 43}}, {"text": "Ethical considerations include transparency, data security, job impact, and fairness. RAG", "metadata": {"uuid": "2b1abc57-9bdf-4d38-96e2-fc9b14d9ce92", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 170, "section_path": ["7 Challenges of RAG", "7.4 Application Domains and Case Studies", "7.4.3 Customer Support and Knowledge Bases"], "section_refs": ["#/texts/231", "#/texts/262", "#/texts/278"], "page_no": 23, "tokens": 17}}, {"text": "implementations also provide valuable feedback for continual improvement of internal support documentation based on real-time user interactions, thereby enhancing both system performance and documentation quality.", "metadata": {"uuid": "0f6d804a-a671-49a0-a9d2-32bab12f480d", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 171, "section_path": ["7 Challenges of RAG", "7.4 Application Domains and Case Studies", "7.4.3 Customer Support and Knowledge Bases"], "section_refs": ["#/texts/231", "#/texts/262", "#/texts/278"], "page_no": 23, "tokens": 29}}, {"text": "Maintaining high retrieval relevance is critical for effective RAG. Strategies to improve retrieval quality include domain-adaptive training, advanced encoders, and query reformulation methods to address vocabulary mismatches [85]. Employing reranking models further boosts relevance by re-scoring initial retrieval results with deeper contextual analysis, enhancing accuracy at the expense of additional computation [4]. Iterative retrieval and chain-of-thought reasoning represent future directions, breaking down complex queries into simpler sub-queries, thus ensuring relevant information retrieval at each reasoning step [90].", "metadata": {"uuid": "de571041-7668-4da2-a6b5-ccb7e399e666", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 172, "section_path": ["7 Challenges of RAG", "7.5 Existing and Potential Solutions", "7.5.1 Retrieval Quality"], "section_refs": ["#/texts/231", "#/texts/289", "#/texts/290"], "page_no": 23, "tokens": 106}}, {"text": "RAG systems introduce latency due to retrieval processes. Solutions include using efficient nearest-neighbor search structures, such as HNSW graphs, which significantly speed up similarity searches [57]. Caching mechanisms, including multi-level and approximate embedding caches (e.g., RAGCache and Proximity cache), enable reuse of previously retrieved information, drastically reducing retrieval time [40, 8]. Adaptive retrieval methods dynamically balance retrieval complexity based on query difficulty, optimizing overall throughput and reducing latency.", "metadata": {"uuid": "5456f3fd-d2ce-45e6-bd3b-c46653f99972", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 173, "section_path": ["7 Challenges of RAG", "7.5 Existing and Potential Solutions", "7.5.2 Latency"], "section_refs": ["#/texts/231", "#/texts/289", "#/texts/292"], "page_no": 23, "tokens": 95}}, {"text": "Effective integration between retrieval and generation models remains essential. Methods include joint end-to-end training of retrievers and generators, enhancing mutual compatibility and performance [52]. Architectural integration techniques, such as RETRO's cross-attention mechanism, dynamically incorporate retrieved facts during generation [10]. Alternatively, prompt-based integration treats LLMs as black-boxes, conditioning on retrieved documents without architectural modifications. Future hybrid approaches involving reinforcement learning and selective retrieval aim to optimize when and how", "metadata": {"uuid": "5ee9bf6c-9d61-4164-a8e0-15ec5ee2ceb6", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 174, "section_path": ["7 Challenges of RAG", "7.5 Existing and Potential Solutions", "7.5.3 Model Integration"], "section_refs": ["#/texts/231", "#/texts/289", "#/texts/294"], "page_no": 23, "tokens": 91}}, {"text": "external knowledge is incorporated into generation processes.", "metadata": {"uuid": "50c9a452-1fae-4ccd-8983-63d031d9829e", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 175, "section_path": ["7 Challenges of RAG", "7.5 Existing and Potential Solutions", "7.5.3 Model Integration"], "section_refs": ["#/texts/231", "#/texts/289", "#/texts/294"], "page_no": 24, "tokens": 8}}, {"text": "Reducing factual hallucinations remains a key focus. RAG inherently mitigates hallucinations by grounding outputs in retrieved evidence [82]. Training models to penalize ungrounded assertions and iterative retrieval within reasoning processes further enhance accuracy [90]. Self-check mechanisms (Self-RAG), where models critique and revise their outputs against retrieval results, significantly reduce hallucinated content [6]. External verification and fact-checking modules complement internal methods, collectively ensuring high factual reliability. For instance, RAG systems to cite sources significantly enhance their reliability by directly linking generated information to supporting evidence. This citation capability plays a crucial role in mitigating the common issue of hallucination, where generative models produce plausible yet inaccurate or fabricated information. By explicitly associating each factual statement with retrieved documents, RAG systems encourage transparency and verifiability, enabling users and downstream processes to quickly assess the accuracy and provenance of claims. Moreover, requiring the model to cite sources during generation inherently promotes grounding outputs in verified data, further reducing the risk of generating unsupported statements [82]. Thus, citation functionality not only enhances user trust but also fosters more disciplined, factually accurate generation, substantially decreasing the likelihood of hallucinated outputs.", "metadata": {"uuid": "4608a51c-0d1a-4a38-bb9a-ce17c17f82c8", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 176, "section_path": ["7 Challenges of RAG", "7.5 Existing and Potential Solutions", "7.5.4 Hallucination"], "section_refs": ["#/texts/231", "#/texts/289", "#/texts/298"], "page_no": 24, "tokens": 235}}, {"text": "Scalability challenges arise as knowledge corpora expand. Advanced indexing, distributed retrieval, and approximate nearest neighbor techniques facilitate efficient handling of large-scale knowledge bases [57]. Selective indexing and corpus curation, combined with infrastructure improvements like caching and parallel retrieval, allow RAG systems to scale to massive knowledge repositories. Research indicates that moderate-sized models augmented with large external corpora can outperform significantly larger standalone models, suggesting parameter efficiency advantages [10].", "metadata": {"uuid": "716ec666-8340-463b-bb77-1b3347443244", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 177, "section_path": ["7 Challenges of RAG", "7.5 Existing and Potential Solutions", "7.5.5 Scalability"], "section_refs": ["#/texts/231", "#/texts/289", "#/texts/300"], "page_no": 24, "tokens": 88}}, {"text": "Rapidly evolving information necessitates regularly updated knowledge bases. RAG systems can efficiently maintain knowledge freshness through incremental updates and selective retrieval methods without requiring frequent retraining [30]. Integrating live search APIs and hybrid retrieval methods ensure real-time information retrieval, addressing dynamic knowledge demands [21]. Continuous updates and user-feedback integration support lifelong learning and timely information access.", "metadata": {"uuid": "d42a0267-086e-4e45-a37b-98e8c1b5e9bf", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 178, "section_path": ["7 Challenges of RAG", "7.5 Existing and Potential Solutions", "7.5.6 Knowledge Freshness"], "section_refs": ["#/texts/231", "#/texts/289", "#/texts/302"], "page_no": 24, "tokens": 70}}, {"text": "Addressing bias in RAG involves curating balanced knowledge sources, employing diversification techniques in retrieval, and adjusting retriever embeddings to counteract inherent biases [46]. Prompts and model training that encourage balanced representation, along with transparency in source attribution, further mitigate bias propagation. This multi-faceted approach helps minimize biases in RAG outputs.", "metadata": {"uuid": "5a4edd1c-7b82-429d-b4d2-bb15a61cbf80", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 179, "section_path": ["7 Challenges of RAG", "7.5 Existing and Potential Solutions", "7.5.7 Bias"], "section_refs": ["#/texts/231", "#/texts/289", "#/texts/304"], "page_no": 24, "tokens": 67}}, {"text": "Combating misinformation involves preventive measures like curating trustworthy knowledge sources and reactive verification through stance classifiers and credibility assessments [66]. Models employing vigilant prompting, cross-verification with multiple retrieved documents, and external fact-checking modules enhance reliability and truthfulness. Robustness against adversarial misinformation insertion through continuous monitoring and data validation further strengthens RAG systems, ensuring accurate information dissemination.", "metadata": {"uuid": "c455ae5f-2aef-4c1f-8f3f-9096da38c99d", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 180, "section_path": ["7 Challenges of RAG", "7.5 Existing and Potential Solutions", "7.5.8 Misinformation"], "section_refs": ["#/texts/231", "#/texts/289", "#/texts/306"], "page_no": 24, "tokens": 73}}, {"text": "Retrieval-Augmented Generation (RAG) has emerged as a paradigm shift in AI, enabling language models to dynamically retrieve and incorporate external knowledge. Studies confirm that RAG-based models significantly outperform purely parametric generative models, achieving state-of-the-art results on knowledge-intensive NLP tasks [52, 45]. Lewis et al. [52] demonstrated that RAG surpasses fine-tuned BART on open-domain question answering (QA), generating more specific and factual responses.", "metadata": {"uuid": "7bfecf9a-9124-42a6-8c65-bfc39f3c8885", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 181, "section_path": ["8 Discussion and Future Direction", "8.1 Synthesis of Findings"], "section_refs": ["#/texts/308", "#/texts/309"], "page_no": 24, "tokens": 95}}, {"text": "The historical development of RAG is rooted in the evolution of open-domain QA architectures. Early approaches like DrQA [12] relied on sparse lexical retrieval, whereas modern RAG implementations integrate differentiable dense retrievers such as Dense Passage Retrieval (DPR) [45]. By conditioning generation on retrieved evidence, RAG mitigates issues like hallucinations and stale knowledge [34]. The comparative analysis of different RAG architectures reveals trade-offs in retrieval effectiveness, generative fluency, and computational cost.", "metadata": {"uuid": "6cfac80f-1f9a-4e1b-967b-6f21d7a62e88", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 182, "section_path": ["8 Discussion and Future Direction", "8.1 Synthesis of Findings"], "section_refs": ["#/texts/308", "#/texts/309"], "page_no": 25, "tokens": 99}}, {"text": "Despite continuous improvements, challenges remain. RAG's dependency on retrieval quality makes it vulnerable to retrieval failures. Errors in retrieving relevant documents lead to incorrect outputs. Additionally, integrating multiple retrieved passages introduces challenges in fusion mechanisms, sometimes resulting in contradictory evidence or increased latency. The need for robust retrieval strategies and enhanced fusion methods remains a critical research direction.", "metadata": {"uuid": "ca6788ff-695c-4020-a574-b9731dbd839a", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 183, "section_path": ["8 Discussion and Future Direction", "8.1 Synthesis of Findings"], "section_refs": ["#/texts/308", "#/texts/309"], "page_no": 25, "tokens": 68}}, {"text": "A key development in RAG is its application to enterprise AI, enabling access to proprietary data without embedding sensitive information into model parameters [35]. This architecture supports data privacy and ensures outputs remain grounded in up-to-date knowledge [74]. Unlike traditional LLMs, which require retraining to update internal knowledge, RAG allows retrieval components to be independently refreshed [74].", "metadata": {"uuid": "c2d9595c-b5d4-4599-b104-59916785d19f", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 184, "section_path": ["8 Discussion and Future Direction", "8.2 Implications for Proprietary Data"], "section_refs": ["#/texts/308", "#/texts/314"], "page_no": 25, "tokens": 73}}, {"text": "However, enterprise RAG deployments introduce new security concerns. The retriever could inadvertently expose confidential information if strict access controls are not enforced. Organizations must implement robust authentication mechanisms, ensuring that retrieved documents align with user permissions. Privacy-preserving techniques such as encrypted search indices and federated retrieval [58] are promising solutions to mitigate risks.", "metadata": {"uuid": "7fb04543-a896-41d7-a61f-f1172b3bf668", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 185, "section_path": ["8 Discussion and Future Direction", "8.2 Implications for Proprietary Data"], "section_refs": ["#/texts/308", "#/texts/314"], "page_no": 25, "tokens": 66}}, {"text": "The ability of RAG to function over proprietary knowledge bases has made it a preferred choice for industries handling sensitive information, including finance, healthcare, and legal sectors. As enterprises scale RAG systems, optimizing retrieval latency and ensuring regulatory compliance will be paramount.", "metadata": {"uuid": "f0c864a7-7617-40b2-9590-1a095d50ed98", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 186, "section_path": ["8 Discussion and Future Direction", "8.2 Implications for Proprietary Data"], "section_refs": ["#/texts/308", "#/texts/314"], "page_no": 25, "tokens": 50}}, {"text": "Key avenues for advancing RAG include:", "metadata": {"uuid": "4b5624c9-9b51-41a5-9c4e-7b1c0c3d48ca", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 187, "section_path": ["8 Discussion and Future Direction", "8.3 Future Research Directions"], "section_refs": ["#/texts/308", "#/texts/318"], "page_no": 25, "tokens": 8}}, {"text": "RAG is finding use in a range of new applications:", "metadata": {"uuid": "808f782d-40e3-4156-a5ed-cc7379412da5", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 188, "section_path": ["8 Discussion and Future Direction", "8.4 Emerging Practical Applications"], "section_refs": ["#/texts/308", "#/texts/326"], "page_no": 25, "tokens": 12}}, {"text": "synthesize answers from private knowledge with strong performance, scalability, and security guarantees [89].", "metadata": {"uuid": "04d7cd63-2340-407e-9d3e-cb64c047e33f", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 189, "section_path": ["8 Discussion and Future Direction", "8.4 Emerging Practical Applications"], "section_refs": ["#/texts/308", "#/texts/326"], "page_no": 26, "tokens": 18}}, {"text": "Retrieval-Augmented Generation (RAG) represents a fundamental advancement in AI, bridging retrieval and generation for improved factuality and adaptability. This review highlights the evolution of RAG from early retrieve-and-read systems to sophisticated architectures integrating neural retrievers and sequence-to-sequence generators. RAG has demonstrated significant benefits across open-domain QA, enterprise applications, and AI-powered search.", "metadata": {"uuid": "1c078867-67b3-4695-b328-258bfb11dddd", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 190, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 26, "tokens": 74}}, {"text": "While RAG enhances generative AI with dynamic knowledge retrieval, several challenges remain. Ensuring high-quality retrieval, handling conflicting retrieved evidence, and scaling retrieval mechanisms for large knowledge bases require further research.", "metadata": {"uuid": "68a84696-6ac4-438a-ac98-d088b4629fe9", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 191, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 26, "tokens": 39}}, {"text": "Additionally, security considerations in proprietary deployments necessitate privacy-preserving retrieval strategies.", "metadata": {"uuid": "d12b87c9-af46-4e0a-ae6e-defa89fc23ad", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 192, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 26, "tokens": 15}}, {"text": "Future research should focus on optimizing retrieval efficiency, refining document fusion strategies, and developing robust evaluation metrics for retrieval-augmented generation. The continued convergence of information retrieval and AI-powered text generation will define the next generation of intelligent assistants, transforming how users interact with digital knowledge.", "metadata": {"uuid": "026b5490-2ed9-42b6-be0e-6432da9b7e79", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 193, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 26, "tokens": 54}}, {"text": "This work (J.A and A.B) was supported by the University of Tennessee startup funding. The authors acknowledge the use of facilities and instrumentation at the UT Knoxville Institute for Advanced Materials and Manufacturing (IAMM) supported in part by the National Science Foundation Materials Research Science and Engineering Center program through the UT Knoxville Center for Advanced Materials and Manufacturing (DMR-2309083). We extend our gratitude to the faculty and staff of UT-ORII for their invaluable support.", "metadata": {"uuid": "2805d641-aaeb-4209-b636-92750ed2daca", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 194, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 26, "tokens": 94}}, {"text": "The authors confirm there is no conflict of interest.", "metadata": {"uuid": "2ae47095-767b-4e3e-95ec-a36204622acb", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 195, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 26, "tokens": 10}}, {"text": "The authors confirm that there is no conflict of interest. The authors confirm that there is no conflict of interest.", "metadata": {"uuid": "8938e8bb-c651-4f83-971c-1842194f6c27", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 196, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 26, "tokens": 22}}, {"text": "The authors confirm that there is no conflict of interest. The authors confirm that there is no conflict of interest.", "metadata": {"uuid": "2df3a1da-6788-49d8-b19e-9134c80f5d1e", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 197, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 26, "tokens": 22}}, {"text": "Katie Millican, George van den Driessche, Jean Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426 , 2022.", "metadata": {"uuid": "bb16a567-efdc-4457-a4cd-66186e3b6a17", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 198, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 27, "tokens": 59}}, {"text": "AAAI Spring Symposium on Generative AI for Social Science Research , 2024.", "metadata": {"uuid": "9714e9e3-df4a-4b1c-9d99-354a6bda5508", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 199, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 28, "tokens": 17}}, {"text": "[33] IBM Research. What is retrieval-augmented generation? Blog, 22 August 2023, 2023. https://research.ibm.com/blog/ retrieval-augmented-generation-RAG .", "metadata": {"uuid": "d501dddb-bdbd-47ad-9258-6eb374a777d2", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 200, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 42}}, {"text": "[34] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL) , pages 874-880, 2021.", "metadata": {"uuid": "11103166-3890-4924-a729-ffd6d475c4a1", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 201, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 62}}, {"text": "[35] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Peroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299 , 2022.", "metadata": {"uuid": "ec2b36d0-f72d-4e31-b39a-78b82116733d", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 202, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 83}}, {"text": "[36] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Peroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research , 24:1-43, 2023.", "metadata": {"uuid": "9be5b0ca-f17f-42f3-b68b-a7fbe778660d", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 203, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 81}}, {"text": "[37] Gautier Izacard, Xin Wan, Christoph Bohm, Kazuma Irie, Nathanael Scharli, and Sebastian Riedel. Unsupervised dense information retrieval with contrastive learning. Transactions of the Association for Computational Linguistics , 2022. arXiv:201.10672.", "metadata": {"uuid": "beb55045-0bd6-4d52-b7ea-8344c2d0a954", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 204, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 67}}, {"text": "[38] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 7969-7992, Singapore, December 2023. Association for Computational Linguistics.", "metadata": {"uuid": "2426c138-e843-40b3-9842-7bf98880bcda", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 205, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 102}}, {"text": "[39] Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan O Arik. Long-context llms meet rag: Overcoming challenges for long inputs in rag. In The Thirteenth International Conference on Learning Representations , 2024.", "metadata": {"uuid": "ed1212f6-1d91-4854-a81f-38e75eff97fa", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 206, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 56}}, {"text": "[40] Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin Jin. Ragcache: Efficient knowledge caching for", "metadata": {"uuid": "3fb81ce5-71d7-439b-8d8f-6be8b51abcee", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 207, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 40}}, {"text": "retrieval-augmented generation. arXiv preprint arXiv:2404.12457 , 2024.", "metadata": {"uuid": "e4744d9e-a342-4c15-9d8f-b31de8ff7ebf", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 208, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 26}}, {"text": "[41] Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Qiuxia Li, and Jun Zhao. Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models. In Proceedings of the 2024 Joint Conference on Computational Language Learning and Language Resources and Evaluation (LREC-COLING) , 2024.", "metadata": {"uuid": "c8e3ddb5-bc1d-41e8-9adc-b667ee890d20", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 209, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 86}}, {"text": "[42] Sheshananda Reddy Kandula. Securing retrieval-augmented generation: Privacy risks and mitigation strategies. SSRN Electronic Journal , 2025.", "metadata": {"uuid": "356c9ec9-5728-476c-adf5-3f4c6f9d310a", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 210, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 35}}, {"text": "[43] Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. Knowledge graph-augmented language models for knowledge-grounded dialogue generation (surge) . arXiv preprint arXiv:2305.18846 , 2023.", "metadata": {"uuid": "108a92d8-1b39-431b-a298-da5e9754d65f", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 211, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 61}}, {"text": "[44] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769-6781, 2020.", "metadata": {"uuid": "185feb24-8137-4d75-baf7-80a2862a42d9", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 212, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 84}}, {"text": "[45] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 6769-6781, Online, 2020. Association for Computational Linguistics.", "metadata": {"uuid": "5195d8e6-0883-4960-9e33-c5847fc3ba94", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 213, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 96}}, {"text": "[46] Taeyoun Kim, Jacob Springer, Aditi Raghunathan, and Maarten Sap. Mitigating bias in rag: Controlling the embedder. arXiv preprint arXiv:2502.17390 , 2025.", "metadata": {"uuid": "17372921-c21b-4c6a-b5cb-dbac1ecad611", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 214, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 55}}, {"text": "[47] Lilly Kumari, Usama Bin Shafqat, and Nikhil Sarda. Retrieval-augmented generation for dialog modeling. In Proceedings of the 37th International Conference on Neural Information Processing Systems (NeurIPS 2023) Workshops , 2023.", "metadata": {"uuid": "94c3f6bc-c204-4ac2-932c-572b78214e0f", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 215, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 29, "tokens": 58}}, {"text": "[48] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL) , pages 6086-6096, Florence, Italy, 2019. Association for Computational Linguistics.", "metadata": {"uuid": "34693d2f-924d-4610-9021-5a6418ea7ea1", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 216, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 30, "tokens": 72}}, {"text": "[64] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375 , 2023.", "metadata": {"uuid": "35815cdf-faf2-4a48-beec-1af2ea74d5c0", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 217, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 31, "tokens": 58}}, {"text": "Daniel Campos, Nick Craswell, and Jimmy Lin. Ragnarok: A reusable retrieval-augmented generation framework and baselines for the TREC 2024 RAG track. arXiv preprint arXiv:2406.16828 , 2024. Introduces the AutoNuggetizer evaluation and shows its high correlation with human judgments.", "metadata": {"uuid": "3ec25598-cf48-4923-a1e5-3e8839546a76", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 218, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 31, "tokens": 73}}, {"text": "Retrieval in the Asia Pacific Region , pages 12-22, 2024.", "metadata": {"uuid": "9551c7f4-ac5a-4065-b852-438c29776ff1", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 219, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 32, "tokens": 18}}, {"text": "Conference on Learning Representations (ICLR), Conference Track Proceedings , 2015.", "metadata": {"uuid": "e20daa73-af4e-456a-979a-f6c5523362b9", "file_path": "data/processed/docling/2507.18910v1.json", "chunk_index": 220, "section_path": ["9 Conclusion"], "section_refs": ["#/texts/334"], "page_no": 33, "tokens": 17}}]
