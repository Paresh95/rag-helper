[{"text": "University of Sydney Sydney kzho3733@uni.sydney.edu.au", "metadata": {"uuid": "302f06f7-ee95-4d8b-9afa-74158c62b59b", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["SEM RAG: SEMANTIC KNOWLEDGE-AUGMENTED RAG FOR IMPROVED QUESTION-ANSWERING *"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 14}}, {"text": "Basem Suleiman, Shijing Chen", "metadata": {"uuid": "d3c38685-9d06-4b5a-9753-61a50f91be0d", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["SEM RAG: SEMANTIC KNOWLEDGE-AUGMENTED RAG FOR IMPROVED QUESTION-ANSWERING *"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 8}}, {"text": "University of New South Wales", "metadata": {"uuid": "016069ff-27de-4e6e-8f5f-a31f1fdb7e00", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["SEM RAG: SEMANTIC KNOWLEDGE-AUGMENTED RAG FOR IMPROVED QUESTION-ANSWERING *"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 5}}, {"text": "Sydney", "metadata": {"uuid": "0cb5b364-1025-4b8e-b053-070de5c46b54", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["SEM RAG: SEMANTIC KNOWLEDGE-AUGMENTED RAG FOR IMPROVED QUESTION-ANSWERING *"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 1}}, {"text": "b.suleiman@unsw.edu.au, arthur.chen@unsw.edu.au", "metadata": {"uuid": "edec8f03-ff5a-4381-bf09-f57c56eab297", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["SEM RAG: SEMANTIC KNOWLEDGE-AUGMENTED RAG FOR IMPROVED QUESTION-ANSWERING *"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 19}}, {"text": "Abdelkarim Erradi Qatar erradi@qu.edu.qa", "metadata": {"uuid": "784bd83c-487a-48ff-9430-4350e8cc0951", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["SEM RAG: SEMANTIC KNOWLEDGE-AUGMENTED RAG FOR IMPROVED QUESTION-ANSWERING *"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 13}}, {"text": "This paper introduces SemRAG, an enhanced Retrieval Augmented Generation (RAG) framework that efficiently integrates domain-specific knowledge using semantic chunking and knowledge graphs without extensive fine-tuning. Integrating domain-specific knowledge into large language models (LLMs) is crucial for improving their performance in specialized tasks. Yet, existing adaptations are computationally expensive, prone to overfitting and limit scalability. To address these challenges, SemRAG employs a semantic chunking algorithm that segments documents based on the cosine similarity from sentence embeddings, preserving semantic coherence while reducing computational overhead. Additionally, by structuring retrieved information into knowledge graphs, SemRAG captures relationships between entities, improving retrieval accuracy and contextual understanding. Experimental results on MultiHop RAG and Wikipedia datasets demonstrate SemRAG has significantly enhances the relevance and correctness of retrieved information from the Knowledge Graph, outperforming traditional RAG methods. Furthermore, we investigate the optimization of buffer sizes for different data corpus, as optimizing buffer sizes tailored to specific datasets can further improve retrieval performance, as integration of knowledge graphs strengthens entity relationships for better contextual comprehension. The primary advantage of SemRAG is its ability to create an efficient, accurate domain-specific LLM pipeline while avoiding resource-intensive fine-tuning. This makes it a practical and scalable approach aligned with sustainability goals, offering a viable solution for AI applications in domain-specific fields.", "metadata": {"uuid": "9ebaeb0e-bdfb-416a-8b07-6b736a062ce2", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["SEM RAG: SEMANTIC KNOWLEDGE-AUGMENTED RAG FOR IMPROVED QUESTION-ANSWERING *"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 269}}, {"text": "By leveraging semantic chunking and knowledge graphs, SemRAG effectively enhances domain-specific LLM performance while minimizing computational demands, addressing a critical challenge in deploying advanced large language models efficiently.", "metadata": {"uuid": "9ebaeb0e-bdfb-416a-8b07-6b736a062ce2", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 1, "section_path": ["SEM RAG: SEMANTIC KNOWLEDGE-AUGMENTED RAG FOR IMPROVED QUESTION-ANSWERING *"], "section_refs": ["#/texts/1"], "page_no": 1, "tokens": 36}}, {"text": "The recent advances of Large Language Model (LLM) have demonstrated broad utility across sectors, but generalpurpose models often struggle with domain-specific accuracy due to limitations in contextual understanding and the prevalence of hallucinations [1]. Although fine-tuning offers improvements, it is constrained by substantial data requirements and high computational costs. Retrieval Augmented Generation addresses these challenges by integrating a retrieval mechanism that supplements pre-trained models with relevant external knowledge, thereby enabling more accurate and contextually appropriate responses. Retrieval Augmented Generation (RAG) is the process of retrieving relevant content from external knowledge and incorporating it into the LLM generation process to produce accurate, relevant, and up-to-date responses [2]. The current RAG process requires improvement in three critical areas: indexing, retrieval, and generation [3]. The indexing stage involves computationally intensive pre-processing and chunking, where", "metadata": {"uuid": "1b49201a-d745-46c2-bb58-79a0e1a57df2", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["1 Introduction"], "section_refs": ["#/texts/11"], "page_no": 1, "tokens": 170}}, {"text": "$^{*}$Citation: Kezhen Zhong. SEMRAG. Pages.... DOI:000000/11111.", "metadata": {"uuid": "af43b9b3-97f5-4a45-8dd0-465d878269e9", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["1 Introduction"], "section_refs": ["#/texts/11"], "page_no": 1, "tokens": 25}}, {"text": "Figure 1: SemRAG framework leveraging semantic chunking and knowledge graphs for enhanced contextual understanding; (a) Semantic Indexing: Segment the document into smaller chunks indexed and stored in a database. (b) Context Retrieval: Retrieve the top k community based on semantic similarity. (c) Knowledge Graph: Information is extracted to build the knowledge graph hierarchy with communities.", "metadata": {"uuid": "5e7c1747-3efc-4414-9256-541e78fe3672", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["1 Introduction"], "section_refs": ["#/texts/11"], "page_no": 2, "tokens": 74}}, {"text": "optimal chunk size presents a trade-off between noise reduction and context retention, thus affecting embedding quality. Inefficiencies in query matching and ranking often limit retrieval performance. Methods such as Hybrid Search and Semantic Ranking [4] can improve certain query types, reduce retrieval errors, and limit irrelevant results. Although re-ranking can be implemented to enhance accuracy, it adds computational overhead and risks degradation from noisy or redundant data. The generation stage struggles with integrating retrieved chunks into prompts, balancing coherence, and preventing hallucinations. Poor prompt design can misinterpret intent, while advanced techniques like few-shot prompting improve performance at the cost of complexity. Extensive context requirements further strain resources and risk suboptimal responses if the model priorities irrelevant details.", "metadata": {"uuid": "dedae000-b925-4970-978c-39de084ddc12", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["1 Introduction"], "section_refs": ["#/texts/11"], "page_no": 2, "tokens": 141}}, {"text": "Semantic chunking has emerged as a key trend for improving contextual performance in RAG pipelines. Research, including a study by Vectara, highlights the importance of optimizing chunking size during the indexing process to enhance RAG's effectiveness [5]. Semantic chunking stands out for its ability to maintain context and coherence, although it requires some computational resources. In contrast, methods like NLTK [6] and Spacy may fall short in accurately identifying sentence boundaries as they focus more on sophisticated rule base chunking that might be syntactically correct but semantically disjoint, on the other hand rule base chunking such as recursive chunking often struggles to preserve context and lead to fragmented and less meaningful chunks. Hence striking a balance between context preservation, semantic consistency, and computational efficiency is crucial for effective chunking strategies.", "metadata": {"uuid": "d03c2567-b83e-411a-aa4a-b8d7050f1f9d", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["1 Introduction"], "section_refs": ["#/texts/11"], "page_no": 2, "tokens": 164}}, {"text": "One promising approach is the integration of Knowledge Graph (KG) as it enables efficient access to structured and unstructured information. By organizing data into entities (nodes) and relationships (edges), KG provides a structured representation that enables precise, context-aware retrieval. It improves relevance through disambiguation, comprehensive summaries, and deeper topic connections, offering superior contextual understanding and intuitive", "metadata": {"uuid": "7d35aef6-a6f6-4178-9963-edf565d6ade8", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["1 Introduction"], "section_refs": ["#/texts/11"], "page_no": 2, "tokens": 72}}, {"text": "navigation [7]. This makes KG more effective than keyword-based systems, especially for linking entities and delivering a holistic view of data in knowledge-rich tasks.", "metadata": {"uuid": "9bcd2cf4-683e-454d-b5d1-5920b4ec0dc4", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["1 Introduction"], "section_refs": ["#/texts/11"], "page_no": 3, "tokens": 30}}, {"text": "This paper addresses the limitations of conventional RAG by introducing SemRAG Figure 1 , which integrates knowledge graphs and semantic chunking. SemRAG enhances LLM output relevance and contextual understanding through local/global community search and semantic indexing. The main contributions of this paper are as follows:", "metadata": {"uuid": "4520c170-c06f-4250-bff7-ea13fdc5cc3b", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["1 Introduction"], "section_refs": ["#/texts/11"], "page_no": 3, "tokens": 57}}, {"text": "While tradiational fine-tuning method offer efficient ways to adapt language models for specific tasks, they can be limited when handling knowledge-intensive or dynamic domains. PEFT relies on the model's pre-existing knowledge base, which may not suffice for domains requiring constant access to updated or specialized information [8]. Promptbased tuning can struggle in tasks where external, domain-specific knowledge is needed beyond what the model was pre-trained on [9]. Database augmentation addresses some of these limitations but may still face challenges in integrating and retrieving the most relevant information efficiently [10].", "metadata": {"uuid": "a2faf440-150f-43db-ba70-9b9361a969c2", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["2 Existing/Related Work", "2.1 Limitations of Traditional Fine-Tuning Methods"], "section_refs": ["#/texts/26", "#/texts/27"], "page_no": 3, "tokens": 109}}, {"text": "In contrast, RAG enhances a model's performance by dynamically retrieving relevant external information during both fine-tuning and inference. This allows RAG to incorporate up-to-date and domain-specific knowledge directly from external sources, making it more effective in knowledge-intensive tasks. By continuously accessing and integrating new information, RAG overcomes the limitations of static fine-tuning approaches, enabling more accurate and contextually appropriate outputs.", "metadata": {"uuid": "23a62c1c-d497-4efc-b3af-ffd4cbd11481", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["2 Existing/Related Work", "2.1 Limitations of Traditional Fine-Tuning Methods"], "section_refs": ["#/texts/26", "#/texts/27"], "page_no": 3, "tokens": 81}}, {"text": "Retrieval-Augmented Generation (RAG) represents a significant advancement in large language models (LLMs) by addressing the limitations of static training data and mitigating issues like hallucinations. Unlike traditional models such as GPT-3, which generate responses based solely on pre-trained knowledge, RAG dynamically retrieves external information, enhancing factual accuracy and contextual relevance. The RAG process consists of three main stages: indexing, retrieval, and generation. Indexing involves extracting, pre-processing, and embedding information into a vector database to facilitate efficient search and retrieval [2, 11]. During retrieval, a user's query is processed using various search methods, such as Hybrid Search [12] for quick responses, Semantic Ranking for nuanced queries, and Vector Search for context-rich domains [13]. The retrieved chunks are then ranked, often using re-ranking techniques to ensure relevance and reduce noise [14, 15]. Finally, the ranked information is incorporated into the prompt, guiding the LLM to generate well-informed responses. Prompt engineering techniques like zero-shot, few-shot, and chain-of-thought prompting further refine the generated output, minimizing hallucinations and improving coherence [16].", "metadata": {"uuid": "d4be7888-f610-4576-ab36-1373aa8d4523", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["2 Existing/Related Work", "2.2 Retrieval-Augmented Generation (RAG)"], "section_refs": ["#/texts/26", "#/texts/30"], "page_no": 3, "tokens": 229}}, {"text": "Despite its potential, optimizing RAG presents several challenges across indexing, retrieval, and generation. Indexing requires breaking documents into retrievable chunks, where aligning chunk sizes with user queries enhances specificity and accuracy [17]. Semantic chunking has emerged as a key trend for improving contextual performance in RAG pipelines [18]. Research, including a study by Vectara, highlights the importance of optimizing chunking size during the indexing process to enhance RAG's effectiveness [5]. Semantic chunking stands out for its ability to maintain context and coherence, although it requires greater computational resources. In contrast, methods like NLTK and Spacy may fall short in accurately identifying sentence boundaries, while recursive chunking often struggles to preserve context. Striking a balance between context preservation, semantic consistency, and computational efficiency is crucial for effective chunking strategies. One promising approach is the integration of Knowledge Graph (KG) as it enables efficient", "metadata": {"uuid": "f8c06868-14ee-4a26-b6a4-c2b3efb3d8a4", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["2 Existing/Related Work", "2.3 Limitation of Retrieval-Augmented Generation (RAG)"], "section_refs": ["#/texts/26", "#/texts/32"], "page_no": 3, "tokens": 182}}, {"text": "access to structured and unstructured information. By organizing data into entities (nodes) and relationships (edges), KG provides a structured representation that enables precise, context-aware retrieval. It improves relevance through disambiguation, comprehensive summaries, and deeper topic connections, offering superior contextual understanding and intuitive navigation [7]. This makes KG more effective than keyword-based systems, especially for linking entities and delivering a holistic view of data in knowledge-rich tasks.", "metadata": {"uuid": "5d935723-4bc6-4497-bb89-38441170c42c", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["2 Existing/Related Work", "2.3 Limitation of Retrieval-Augmented Generation (RAG)"], "section_refs": ["#/texts/26", "#/texts/32"], "page_no": 4, "tokens": 86}}, {"text": "GraphRAG enhances traditional RAG by integrating structured knowledge through knowledge graphs, modifying indexing, retrieval, and generation into graph construction, graph-guided retrieval, and graph-augmented response generation [19]. In the indexing phase, GraphRAG builds a graph database from sources, public knowledge graphs or proprietary data, mapping nodes, edges, and relationships to facilitate structured retrieval. The retrieval process identifies relevant graph elements-entities, paths, or subgraphs-by aligning user queries with graph semantics, ensuring precise information extraction. During generation, retrieved graph elements augment prompts, enabling the model to generate more contextually accurate and informed responses [20]. Microsoft's advancements in GraphRAG introduce techniques such as iterative entity extraction and building hierarchical community detection via the Leiden algorithm, improving response retrieval efficiency and accuracy [21]. By leveraging structured knowledge, GraphRAG enhances retrieval efficiency and the contextual depth of generated responses compared to traditional RAG methods.", "metadata": {"uuid": "1fdaf5c4-3329-4445-8ade-20512e48e898", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.1 Overview of Graph Retrieval-Augmented Generation"], "section_refs": ["#/texts/37", "#/texts/38"], "page_no": 4, "tokens": 184}}, {"text": "Inspired by Microsoft's Graph RAG [21], SemRAG advances the standard Retrieval-Augmented Generation (RAG) framework by integrating semantic indexing with knowledge graphs, thereby structuring data into semantically coherent and contextually rich chunks. As a result, it improves indexing and retrieval efficiency and has been shown to reduce hallucination in domain-specific applications such as finance [22]. At the core of this approach lies semantic chunking, a process that groups sentences based on semantic similarity while leveraging the hierarchical architecture of knowledge graphs to support both localized and global information retrieval. This method is designed to optimize computational performance without sacrificing contextual accuracy. By incorporating specialized chunking and summarization techniques within the Graph-RAG pipeline, SemRAG achieves significant reductions in processing time and resource consumption. This optimized mechanism effectively lowers the computational burden typically associated with knowledge graph-based operations, increasing the scalability and practical viability of language models in large-scale, complex data environments.", "metadata": {"uuid": "fac391fe-7664-4064-a964-40bc09918997", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG"], "section_refs": ["#/texts/37", "#/texts/40"], "page_no": 4, "tokens": 186}}, {"text": "The Semantic chunking implementation focuses on incorporating a semantic chunking methodology into a lightweight Graph RAG framework, due to its adaptability and reduced computational requirements. Efforts to integrate this approach into a more resource-intensive platform proved challenging, prompting a shift to a simpler system better suited for customization. By applying cosine similarity for chunking with buffer size, a parameter that determines the number of adjacent sentences combined around a central sentence to preserve contextual coherence-this process ensures semantic integrity within chunks. Additionally, by relying on locally hosted models, the process efficiently handles domain-specific texts while minimizing overhead. By integrating the robust chunking process, streamlined embeddings, and consistent evaluations, this setup achieves a balance between accuracy, scalability, and efficient computational power.", "metadata": {"uuid": "911caf17-0e7f-487b-93bd-c9b36fd2b0e0", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.1 Semantic Chunking"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/42"], "page_no": 4, "tokens": 144}}, {"text": "The semantic chunking process aims to enhance the contextual understanding and retrieval capabilities of large language models (LLMs). By dynamically grouping sentences based on semantic similarity, the method ensures cohesive and contextually rich chunks that respect token limits.", "metadata": {"uuid": "997c7cef-b0b2-469f-9419-adf46e3e91f1", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 4, "tokens": 45}}, {"text": "Cosine Distance adjacent n -neighbor sentences while respecting token limits. A semantic group chunk g is now defined as [23]:", "metadata": {"uuid": "53ba0102-ac60-47b5-b60d-1eddfdf88c3d", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 4, "tokens": 25}}, {"text": "g = \\left \\{ c _ { i } \\, \\Big | \\, ( d ( c _ { i } ) , d ( c _ { i + k } ) ) = 1 - \\frac { d ( c _ { i } ) \\cdot d ( c _ { i + k } ) } { \\| d ( c _ { i } ) \\| _ { 2 } \\, \\| d ( c _ { i + k } ) \\| _ { 2 } }", "metadata": {"uuid": "c7649ae6-ef5c-42c7-9197-914852e26aca", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 4, "tokens": 108}}, {"text": "where c$_{i}$ and c$_{i}$$_{+}$$_{k}$ are sentences or sentence groups within n neighborhood, d ( c ) represents the embedding of a sentence or group of sentences, and \u03c4 is the threshold for cosine distance to preserve semantic cohesion. If a chunk g exceeds the token limit of 1024, it is split into smaller, overlapping sub-chunks that should be within 128 tokens, this can effectively group semantic similar neighboring sentences into the same chunk:", "metadata": {"uuid": "44a38015-0809-4f20-bd99-bc7be70d130a", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 101}}, {"text": "g = \\bigcup _ { j = 1 } ^ { m } g _ { j } , \\quad \\text {where} \\, g _ { j } \\cap g _ { j + 1 } \\neq \\emptyset , \\, | g _ { j } | \\leq 1 0 2 4 , \\text { and } | g _ { j } \\cap g _ { j + 1 } | = 1 2 8 ,", "metadata": {"uuid": "9a214a61-adfa-4bcb-98e9-1e99ae781914", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 103}}, {"text": "where g$_{j}$ represents the j -th sub-chunk, ensuring overlap between adjacent sub-chunks g$_{j}$ and g$_{j}$$_{+}$$_{1}$ for contextual continuity.", "metadata": {"uuid": "fb595df4-99d8-4b61-b729-114f61dfe763", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 45}}, {"text": "Input: Document set D = { d$_{1}$, . . . , d$_{N}$ } ; threshold \u03b8 ; buffer size b ; token limit T$_{max}$", "metadata": {"uuid": "dd9e976d-c08a-41ce-b08e-1ac581793665", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 38}}, {"text": "Output: Chunk set C", "metadata": {"uuid": "3daac2d3-d7ea-4646-a1f9-d2719dbad093", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 5}}, {"text": "1 foreach d \u2208 D do", "metadata": {"uuid": "76ef03cc-1fef-45a0-ab3b-527cda8c6eef", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 7}}, {"text": "2", "metadata": {"uuid": "2be6ff66-16f7-4c66-8d7f-9af1b5d0ca9e", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "3", "metadata": {"uuid": "9869acec-72c6-4dff-89d4-620db02a90b2", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "4", "metadata": {"uuid": "40843b63-5ac1-4f4a-840a-e6e824b774f8", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "5", "metadata": {"uuid": "7365ae8b-6560-4e3a-ae09-38a051d5c9f6", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "6", "metadata": {"uuid": "eef688a6-f120-4dc9-b5eb-6b4a98b38cbf", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "7", "metadata": {"uuid": "6d38608a-67a3-47e3-8407-93b2b8119500", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "8", "metadata": {"uuid": "56874e9f-bbd0-4672-b66b-9af7a81afd55", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "9", "metadata": {"uuid": "f1b66372-ef6d-4d04-862d-f5d9abff1663", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "10", "metadata": {"uuid": "b32d5feb-d64b-401f-aeb0-1c3296c4f027", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "11", "metadata": {"uuid": "92a82439-2ac0-4f5f-b733-0dfee257b2bd", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "12", "metadata": {"uuid": "be3ed2b4-eb8b-4920-9c42-5b633b2a50b6", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "13", "metadata": {"uuid": "c1dcd4c5-bbaf-443d-81ac-e1632e917af1", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "14", "metadata": {"uuid": "4000da6b-9b75-4937-b5a5-e85578965d76", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "15", "metadata": {"uuid": "3587db42-1ce6-49f6-8830-9440fb69b717", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "16", "metadata": {"uuid": "999c9e2f-9497-427d-a3db-52c9e0980da5", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "17", "metadata": {"uuid": "307a8ed7-8cdd-4e6a-934e-6d88833b55a2", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "S \u2190 Split ( d ) // Sentences S = { s$_{1}$, . . . , s$_{m}$ }", "metadata": {"uuid": "ed2f7262-8697-4bfe-aa70-dcb1ad303d42", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 28}}, {"text": "\u02c6 S \u2190 BufferMerge ( S , b ) // Contextual merging", "metadata": {"uuid": "aeaefb8c-1a84-445b-99b2-ebc201c51e27", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 14}}, {"text": "Z \u2190 { z$_{i}$ = Embed ( \u02c6 s$_{i}$ ) } | \u02c6 S | i = 1 // LLM embeddings", "metadata": {"uuid": "b1b9eb2b-a62f-4cdb-969c-ad98add9d8da", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 33}}, {"text": "for i = 1 to | \u02c6 S | - 1 do", "metadata": {"uuid": "4e8829de-1310-4d47-93ce-4b42c1be410d", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 15}}, {"text": "d$_{i}$ \u2190 1 - cos ( z$_{i}$ , z$_{i}$$_{+}$$_{1}$ ) // Cosine distance", "metadata": {"uuid": "477fdc82-c6e8-4100-87b2-fd1e33b39da8", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 35}}, {"text": "c \u2190 [ ] , C$_{d}$ \u2190 \uf638 ;", "metadata": {"uuid": "80f43944-516a-4949-93cb-64599ca1073b", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 15}}, {"text": "for i = 1 to | \u02c6 S | do", "metadata": {"uuid": "238def38-ad8c-4130-8ce2-72ed8f6297b1", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 12}}, {"text": "if d$_{i}$", "metadata": {"uuid": "d5bee0c6-b6e7-42fe-9770-821c43a092cc", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 6}}, {"text": "Append \u02c6 s$_{i}$ to c ;", "metadata": {"uuid": "e20e7411-d28e-4ef5-82b4-07c76fc3a646", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 11}}, {"text": "else", "metadata": {"uuid": "6711ec86-7922-4fde-a2eb-7db64caaedd5", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 1}}, {"text": "C$_{d}$ \u2190 C$_{d}$ | { c } ; c \u2190 [ ] ;", "metadata": {"uuid": "c8428579-aea9-4496-9e41-71607b04a1b9", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 21}}, {"text": "foreach c \u2208 C$_{d}$ do", "metadata": {"uuid": "e04241c1-874c-4921-9c83-1b988ec126c1", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 10}}, {"text": "if Tokens ( c ) > T$_{max}$ then", "metadata": {"uuid": "204c892a-2bff-47f2-bb5a-ec0f6f0ad1f6", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 12}}, {"text": "c \u2190 SplitWithOverlap ( c )", "metadata": {"uuid": "79b57358-c906-4b54-9c53-c1dd7144cbbd", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 8}}, {"text": "C \u2190 C \u222a C$_{d}$ ;", "metadata": {"uuid": "8c01b7df-5dc4-41c7-ba82-8900ade5c0ce", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 11}}, {"text": "return C", "metadata": {"uuid": "271931ae-b6b5-442f-bb95-995f688dc2d0", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.2 Indexing"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/44"], "page_no": 5, "tokens": 2}}, {"text": "The enhanced Graph RAG pipeline involves selecting the most contextually relevant chunks and community reports from the constructed knowledge graph to answer user queries efficiently. By leveraging local and global RAG strategies, the system identifies key entities, communities, and their relationships, then ranks and filters potential matches against the user's prompt [21].", "metadata": {"uuid": "192b0fcb-8328-42cc-bfaf-8a88f95ad5dc", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 5, "tokens": 62}}, {"text": "Knowledge Graph Community Retrieval is technique, employed within GraphRAG [21] systems, enhances the contextual relevance of information retrieved in response to user queries. Rather than retrieving isolated facts or disconnected text passages, the system first constructs a knowledge graph that maps entities and their interrelationships, then applies community detection algorithms to identify clusters of semantically related entities and facts. When a query is issued, the system locates the most pertinent communities within the graph and extracts their summarized content. This approach ensures that the generated responses are not only contextually rich and interconnected but also semantically aligned with the user's intent.", "metadata": {"uuid": "4f4cc027-5dd3-46c8-a365-a9544bb7cc54", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 5, "tokens": 119}}, {"text": "Given a community C$_{i}$ = ( V$_{i}$, E$_{i}$ ) in a knowledge graph, the community summary report S ( C$_{i}$ ) is constructed as:", "metadata": {"uuid": "95ee26f2-afc4-468a-ba2c-9cf8824e124b", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 5, "tokens": 43}}, {"text": "S ( C _ { i } ) = \\text {LLM} _ { \\text {summarize} } \\left ( \\bigcup _ { v \\in V _ { i } } s ( v ) \\cup \\bigcup _ { ( v _ { j } , v _ { k } ) \\in E _ { i } } s ( v _ { j } , v _ { k } ) \\right ) ,", "metadata": {"uuid": "75fe7d2e-3411-4c04-8baf-e8b645234949", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 5, "tokens": 92}}, {"text": "where s ( v ) represents the summary of node (Entities) v \u2208 V$_{i}$ , s ( v$_{j}$, v$_{k}$ ) represents the summary of edge (Relationships) ( v$_{j}$, v$_{k}$ ) \u2208 E$_{i}$ , and LLM$_{summarize}$ is the large language model function used to aggregate and generate a coherent summary which later in retrieval process for better contextual searching.", "metadata": {"uuid": "0f2b5830-ce83-4909-89ec-0a6a91058ba7", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 6, "tokens": 98}}, {"text": "Local Graph RAG Search : The local search method identifies entities and text chunks relevant to a user query Q and optional history H , prioritizing contextually relevant information. The process is defined as:", "metadata": {"uuid": "5dc0cf97-5798-4bce-801f-247fbb21e28c", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 6, "tokens": 39}}, {"text": "\\mathcal { D } _ { \\text {retrieved} } = \\text {Top} _ { k } \\left ( \\{ v \\in \\mathcal { V } , g \\in \\mathcal { G } \\, | \\, \\text {sim} ( v , Q + H ) > \\tau _ { e } \\wedge \\text {sim} ( g , v ) > \\tau _ { d } \\} \\right )", "metadata": {"uuid": "e03c5d96-058b-44ab-bcac-618272787ed3", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 6, "tokens": 98}}, {"text": "where D$_{retrieved}$ contains top-ranked entities v and text chunks q from the knowledge graph V and Chunks G . Relevance is determined by similarity thresholds \u03c4$_{e}$, \u03c4$_{d}$ , with the result constrained to fit the pre-defined window size L .", "metadata": {"uuid": "9e640111-540b-428b-9716-2a264044630f", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 6, "tokens": 58}}, {"text": "Global Graph RAG Search : The global search method uses the top-K most important and central community reports to generate a response, prioritizing relevant and central information. The process is defined as:", "metadata": {"uuid": "4e2f2cfd-5bc7-44c3-9767-eca84fef5e25", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 6, "tokens": 38}}, {"text": "\\mathcal { D } _ { \\text {retrieved} } = \\text {Top} _ { k } \\left ( \\bigcup _ { r \\in \\mathcal { R } _ { \\text {Top} \\cdot K } ( Q ) } \\bigcup _ { c _ { i } \\in C _ { r } } \\left ( \\bigcup _ { p _ { j } \\in c _ { i } } ( p _ { j } , \\text {score} ( p _ { j } , Q ) ) \\right ) , \\text {score} ( p _ { j } , Q ) \\right )", "metadata": {"uuid": "3dab401f-78fb-4aaf-9b8a-cb2a07dc5967", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 6, "tokens": 140}}, {"text": "where D$_{retrieved}$ represents the top-K community reports selected based on their relevance to the query Q and centrality in the community hierarchy, C$_{r}$ is the set of text chunks from each report r . Within each chunk c$_{i}$ , p$_{j}$ represents the points (sub-pieces), and score ( p$_{j}$, Q ) indicates the relevance of each point p$_{j}$ to the query. The function Top$_{k}$ selects the top-K most relevant points based on their scores.", "metadata": {"uuid": "db004708-4160-4eff-9410-b1a50cb223e2", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 6, "tokens": 113}}, {"text": "Figure 2: SemRAG consist of two phases, semantic indexing and graph communities construction. (a) Semantic Chunking: long text has been split and segmented into meaningful chunks based on semantic relevance. (b) Graph Communities Construction: building a knowledge graph based on hierarchical community summarization.", "metadata": {"uuid": "9260aae2-95da-4921-bc51-96cab48a63c5", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 6, "tokens": 59}}, {"text": "As shown in figure 2, the semantic chunking algorithm works by first splitting the input text into individual sentences, then encoding each sentence into a vector using a pre-trained language model. It calculates the cosine similarity between each sentence and the current chunk to determine semantic closeness. If the similarity is high, the sentence is grouped with the current chunk; otherwise, a new chunk is started. This results in contextually meaningful groups of sentences. These chunks can then be used for tasks like entity extraction, summarization, and building knowledge graphs, enabling structured understanding of long, unstructured text.", "metadata": {"uuid": "0e2453e3-c73e-4b19-8409-8ebd37158677", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["3 Methodology", "3.2 SemRAG", "3.2.3 Retrieval"], "section_refs": ["#/texts/37", "#/texts/40", "#/texts/89"], "page_no": 6, "tokens": 118}}, {"text": "Datasets Two datasets are chosen to evaluate SemRAG enhancement to the Graph Rag pipeline; MultiHopRAG is a cross-domain QA dataset, which is designed to assess retrieval and reasoning across documents with associated meta data within RAG pipelines. It consists of 609 data corpus and 2,566 Q&A, each by evidence spanning 2 to 4 documents within the data corpus. The Q&A incorporate document metadata, representing complex, real-world scenarios often encountered in RAG applications $^{2}$[24]. RAG Mini Wikipedia is a lightweight version of a Wikipedia corpus, specifically designed for the RAG pipeline. It contains 918 Q&A pairs along with a large data corpus that is scattered across 3,200 entries. As a necessary pre-processing step, entries with similar content are merged before constructing the knowledge graph$^{3}$.", "metadata": {"uuid": "f59d216f-f78b-4fc0-8ecb-1236714dd6f9", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup"], "section_refs": ["#/texts/107"], "page_no": 7, "tokens": 171}}, {"text": "Table 1: Comparison of Chunking Metrics Across Buffer Sizes in MultiHop and Wiki Datasets", "metadata": {"uuid": "ae881fc1-f4d5-45ca-8d5b-f2b738986a39", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup"], "section_refs": ["#/texts/107"], "page_no": 7, "tokens": 19}}, {"text": "Models Selection To evaluate the performance of SemRAG pipeline, several models are adapted to evaluate the performance with the above data sets with Mistral (7B-Instruct-Q4_0)[25], Llama3 (8B-Instruct-Q4_0) [26], Gemma2(9BInstruct-Q3_K_M)[27]. The first experiment analysis the performance of each model from fixed size chunking to different semantic chunking size within the SemRAG pipeline and a deep comparison of the different models' performances with the SemRAG pipeline with semantic chunking size against baseline Naive RAG and baseline Graph RAG. In the second experiment, we further analysed the semantic chunking size impact on the LLMs' performance in different data corpus.", "metadata": {"uuid": "62a8038c-a078-4764-a095-0c09169ed1b0", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.1 Evaluation"], "section_refs": ["#/texts/107", "#/texts/110"], "page_no": 7, "tokens": 157}}, {"text": "Evaluation Metrics. Each models evaluated with Nomic-Embed-Text from Ollama for text embedding in knowledge construction and the baseline RAG vector database for compression [28]; llama3.2 for knowledge communities retrieval; ChatGPT-4-o Mini for evaluation metric to assess answer similarity, relevance, and correctness by comparing RAG-generated responses to ground-truth answers.", "metadata": {"uuid": "bc02c5b5-b6da-449e-a056-0ff355865592", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.1 Evaluation"], "section_refs": ["#/texts/107", "#/texts/110"], "page_no": 7, "tokens": 73}}, {"text": "Retrieval-Augmented Generation Assessment (RAGAS) is used to compare baseline RAG to SemiRAG, it is a metric framework to evaluate RAG-based models, focusing on Answer Correctness, Similarity, and Relevance to ensure factual accuracy and contextual coherence [29]. Answer Similarity computes cosine similarity between generated and ground-truth answers, while Answer Correctness balances factual and semantic overlap using an F1 score. Answer Relevance measures alignment by reverse-engineering the question from the generated response.", "metadata": {"uuid": "8ddf55b7-a967-4862-81a2-0f57e75cf0ac", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.1 Evaluation"], "section_refs": ["#/texts/107", "#/texts/110"], "page_no": 7, "tokens": 102}}, {"text": "As shown in Figure 3a, adding context significantly improves LLM performance, with the biggest jump from Buffer 0 to Buffer 2. LLaMA3 and Gemma2 outperform Mistral, with LLaMA3 excelling in correctness, as LLaMA3 with a large buffer is the best configuration, while Mistral with Global retrieval and a sufficient buffer is the most efficient alternative. Detail improvement can be seen in Figure 3b, as Mistral benefits significantly from retrieval strategies, with Global retrieval yielding the best results, while Naive retrieval performs the worst.", "metadata": {"uuid": "66805dca-e061-4b32-b933-17ac2397f08d", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.1 Evaluation"], "section_refs": ["#/texts/107", "#/texts/110"], "page_no": 7, "tokens": 120}}, {"text": "$^{2}$https://huggingface.co/datasets/yixuantt/MultiHopRAG", "metadata": {"uuid": "c87bedb9-73ef-438e-b4b9-65cee6bcf7ac", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.1 Evaluation"], "section_refs": ["#/texts/107", "#/texts/110"], "page_no": 7, "tokens": 21}}, {"text": "$^{3}$https://huggingface.co/datasets/rag-datasets/rag-mini-wikipedia", "metadata": {"uuid": "022d2005-182e-47fd-ab15-0171a16239f1", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.1 Evaluation"], "section_refs": ["#/texts/107", "#/texts/110"], "page_no": 7, "tokens": 21}}, {"text": "Figure 3: Comparison of LLMs and Mistral Performance with Different Buffer Sizes (AS = Answer Similarity, AR = Answer Relevancy, AC = Answer Correctness)", "metadata": {"uuid": "d018e99f-ea55-4aea-bd66-4da21d9b18b1", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.1 Evaluation"], "section_refs": ["#/texts/107", "#/texts/110"], "page_no": 8, "tokens": 38}}, {"text": "As shown in Figure 4, Both Gemma2 and Llama3 exhibit higher scores in answer correctness with the Semantic Chunking Size 0 compared to the Naive RAG approach. Furthermore, the semantic buffer size 0 has shown significant improvement in answer relevancy across all models, which indicates a robust semantic chunking size leading to a more cohesive, informative chunk for LLMs to interpret, resulting in increasing answer relevancy performances.", "metadata": {"uuid": "d4ad5804-5658-498c-94fa-08f08d39bcd0", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.2 Model Performance with SemRAG"], "section_refs": ["#/texts/107", "#/texts/120"], "page_no": 8, "tokens": 89}}, {"text": "Figure 4: RAGAS metrics comparison with Multi Hop-RAG (Naive RAG vs Semantic Chunking)", "metadata": {"uuid": "0ff7aec5-fbbf-45d9-9b6e-5762e7b3d891", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.2 Model Performance with SemRAG"], "section_refs": ["#/texts/107", "#/texts/120"], "page_no": 8, "tokens": 24}}, {"text": "Based on Figure 5, semantic Chunking with buffer size 0 generally achieves higher Answer Relevancy scores than Fixed-Size Chunking, especially for models like Llama3 and Mistral. However, Fixed-Size Chunking consistently outperforms Semantic Chunking in Answer Correctness for all models. At buffer size 5, Semantic Chunking exhibits a similar trend, with improved relevancy but lagging correctness compared to Fixed-Size Chunking.", "metadata": {"uuid": "3e2a8df7-11f7-40c7-b84e-0485be42ef63", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.2 Model Performance with SemRAG"], "section_refs": ["#/texts/107", "#/texts/120"], "page_no": 8, "tokens": 93}}, {"text": "Furthermore, the performance of Mistral in SemRAG highlights their substantial capabilities, with Local Method exhibiting great performance strengths across varying buffer sizes and configurations in figure 3b, showing the benefits of semantic chunking, with Fixed Size is best for correctness, while Semantic Chunk (5) boosts relevancy but slightly lowers correctness as more relevance information is being retrieved. However, specific configurations revealed significant limitations in semantic chunking, as some values for Llama 3 and Gemma dropped to zero with small sematic buffer size. This indicates that the models often failed to answer questions when smaller chunk sizes were used, due to", "metadata": {"uuid": "2926f593-50f9-496f-be15-2a8058a87cb7", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.2 Model Performance with SemRAG"], "section_refs": ["#/texts/107", "#/texts/120"], "page_no": 8, "tokens": 125}}, {"text": "Figure 5: RAGAS metrics comparison with Multi Hop-RAG (Fixed Size vs Semantic Chunking with Knowledge Graph Integration", "metadata": {"uuid": "afdcfcf4-9f2d-4cd7-b475-3bc6b5ac9ea3", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.2 Model Performance with SemRAG"], "section_refs": ["#/texts/107", "#/texts/120"], "page_no": 9, "tokens": 25}}, {"text": "insufficient information retrieval by the RAG. As a result, these models frequently generated \"Insufficient Information\" outputs, particularly in MultiHopRAG tasks that require the integration of multiple pieces of information. Finally, the data reveal the critical role of optimizing retrieval settings within RAG to ensure a balance between information sufficiency and specificity, ultimately supporting language models in generating accurate and reliable answers.", "metadata": {"uuid": "404b720a-44f8-416b-a536-762e8acd6aec", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.2 Model Performance with SemRAG"], "section_refs": ["#/texts/107", "#/texts/120"], "page_no": 9, "tokens": 78}}, {"text": "Figure 6: Buffer Size vs Graph Complexity", "metadata": {"uuid": "df297ce0-e80f-47ed-bae2-8bd730d52f5f", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.3 Semantic Chunking and Buffer Size"], "section_refs": ["#/texts/107", "#/texts/129"], "page_no": 9, "tokens": 9}}, {"text": "Figure 7 shows semantic chunking performance improves with buffer size, peaking at size 5 for both Answer Relevance and Correctness. This aligns with the dataset's structure-news articles with sub-100-word sentences-allowing optimal context preservation and minimal noise. At buffer size 5, semantic chunking outperforms the naive baseline by better capturing natural content segmentation and improving knowledge graph coherence. Future buffer tuning may be required to adapt to varying dataset profiles.", "metadata": {"uuid": "535d3d0c-6ef0-4661-b339-e393588c94e8", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.3 Semantic Chunking and Buffer Size"], "section_refs": ["#/texts/107", "#/texts/129"], "page_no": 9, "tokens": 93}}, {"text": "As shown in Figure 8, the Wiki Dataset, Answer Correctness peaks at buffer size 12 before gradually declining, indicating that excessively large buffers may reduce performance. Answer Similarity remains stable ( 0.8) across buffer", "metadata": {"uuid": "11a88e23-533d-4ff2-b23c-d1af3dcb4354", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.3 Semantic Chunking and Buffer Size"], "section_refs": ["#/texts/107", "#/texts/129"], "page_no": 9, "tokens": 46}}, {"text": "Figure 7: Performance of Buffer Sizes (0-10) vs Naive RAG Based on RAGAS Test Metrics Llama 3 (Multi-Hop)", "metadata": {"uuid": "33448236-8f5d-404c-9f57-d93bf872f3a3", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.3 Semantic Chunking and Buffer Size"], "section_refs": ["#/texts/107", "#/texts/129"], "page_no": 10, "tokens": 33}}, {"text": "Figure 8: Performance of Buffer Sizes (0-30) vs Naive RAG Based on RAGAS Test Metrics Llama 3 (wiki)", "metadata": {"uuid": "cef6ccbb-b47a-446f-8345-1825443b3cec", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.3 Semantic Chunking and Buffer Size"], "section_refs": ["#/texts/107", "#/texts/129"], "page_no": 10, "tokens": 32}}, {"text": "sizes for both semantic chunking and Naive RAG, suggesting minimal sensitivity to buffer variation. Answer Relevancy shows moderate fluctuations but stays near 0.7 for semantic chunking, with Naive RAG slightly lower. These results highlight buffer size as a key factor for correctness and relevancy, with limited impact on similarity", "metadata": {"uuid": "4d243aa0-85b1-45c7-a063-ce7ba880c64b", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.3 Semantic Chunking and Buffer Size"], "section_refs": ["#/texts/107", "#/texts/129"], "page_no": 10, "tokens": 66}}, {"text": "Across both datasets, buffer size demonstrates a positive correlation with answer quality (correctness, relevancy, and similarity) up to a critical threshold. In the initial range-buffer sizes 0-5 for Multi-hop and 0-10 for Wiki-models", "metadata": {"uuid": "385289ea-bf3e-4aa5-a9fb-92ac71dc89b0", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.3 Semantic Chunking and Buffer Size"], "section_refs": ["#/texts/107", "#/texts/129"], "page_no": 10, "tokens": 51}}, {"text": "Figure 9: Optimal Chunk Comparison (Mixtral) (Multi-hop vs Wiki Dataset)", "metadata": {"uuid": "0349474c-7363-48ac-bb0e-1f0395a87ad5", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.3 Semantic Chunking and Buffer Size"], "section_refs": ["#/texts/107", "#/texts/129"], "page_no": 11, "tokens": 18}}, {"text": "benefit from enriched contextual information that supports improved reasoning and content generation. However, beyond these optimal points, performance either plateaus or declines. This is likely due to information overload, where the generative model is burdened by excessive or irrelevant context, diminishing its ability to prioritize salient facts. Thus, optimal buffer sizing must be approached not as a maximization task, but as a balance between context richness and information sparsity.", "metadata": {"uuid": "c02b270a-794c-4fc7-a5b7-e3caec4bd597", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.3 Semantic Chunking and Buffer Size"], "section_refs": ["#/texts/107", "#/texts/129"], "page_no": 11, "tokens": 85}}, {"text": "Dataset structure plays a central role in determining optimal buffer configurations. In the Wiki corpus, where documents are often lengthy and context is distributed across many sentences, larger buffers are beneficial in providing necessary breadth. Conversely, the Multi-hop dataset features shorter, semantically denser passages, where smaller buffers help maintain focus and reduce the inclusion of irrelevant or redundant information. This distinction reinforces that maximizing buffer size is not universally effective; instead, it must be carefully aligned with the semantic and structural characteristics of the source corpus.", "metadata": {"uuid": "594981ab-fa41-41f7-8f30-9ad6a5149c92", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.3 Semantic Chunking and Buffer Size"], "section_refs": ["#/texts/107", "#/texts/129"], "page_no": 11, "tokens": 100}}, {"text": "The table 4 of RAGAS test metrics and time for knowledge graph construction within SemRAG across different buffer sizes reveals a clear trade-off between accuracy and efficiency. Larger buffers improve answer correctness and relevancy by providing more context, but they also significantly increase KGs constitution times. The data shows that moderate buffer sizes (around 4-7) achieve the best balance, yielding the highest correctness (~0.326) (buffer 7) and strong relevancy (~0.575) (Buffer 5) without excessive delays. In contrast, very large buffers lead to diminishing returns, with correctness slightly dropping due to potential context overload, while response time becomes impractically long. Therefore, choosing an optimal buffer size is crucial-too small sacrifices accuracy, while too large slows performance. A mid-range buffer provides the best trade-off, ensuring high-quality responses with reasonable speed.", "metadata": {"uuid": "a87627a0-82bd-41ef-bcbf-e1136d14ca8d", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.3 Semantic Chunking and Buffer Size"], "section_refs": ["#/texts/107", "#/texts/129"], "page_no": 11, "tokens": 176}}, {"text": "Hence, the above findings highlight that a one-size-fits-all approach to semantic buffer size is insufficient for optimizing GraphRAG performance. Buffer sizes must be carefully calibrated to the characteristics of the data corpus to avoid excessive content that may introduce noise and negatively impact both relevancy and correctness. The results indicate that the optimal chunk size varies based on the corpus's semantic density and structure. For dense, cohesive datasets, larger chunks can enhance performance by capturing interconnected information. In contrast, smaller chunks are more effective for less cohesive datasets, such as news articles, as they help reduce noise and less computational overhead [30]. While semantic chunking generally outperforms fixed-size methods in terms of retrieval relevance, its effectiveness is highly dependent on the semantic properties of the corpus. This is further supported by the findings of Derya et al., whose research demonstrates that dynamic chunking strategies-segmenting text based on semantic coherence rather than fixed lengths-significantly improve contextual integrity, resulting in more accurate retrieval and higher-quality generated responses across varied datasets [31]. Enhancing chunking with methods like semantic structuring or NLP techniques [6]can further improve performance, especially when the data structure is not well-suited to conventional chunking. Tailoring chunking strategies to the specific needs of each corpus boosts the accuracy and relevancy of the Graph RAG pipeline across diverse datasets.", "metadata": {"uuid": "1f80fb11-e7cb-48a3-acec-42afa2ee4a42", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["4 Experimental Setup", "4.3 Semantic Chunking and Buffer Size"], "section_refs": ["#/texts/107", "#/texts/129"], "page_no": 11, "tokens": 273}}, {"text": "This paper introduces SemRAG, a framework that enhances conventional RAG by integrating knowledge graph and semantic chunking algorithms. Semantic RAG (SemRAG) offers notable improvements over Knowledge Graph RAG (KG-RAG) by leveraging semantic chunking to enhance computational efficiency and retrieval accuracy. SemRAG segments documents into coherent chunks based on cosine similarity, preserving context while reducing redundancy, which significantly improves retrieval relevance and correctness. This approach outperforms KG-RAG in handling large datasets due to its lower computational overhead and better adaptability in resource-constrained environments. While KG-RAG excels in capturing intricate relationships between entities, SemRAG strikes a balance by achieving superior contextual understanding and answer relevancy without the complexity and scalability challenges associated with maintaining large knowledge graphs. Overall, SemRAG provides a more efficient and practical solution for domainspecific tasks. It offers a scalable and computationally efficient method for integrating domain-specific knowledge into LLMs, resulting in an 11% to 12% improvement in answer relevancy as illustrated in table 3. Compared to conventional RAG methods, SemRAG exhibits superior performance in answer relevancy, correctness, and similarity metrics in tested LLMs, particularly when employing optimized chunking sizes, as shown in Figure 4.", "metadata": {"uuid": "01937e73-9f7f-414c-832a-f91697fb6e16", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["5 Conclusion"], "section_refs": ["#/texts/150"], "page_no": 12, "tokens": 254}}, {"text": "Future work should explore lightweight approaches to integrating knowledge graphs in RAG systems to reduce computational overhead without compromising answer quality. The Nano Graph RAG pipeline used in this study exemplifies a customizable, resource-efficient solution. Emerging frameworks like Light RAG further streamline this approach with hybrid search methods to enhance retrieval efficiency and accuracy [32].", "metadata": {"uuid": "70048213-635d-465a-b94b-0e017be130e0", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["6 Future Work"], "section_refs": ["#/texts/152"], "page_no": 12, "tokens": 65}}, {"text": "Another key direction is developing a ground-truth metric for evaluating chunk boundaries, enabling more precise and cohesive data segmentation. This would improve semantic chunking by minimizing noise and optimizing chunk size for higher answer relevance and correctness.", "metadata": {"uuid": "2bcb2c07-1cfe-47db-9914-2814cc97cbb2", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["6 Future Work"], "section_refs": ["#/texts/152"], "page_no": 12, "tokens": 43}}, {"text": "Argentic chunking, which isolates atomic facts for entity-aware retrieval, offers improved precision over traditional semantic chunking. Though more computationally intensive, it enables fine-grained, context-rich retrieval when supported by advanced LLMs [33].", "metadata": {"uuid": "19bc6fc5-0901-4fb1-a2b9-9b0bc4dff508", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["6 Future Work"], "section_refs": ["#/texts/152"], "page_no": 12, "tokens": 48}}, {"text": "Overall, adapting chunking strategies and knowledge graph integration to the semantic structure of the data can yield more efficient, accurate, and scalable RAG systems [5].", "metadata": {"uuid": "363ec7d8-2f8f-4869-83f2-650749787595", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["6 Future Work"], "section_refs": ["#/texts/152"], "page_no": 12, "tokens": 32}}, {"text": "Firstly, I would like to express my deepest gratitude to Dr. Basem Suleiman, my thesis supervisor, for the invaluable guidance, support and encouragement throughout the entire thesis project. His insights and expertise were crucial to the completion of this thesis. I am also thankful to Arthur Chen from UNSW for providing the necessary resource and provide valuable feedback and insight on the bigger picture.", "metadata": {"uuid": "13dd9543-d0e1-4d37-a768-a2e01be9a21e", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["6 Future Work"], "section_refs": ["#/texts/152"], "page_no": 12, "tokens": 75}}, {"text": "To the academic staff at the School of Electrical and Computer Engineering, thanks for the teaching, guidance and the patient for my past 4 years in Sydney Uni.", "metadata": {"uuid": "77854dbf-234e-4eb8-b462-5f43290302ed", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["6 Future Work"], "section_refs": ["#/texts/152"], "page_no": 12, "tokens": 32}}, {"text": "Finally, I am sincerely grateful to my family members and friends for their unwavering support and encouragement throughout my undergraduate studies. Without their love and understanding, this thesis would not have been possible.", "metadata": {"uuid": "13cf2294-5f83-4e2a-881e-25db043657ab", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["6 Future Work"], "section_refs": ["#/texts/152"], "page_no": 12, "tokens": 37}}, {"text": "Table 2: Chunks, Graph Structure, and Processing Time for Mistral, Llama3, and Gemma2", "metadata": {"uuid": "1b48b658-fa60-4415-a5f7-dfe11343ce5e", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["7 Appendix"], "section_refs": ["#/texts/184"], "page_no": 15, "tokens": 26}}, {"text": "Table 4: Performance Metrics with different buffer size (Wiki Data). The best score is highlighted in blue and the longest time to construct a knowledge graph is denoted in red.", "metadata": {"uuid": "8672f22b-d11b-4275-aded-6e9c8cd8a27b", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["7 Appendix"], "section_refs": ["#/texts/184"], "page_no": 16, "tokens": 36}}, {"text": "Table 5: Performance Metrics with different buffer size (MultiHop). The best score is highlighted in blue and the longest time to construct a knowledge graph is denoted in red.", "metadata": {"uuid": "e4cc0157-a137-4eb5-9586-074fcb5403bb", "file_path": "data/processed/docling/2507.21110v1.json", "chunk_index": 0, "section_path": ["7 Appendix"], "section_refs": ["#/texts/184"], "page_no": 16, "tokens": 36}}]
