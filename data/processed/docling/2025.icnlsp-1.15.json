[{"uuid": "60aef500-9fdf-40c1-a3ba-a82d13fe1d56", "text": "Seemab Latif $^{\u2217}$, Huma Ameer$^{\u2020}$, Muhammad Hannan Akram\u2020 and Mehwish Fatima\u2020", "label": "text", "section_path": ["The Chunking Paradigm: Recursive Semantic for RAG Optimization"], "section_refs": ["#/texts/0"], "page_no": 1, "char_len": 81}, {"uuid": "e66709de-d3f0-4c8f-b14f-c4022272fceb", "text": "School of Electrical Engineering and Computer Science, National University of Sciences and Technology (NUST), Islamabad, Pakistan {seemab.latif, hameer.msds20seecs,makram.bds23seecs, mehwish.fatima}@seecs.edu.pk", "label": "text", "section_path": ["The Chunking Paradigm: Recursive Semantic for RAG Optimization"], "section_refs": ["#/texts/0"], "page_no": 1, "char_len": 211}, {"uuid": "aecf9289-94a4-40ea-bad3-89c3af301255", "text": "Retrieval Augmented Generation (RAG) has risen to prominence for boosting the capabilities of Large Language Models (LLMs) through the integration of external knowledge. Notably, the document chunking process plays a central role in the performance of RAG pipelines. Nevertheless, incoherent document splits and inappropriate chunk sizes hinder retrieval efficiency and contextual accuracy. To address this, we propose Recursive Semantic Chunking (RSC), a dynamic and adaptive chunking framework that ensures semantic coherence. It maintains coherence by recursively splitting large chunks and merging smaller ones. Unlike conventional methods, RSC preserves contextual integrity while optimizing retrieval efficiency. The evaluation across 4 distinct datasets outperformed traditional semantic chunking techniques on evaluation metrics; contextual relevancy, contextual precision, contextual recall, retrieval time, faithfulness and answer relevancy. Results demonstrate that RSC consistently outperforms traditional chunking techniques, achieving higher contextual relevancy and total score while maintaining efficient retrieval times. These findings highlight the potential to optimize RAG systems and to improve the document chunking steps in the systems.", "label": "text", "section_path": ["The Chunking Paradigm: Recursive Semantic for RAG Optimization"], "section_refs": ["#/texts/0"], "page_no": 1, "char_len": 1259}, {"uuid": "9b6f392c-5c0f-4e85-8cf5-4ba10b3f2448", "text": "Large Language Models (LLMs) are widely adopted across various domains in the form of chatbots, AI assistants, and other applications (Siddharth and Luo, 2024; Sahlman et al., 2023). The performance of LLMs is enhanced via the integration of external knowledge sources, specifically for custom applications. In addition, we can leverage the capabilities of LLMs without training them. The aforementioned enhancement can be made via", "label": "text", "section_path": ["1 Introduction"], "section_refs": ["#/texts/5"], "page_no": 1, "char_len": 431}, {"uuid": "1867fb33-c8ae-46b9-97d6-893f79437003", "text": "$^{\u2217}$Corresponding author.", "label": "footnote", "section_path": ["1 Introduction"], "section_refs": ["#/texts/5"], "page_no": 1, "char_len": 27}, {"uuid": "69da3794-423e-4483-9776-1fb2038e2349", "text": "$^{\u2020}$Equal contribution.", "label": "footnote", "section_path": ["1 Introduction"], "section_refs": ["#/texts/5"], "page_no": 1, "char_len": 25}, {"uuid": "c7dcf926-4f22-461e-a100-9ab67ae6afde", "text": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020).", "label": "text", "section_path": ["1 Introduction"], "section_refs": ["#/texts/5"], "page_no": 1, "char_len": 58}, {"uuid": "1cda2d26-75d0-435b-adce-c8423f00c9e4", "text": "The RAG process begins with a user's query being sent to the LLM, which generates a retrieval request based on that query. This request is forwarded to the retriever system, which searches the vector database. Embeddings of documents chunk i.e. context is stored in vector database. The relevant context is then retrieved and combined with the user's query before being sent to the LLM for a final response, as shown in Figure 1. Researchers have developed various RAG-based solutions across different domains, such as finance and healthcare (Alkhalaf et al., 2024; He et al., 2024; Feng et al., 2024; Mathur et al., 2024).", "label": "text", "section_path": ["1 Introduction"], "section_refs": ["#/texts/5"], "page_no": 1, "char_len": 623}, {"uuid": "07a00ebb-f0b1-4f1a-91b2-c6e56d687af1", "text": "The critical aspect of the RAG pipeline is the chunking of documents. Chunking in RAG systems is a technique that breaks down large documents into smaller, manageable segments known as \"chunks\" (LangChain, 2024). This process is crucial as it enhances the efficiency and accuracy of information retrieval, which leads to better outcomes for the system. The nature of context retrieved from the vector database is based on the segmentation of these documents, therefore, the choice of chunking techniques is a significant step in the pipeline (Setty et al., 2024). The chunking techniques directly affect the quality of retrievedcontext and retrieval time. It eventually affects the quality of the product that is utilizing RAGbased applications. The choice of chunking is quite challenging i.e. larger chunks can lead to slower retrieval, or retrieve irrelevant chunks and small chunks may not adhere to a coherent information unit. Recently, there has been a shift in research focus towards optimal chunking techniques i.e. (Yepes et al., 2024). Although frameworks such as LangChain (AI, 2024) and LamaIndex (Liu, 2022) have various chunking strategies. Due to complexities of the document structure, and cus-", "label": "text", "section_path": ["1 Introduction"], "section_refs": ["#/texts/5"], "page_no": 1, "char_len": 1211}, {"uuid": "96f24427-2ce7-4835-b4c1-c1692da41205", "text": "Figure 1: Information Flow in Retrieval Augmented Generation (RAG)", "label": "caption", "section_path": ["1 Introduction"], "section_refs": ["#/texts/5"], "page_no": 2, "char_len": 66}, {"uuid": "bdb5fda1-bf84-4ce0-94fd-551ad1e4bfa8", "text": "tom systems, it is still a challenging task at hand.", "label": "text", "section_path": ["1 Introduction"], "section_refs": ["#/texts/5"], "page_no": 2, "char_len": 52}, {"uuid": "be961ce3-fd71-4d0a-bc1c-ce6b870983d0", "text": "In this paper, we propose Recursive Semantic Chunking that focuses on optimizing the semantic chunking of the documents. The following are the key contributions of this work:", "label": "text", "section_path": ["1 Introduction"], "section_refs": ["#/texts/5"], "page_no": 2, "char_len": 174}, {"uuid": "a8c561ac-f393-41a8-b3c2-4e4809369363", "text": "Retrieval Augmented Generation systems rely on the context returned from the retrieval algorithms, making chunking a key factor in the pipeline (Yepes et al., 2024). Therefore, the choice of chunking strategies is a critical step. Ineffective techniques can result in either incomplete chunks leading to losing context or large chunks with irrelevant information negatively impacting the accuracy of the retrieval (Setty et al., 2024).", "label": "text", "section_path": ["2 Related Work"], "section_refs": ["#/texts/19"], "page_no": 2, "char_len": 435}, {"uuid": "52b47533-0a8f-4887-be8d-f5b9d367df7c", "text": "One of the common approaches is to split the document based on fixed numbers of chunks. However, it has a potential loss of context in both cases larger or smaller chunk size (Teja, 2023). To address this, the researchers introduce recursive split by character technique (LangChain, 2023). It recursively splits keeping the longest text chunks together with a need to define and constant adjustment of chunk size overlapping making it computationally expensive.", "label": "text", "section_path": ["2 Related Work"], "section_refs": ["#/texts/19"], "page_no": 2, "char_len": 461}, {"uuid": "e21b0385-4a02-407a-bb12-1bd535572411", "text": "Although the recursive text split tends to keep the chunks semantically closed together, it does not directly account for semantic meaning. Conversely, semantic chunking (LangChain, 2024) groups the text that is semantically similar together. It first splits the text into sentences and groups them into three sentences, then merges similar groups in the embedding space. However, this technique does not ensure optimal chunk sizes. Since its mechanism is dependent on the similarity of the embedding vectors, it may lead to larger chunks and cause hallucinations.", "label": "text", "section_path": ["2 Related Work"], "section_refs": ["#/texts/19"], "page_no": 2, "char_len": 564}, {"uuid": "5fd4f9f2-ded8-4cf2-9057-1423b51a91c9", "text": "Agentic chunking (FullStackRetrieval, 2024) pushed this idea further by leveraging Large Language Models. It converts text into propositions via LLMs (Chen et al., 2024). Propositions are defined as standalone statements that convey a single fact clearly without needing extra context. It can be referred to as the smallest unit of meaning within a text, each expressing one distinct idea. Propositions retain the semantic meaning in individual statements as shown in the following example:", "label": "text", "section_path": ["2 Related Work"], "section_refs": ["#/texts/19"], "page_no": 2, "char_len": 490}, {"uuid": "46bea3d5-e61c-4fd8-9e00-7f1254143331", "text": "Once the propositions are created, these are passed to an LLM, which is then prompted to group these chunks. This approach offers flexibility and higher accuracy. Nevertheless, it requires wellcrafted prompts and dependency on the capability of the acquired LLM.", "label": "text", "section_path": ["2 Related Work"], "section_refs": ["#/texts/19"], "page_no": 2, "char_len": 262}, {"uuid": "ac5edd2d-fb02-42ed-8c5b-47129da884bd", "text": "Working on efficient chunking techniques is an open research area as not much has been explored in this regard.", "label": "text", "section_path": ["2 Related Work"], "section_refs": ["#/texts/19"], "page_no": 2, "char_len": 111}, {"uuid": "566cf283-7566-40ef-9ddd-808cd1e8c32a", "text": "\"Three new products were launched this year, expanding our reach into international markets.\"", "label": "text", "section_path": ["2 Related Work"], "section_refs": ["#/texts/19"], "page_no": 3, "char_len": 93}, {"uuid": "6ae0b6b8-ef55-40ee-8cf8-3197432e23fa", "text": "We introduce a new large-scale news dataset, named NewsMatrix-71$^{1}$. It covers a diverse set of news categories over multiple years.", "label": "text", "section_path": ["3 Dataset"], "section_refs": ["#/texts/31"], "page_no": 3, "char_len": 135}, {"uuid": "32699e9a-90f7-4b8e-b164-1328a2df1915", "text": "We compile this dataset by scraping English news articles from Dawn$^{2}$, Tribune$^{3}$, and Daily Times$^{4}$. This dataset covers the span of three years (2021-2023) and has up to 96,859 news articles categorized into 71 unique topics, including Business, Fashion, Health, World, and more. It offers a diverse, time-spanning, and category-rich corpus suitable for various NLP tasks. It captures a broad spectrum of global and regional news, making it a valuable resource for research. Given the size and scope of this dataset, we will selectively release a publicly available subset to facilitate reproducibility and further research.", "label": "text", "section_path": ["3 Dataset", "3.1 Scraped News Dataset (NewsMatrix-71)"], "section_refs": ["#/texts/31", "#/texts/33"], "page_no": 3, "char_len": 637}, {"uuid": "ee95abd1-30ab-4f7e-be4c-3250076dcd69", "text": "This section presents the Recursive Semantic Chunking framework in detail. The primary objective is to ensure the splitting of chunks is semantically coherent and maintains the integrity of the content. In addition, the size of the chunks should be optimal. The standard semantic chunking technique tends to generate large chunks, which", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 3, "char_len": 336}, {"uuid": "cf4331ad-c0d7-4d30-8ba7-b4b08e8e5835", "text": "$^{1}$This data will be published publicly and free for research purposes after the paper\u2019s acceptance. It will be shared under the Creative Commons Attribution 4.0 International License (CC BY 4.0)", "label": "footnote", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 3, "char_len": 198}, {"uuid": "70294999-33ea-46d8-916d-874ca4768ffb", "text": "$^{2}$Dawn", "label": "footnote", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 3, "char_len": 10}, {"uuid": "4708cc96-dcae-4d18-b2d3-2d4de05e92e7", "text": "$^{3}$Tribune", "label": "footnote", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 3, "char_len": 13}, {"uuid": "822d7779-4b28-485e-97d2-8a58f02c05cf", "text": "$^{4}$Daily Times", "label": "footnote", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 3, "char_len": 17}, {"uuid": "c71748f9-389c-40cc-b607-ed4e5bb6e062", "text": "negatively impact the performance of retrievalaugmented generation systems. Furthermore, in custom RAG projects, documents often belong to specific topics, and larger chunks reduce system efficiency.", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 3, "char_len": 199}, {"uuid": "47be9844-f242-4091-8884-74b2ed4116d6", "text": "Algorithm 1 provides a detailed outline of the proposed chunking process. All predefined values are determined after extensive experimentation. The following steps describe the pipeline.", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 3, "char_len": 186}, {"uuid": "7e38efd2-d738-4f93-8533-e47202beb9dd", "text": "The data store consists of files f$_{i}$ containing textual data stored as strings T$_{i}$ . Since LLMs have token limits, each T$_{i}$ undergoes a length check. If it exceeds the threshold T$_{max}$ , the file is split into smaller segments { t$_{1}$, t$_{2}$, . . . , t$_{n}$ } , ensuring that | t$_{j}$ | \u2264 T$_{max}$ . The splitting occurs at the nearest sentence boundary (e.g., full stop, question mark) to preserve linguistic coherence.", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 3, "char_len": 442}, {"uuid": "116557c4-8c53-4834-91e3-cd7f3f931047", "text": "Each segment t$_{j}$ undergoes an initial semantic chunking process (LangChain, 2024). In this step, the semantically similar texts are grouped in the embedding space, forming C$_{0}$ = { c$_{1}$, c$_{2}$, . . . , c$_{m}$ } , where c$_{k}$ represents an initial chunk.", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 3, "char_len": 268}, {"uuid": "44069895-f761-46e2-9ccb-f8a22872638e", "text": "For each chunk c$_{k}$ \u2208 C$_{0}$ , the semantic chunker is recursively applied if its length exceeds the threshold T$_{chunk}$ (1,500 characters). With each recursive iteration, the breakpoint threshold parameter is gradually reduced, ensuring that large chunks are broken into smaller, semantically meaningful segments. The recursive function R ( c, T ) operates as follows:", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 3, "char_len": 375}, {"uuid": "a2520c38-9025-4e5d-8c32-665b49887d43", "text": "R ( c , T ) = \\begin{cases} c & \\text {if $|c|\\leq T$} \\\\ R ( \\text {split} ( c , T - \\delta ) , T - \\delta ) & \\text {if $|c|\\geq T$} \\end{cases}", "label": "formula", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 3, "char_len": 146}, {"uuid": "01c2c6de-bc03-436c-9c61-0accec211f84", "text": "where \u03b4 represents a small reduction factor to progressively decrease chunk size in each iteration. The reduction factor \u03b4 is heuristically set to 3 after initial experimentation. Although not tuned through systematic search, this value is selected to ensure a gradual and controlled recursive breakdown of large chunks. This value is kept fixed across all datasets to maintain consistency and reproducibility.", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 3, "char_len": 410}, {"uuid": "36310d0c-ea23-4790-91ee-4843f5f00986", "text": "Following recursive chunking, some chunks may become too short (i.e., less than T$_{merge}$ , set to 350 characters). Extremely small chunks may lack semantic coherence, leading to information loss. To address this, the similarity score of smaller chunks is calculated with both preceding and subsequent chunks. It is merged with the chunk that has the highest similarity score. This ensures semantic integrity while preventing the loss of meaningful text. The merging process is defined as follows:", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 499}, {"uuid": "63260640-5382-4622-a9aa-41094f77e76c", "text": "\\text {For} \\, i = 1 \\, \\text {to} \\, n \\, \\colon \\, \\begin{cases} \\text {If} \\, | c _ { i } |", "label": "formula", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 94}, {"uuid": "f644968e-8a1b-4841-b342-e58c3e8b1c33", "text": "Here, S$_{prev}$ and S$_{next}$ represent the similarity scores between the small chunk c$_{i}$ and its neighboring chunks c$_{i}$$_{-}$$_{1}$ and c$_{i}$$_{+1}$ , respectively. The chunk c$_{i}$ is merged with the chunk that has the highest similarity score, ensuring that the resulting merged chunk maintains semantic coherence.", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 330}, {"uuid": "bb19b056-3d7f-42b7-835b-f82138ee68cb", "text": "Finally, the algorithm checks whether any chunk exceeds the threshold T$_{final}$ (2,500 characters). If a chunk surpasses this limit, it undergoes a recursive character-based text split (LangChain, 2023). The final adjustment process is defined as:", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 249}, {"uuid": "2be1eaa7-a375-4275-8df3-d2309a48acd8", "text": "\\text {For} \\, i = 1 \\, \\text {to} \\, m \\, \\colon \\, \\begin{cases} \\text {If} \\, | c _ { i } | > T _ { \\text {final} } \\, \\colon \\\\ \\text {Apply Recursive Split Function} \\, \\colon \\\\ \\text {c} _ { i } \\, \\text {Re} \\, \\text {RecursiveSplit} ( c _ { i } , T _ { \\text {final} } ) \\end{cases}", "label": "formula", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 291}, {"uuid": "634ae349-754a-474b-8938-fbc2ba342d77", "text": "This step ensures that the final chunk set, C$_{final}$ = { c$_{1}$, c$_{2}$, . . . , c$_{m}$ } , meets size constraints while maintaining semantic coherence. The processed chunks are then stored in vector databases for RAG tasks.", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 230}, {"uuid": "eb5489a5-79a3-4d0c-af99-53097418d253", "text": "While our method incorporates components from existing LangChain utilities, i.e. semantic chunking for initial grouping and character-based recursive splitting for final chunk size enforcement. These steps function as structural helpers rather than the core of our approach. The key innovation of RSC lies in its intermediate refinement", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 336}, {"uuid": "8987f73c-adbd-44b1-8c9b-18d13b7d1f68", "text": "Algorithm 1: Recursive Semantic Chunking", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 40}, {"uuid": "ceb06643-a2f9-4db7-a1fc-e8c6c3fbd1b6", "text": "Input: Dataset D = { f$_{1}$ , f$_{2}$ , . . . , f$_{N}$ } Maximum chunk size T$_{max}$ = 15 , 000; Recursive chunking threshold T$_{chunk}$ = 1 , 500; Final chunk size threshold T$_{final}$ = 2 , 500; Minimum chunk size for merging T$_{merge}$ = 350 Output: Final set of chunks C$_{final}$", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 290}, {"uuid": "534bbf8a-2d7b-40d4-b1de-e8ce10d3ace2", "text": "7", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 1}, {"uuid": "395c7a0f-ee20-42c7-881b-2a406b65eaf2", "text": "8", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 1}, {"uuid": "d275ce8c-3567-408e-95e0-6364ea2abfa0", "text": "9", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 1}, {"uuid": "1eb99c87-635b-4542-b69f-13859d59494c", "text": "10", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "fe039851-e462-4ecc-9d85-3f4a74e4af32", "text": "11", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "1e23a252-592f-4746-8c63-2be2dc6afb6d", "text": "12", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "6815465b-2e50-4719-a67a-99774c4611ad", "text": "13", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "ceec57e0-3ea3-4ad6-ab90-9ab00f45818a", "text": "14", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "caffacb6-b9c8-431c-a538-a07d0a658857", "text": "15", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "2c145b7e-7aac-4f77-8a63-19a6b2735778", "text": "16", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "bf45c726-dcae-41ee-8d27-270e6eb95dbe", "text": "17", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "b96357db-17e6-43b7-9362-d7335ab6fd3c", "text": "18", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "c4872e9e-d8f8-42b5-8bde-03539a189dce", "text": "19", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "bf9f4458-ed47-414e-b053-703efbe9160b", "text": "20", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "ce8d33c9-a179-4f4a-9322-dd2f8fa1b6f6", "text": "21", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "2e1c1352-a7c5-4722-821e-2f3ca268c18b", "text": "22", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "6da1d78c-5187-40c0-b2b7-13f206099c51", "text": "23", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "e28d7a13-2e20-4297-9d67-e4f1f16e8f45", "text": "24", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "47fe7ee1-cc82-48d2-872c-974a29f99bd1", "text": "25", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "a1524ad5-0e47-44e9-9883-d44d56996897", "text": "26", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "867de6c0-3dd6-49b8-9f27-4b2668724ad6", "text": "27", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "5c9413b6-c8ef-4696-894c-0fd22f73eedc", "text": "28", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 2}, {"uuid": "9145dfe4-65bb-442b-8622-354dd18e6da9", "text": "foreach chunk c$_{k}$ \u2208 C$_{0}$ do if | c$_{k}$ | \u2264 T$_{merge}$ then Compute similarity with previous chunk: S$_{prev}$ \u2190 similarity ( c$_{k}$$_{-}$$_{1}$ , c$_{k}$ ) Compute similarity with next chunk: S$_{next}$ \u2190 similarity ( c$_{k}$ , c$_{k}$$_{+1}$ ) if S$_{prev}$ \u2265 S$_{next}$ then Merge with previous chunk: c$_{k}$$_{-}$$_{1}$ \u2190 c$_{k}$$_{-}$$_{1}$ + c$_{k}$ else Merge with next chunk: c$_{k}$$_{+}$$_{1}$ \u2190 c$_{k}$ + c$_{k}$$_{+1}$", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 441}, {"uuid": "4ae36aca-fd2c-4c33-a53d-fb4397ae2b87", "text": "logic: recursive breakdown with dynamic thresholds, similarity-based merging of smaller chunks, and controlled preservation of semantic coherence. These operations are not present in the baseline LangChain chunkers and are designed to address the limitations of fixed-size or purely embeddingbased segmentation. Therefore, while we leverage LangChain for low-level chunk initialization and splitting, the significant performance improvements observed in contextual and answer-level metrics stem from our recursive and adaptive chunking strategy.", "label": "text", "section_path": ["4 Recursive Semantic Chunking"], "section_refs": ["#/texts/35"], "page_no": 4, "char_len": 545}, {"uuid": "ca888605-4a88-47f0-b067-8c71e21bad78", "text": "Our evaluation framework is designed to rigorously assess the impact of our proposed technique: Recursive Semantic Chunking (RSC). Incorporating RSC in the RAG pipeline for question-answering tasks, we demonstrate its capabilities in preserving contextual coherence and improving retrieval precision. This section details our evaluation methodology, covering dataset selection, synthetic question", "label": "text", "section_path": ["5 Experimental Design"], "section_refs": ["#/texts/93"], "page_no": 4, "char_len": 396}, {"uuid": "4166c108-d91b-4327-b019-a61f2660f8d6", "text": "Table 1: Summary of Datasets used for Evaluating the Proposed Chunking Technique, including Open-source Corpora and the Custom Dataset NewsMatrix-71.", "label": "caption", "section_path": ["5 Experimental Design"], "section_refs": ["#/texts/93"], "page_no": 5, "char_len": 149}, {"uuid": "fa9cbe76-49b4-4e2d-a737-29d9cab9c079", "text": "generation, chunking techniques, implementation setup, and performance metrics", "label": "text", "section_path": ["5 Experimental Design"], "section_refs": ["#/texts/93"], "page_no": 5, "char_len": 78}, {"uuid": "0abd3b11-4953-436c-97d6-b525ab18e1b8", "text": "We evaluate our proposed chunking technique using four datasets, including three open-source corpora-BBC (Greene and Cunningham, 2006), SQuAD (Rajpurkar et al., 2016), and QuaC (Choi et al., 2018)-along with a custom-scraped news dataset, NewsMatrix-71. The NewsMatrix-71 dataset, created by scraping English news articles, is stored in .txt format. For experimentation, we use a 1,500-article subset containing 677,258 words and 4,227,679 characters. A summary of all datasets is provided in Table 1.", "label": "text", "section_path": ["5 Experimental Design", "5.1 Datasets"], "section_refs": ["#/texts/93", "#/texts/97"], "page_no": 5, "char_len": 501}, {"uuid": "8489fc15-735b-4705-9f24-eb599efd8254", "text": "These evaluations of the chunking techniques are based on the response from the question-answering system. Therefore, we utilized LLM to create synthetic questions from each dataset. For each dataset, we randomly generate 50 synthetic questions per dataset to balance computational feasibility with evaluation diversity. This quantity is consistent with recent study Merola and Singh, 2025. This quantity is consistent with recent study Merola and Singh, 2025. To generate synthetic questions, we randomly selected passages from each dataset. To ensure reasonable topic coverage, we manually examined multiple random subsets and selected one for question generation. While this approach does not guarantee perfect topic stratification, it provides a practical balance between topic diversity and simplicity in sampling. We employ Gemini Flash 1.5 to generate corresponding questions. The ChatPromptTemplate module from LangChain is used to structure the input prompt, guiding the model to generate relevant and contextaware questions for each passage. Once generated, the synthetic questions are stored and later used to assess the retrieval and response quality of different chunking techniques. By introducing synthetic", "label": "text", "section_path": ["5 Experimental Design", "5.2 Synthetic Question Generation"], "section_refs": ["#/texts/93", "#/texts/99"], "page_no": 5, "char_len": 1221}, {"uuid": "a0a61566-ac49-490f-90b7-fe27f33afae1", "text": "queries, we create an additional layer of evaluation that allows us to measure how well-chunked text segments support question-answering tasks beyond the scope of existing datasets.", "label": "text", "section_path": ["5 Experimental Design", "5.2 Synthetic Question Generation"], "section_refs": ["#/texts/93", "#/texts/99"], "page_no": 5, "char_len": 181}, {"uuid": "faf7e64a-22ec-4d1f-b662-c621715de6fb", "text": "To establish a baseline, we implement three widely used chunking techniques. Recursive Character Text Splitter segments (LangChain, 2023), and Semantic Chunking (LangChain, 2024). Next, we employ our proposed technique; Recursive Semantic Chunking framework for comparison.", "label": "text", "section_path": ["5 Experimental Design", "5.3 Chunking Techniques"], "section_refs": ["#/texts/93", "#/texts/102"], "page_no": 5, "char_len": 273}, {"uuid": "302ea102-8ab2-4173-9ef8-b1ca31e1e68a", "text": "For downstream question-answering tasks, we store the chunks in the RAG pipeline using LangChain$^{5}$. All the techniques use \" all-MiniLML6-v2 \" $^{6}$embedding. The resulting chunks are stored in the Facebook AI Similarity Search (FAISS) vector database (Douze et al., 2024). The \" ChatPromptTemplate module \" is used with \" Gemini Flash 1.5 \" $^{7}$, a state-of-the-art Large Language Model optimized for contextual reasoning.", "label": "text", "section_path": ["5 Experimental Design", "5.4 Implementation Details"], "section_refs": ["#/texts/93", "#/texts/104"], "page_no": 5, "char_len": 430}, {"uuid": "b550cf4e-2549-487d-955d-54fcfad0e1e8", "text": "We assess chunking techniques by integrating them into the RAG pipeline for a question-answering task. For evaluation, we use DeepEval by Confident AI $^{8}$, an open-source framework designed for LLM evaluation. DeepEval leverages LLMs and other NLP models to measure performance. In our study, GPT-3.5-turbo generates answers, with evaluation metrics focusing on contextual accuracy and relevance in both retrieval and generation stages. The following formulas are taken from DeepEval for evaluation. Additionally, we compare retrieval time across different strategies.", "label": "text", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 5, "char_len": 571}, {"uuid": "00f40521-0810-4eae-a88f-07deeb006685", "text": "It measures how well relevant nodes are ranked higher in the retrieval context.", "label": "text", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 5, "char_len": 79}, {"uuid": "31eb9888-cc90-4cdf-9b77-f5f9c65ceded", "text": "C P = \\frac { 1 } { \\text {Rel. Nodes} } \\sum _ { k = 1 } ^ { n } \\left ( \\frac { \\text {Rel. Nodes to } k } { k } \\times r _ { k } \\right )", "label": "formula", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 5, "char_len": 140}, {"uuid": "e1b6555b-3681-46e9-8346-a00fa47a149b", "text": "where r$_{k}$ is 1 for relevant nodes, 0 otherwise.", "label": "text", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 5, "char_len": 51}, {"uuid": "c72d2adc-1d58-4608-bce2-6da1028d67bc", "text": "$^{5}$LangChain", "label": "footnote", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 5, "char_len": 15}, {"uuid": "066e0595-6334-492e-b285-9f8bd91531ad", "text": "$^{6}$Sentence Embedding: all-MiniLM-L6-v2", "label": "footnote", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 5, "char_len": 42}, {"uuid": "3fad5c78-12b0-4f6d-8ee7-85db0401d573", "text": "$^{7}$Gemini Flash 1.5", "label": "footnote", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 5, "char_len": 22}, {"uuid": "9b8fd293-decf-416b-b11c-a67c9ce7f870", "text": "$^{8}$https://www.confident-ai.com", "label": "footnote", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 5, "char_len": 34}, {"uuid": "fbe88ea6-9fca-4b1b-b2fb-20160482c5d4", "text": "The metric evaluates the ability of the system to capture relevant information:", "label": "text", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 6, "char_len": 79}, {"uuid": "ac7c1973-b335-4bd6-b1d2-67681e4f4558", "text": "C R = \\frac { \\text {Attributable Statements} } { \\text {Total Statements} }", "label": "formula", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 6, "char_len": 76}, {"uuid": "93a54e7b-dbc7-4596-973f-f87b8ad5bb25", "text": "It measures the overall relevance of the retrieval context with respect to the query:", "label": "text", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 6, "char_len": 85}, {"uuid": "951ffee9-18cf-4b68-a8a1-1100f4e9d86b", "text": "C R e l = \\frac { R e l e v a n t S t a t e m e n t s } { T o t a l S t a t e m e n t s }", "label": "formula", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 6, "char_len": 89}, {"uuid": "87b80230-a827-4c0d-90c0-83be2b6f8e79", "text": "Answer Relevancy evaluates the relevance of the generated output:", "label": "text", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 6, "char_len": 65}, {"uuid": "59a56d46-1248-4c73-a729-811bb9f4d90b", "text": "A R = \\frac { R e l e v a n t S t a t e m e n t s } { T o t a l S t a t e m e n t s }", "label": "formula", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 6, "char_len": 85}, {"uuid": "c956b82b-1663-4654-b325-71e07e87625e", "text": "Faithfulness measures how factually accurate the output is:", "label": "text", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 6, "char_len": 59}, {"uuid": "649ce7f6-79ef-4a4a-a79c-9806126a4a14", "text": "\\text {Faithfulness} = \\frac { \\text {Trutful Claims} } { \\text {Total Claims} }", "label": "formula", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 6, "char_len": 80}, {"uuid": "a9297e3f-8a34-41c2-9523-c18124b53365", "text": "The Retrieval Time RT is defined as the total time taken to retrieve the context and generate the final answer for a query:", "label": "text", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 6, "char_len": 123}, {"uuid": "583f4eb0-3d15-4ecd-8a95-8bbc11a9e276", "text": "R T = t _ { \\text {end} } - t _ { \\text {start} }", "label": "formula", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 6, "char_len": 49}, {"uuid": "3305edeb-192e-4226-86c6-d6fdf503f6d7", "text": "These evaluation metrics allow us to compare the trade-offs between semantic integrity, retrieval effectiveness, and computational efficiency across different chunking approaches.", "label": "text", "section_path": ["5 Experimental Design", "5.5 Evaluation metrics"], "section_refs": ["#/texts/93", "#/texts/106"], "page_no": 6, "char_len": 179}, {"uuid": "7e937a49-fbdd-47ba-bb77-4ed943ac2d9f", "text": "Table 2 shows the chunk counts for different techniques. RSC achieves the best balance between granularity and coherence. In contrast, the Recursive Character Text Splitter generates the highest number of chunks due to its character-based splitting, while Semantic Chunking produces the fewest, resulting in larger segments. This balance reflects an important trade-off in RAG system design. Excessive chunking can inflate the retrieval space, leading to fragmented context. While larger chunks provide broader context, they increase the risk of irrelevant retrieval, hallucinations,", "label": "text", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 6, "char_len": 583}, {"uuid": "fda16b25-3e07-457a-b58d-32fa5cce7b98", "text": "Table 2: Number of Chunks Formed by Each Chunking Method Across Datasets.", "label": "caption", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 6, "char_len": 73}, {"uuid": "e4f5c74d-8a07-4d19-9ce7-5211374fe9c1", "text": "and longer retrieval times. RSC finds a middle ground, ensuring semantic integrity while maintaining meaningful chunk sizes. By keeping the chunk count within an optimal range, RSC improves contextual relevancy, as further supported by the downstream performance metrics in Table 3.", "label": "text", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 6, "char_len": 282}, {"uuid": "53b0f6ad-f7c8-4b10-8bd8-c61d95245212", "text": "Table 3 presents the comparative performance of chunking techniques on the question-answering task across multiple datasets. The proposed Recursive Semantic Chunking consistently outperforms other techniques, particularly in Contextual Relevancy and Total Score, while maintaining an optimal balance between chunk size and retrieval efficiency.", "label": "text", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 6, "char_len": 344}, {"uuid": "8d96dd63-24ee-4f85-8871-80b845f36cc5", "text": "The performance of chunking techniques across the datasets reveals interesting trends as shown in Figure 2. The best results are observed in SQuAD and NewsMatrix-71. SQuAD, achieving the highest Total Score under RSC, highlights the advantage of semantically coherent segmentation in structured question-answering datasets. NewsMatrix-71 achieves the highest Contextual Relevancy with RSC, demonstrating its effectiveness in handling diverse and large-scale articles.", "label": "text", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 6, "char_len": 467}, {"uuid": "28885a5d-a293-4f4c-9a8f-8751c5056aee", "text": "In contrast, QuAC performs the worst, particularly under Semantic Chunking and Recursive Semantic Chunking. This is likely due to its conversational nature, which demands deeper contextual understanding.", "label": "text", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 6, "char_len": 203}, {"uuid": "3634c454-e408-43a6-acad-90bd2be1af5a", "text": "While RSC does not lead in Answer Relevancy across all datasets, it is an important metric for evaluating end-to-end RAG performance. It consistently achieves top performance in Total Score and Contextual Relevancy. It is important to note that Answer Relevancy may be influenced by factors beyond chunking quality, such as the formulation of user queries (Sclar et al., 2024) or reasoning behavior of the language model during generation (Jiang et al., 2025). In contrast, Contextual Relevancy more directly reflects the quality and alignment of retrieved content with the query, making it a", "label": "text", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 6, "char_len": 592}, {"uuid": "b49831af-d94b-47fa-8202-7725ce485f9c", "text": "Table 3: Performance Metrics for Different Chunking Techniques Across Datasets. Scores are out of 50, except Total Score (out of 250). Retrieval time is measured in seconds.", "label": "caption", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 7, "char_len": 173}, {"uuid": "b3ca768a-54c0-4085-b527-a53bd3fd8347", "text": "stronger indicator of chunking effectiveness.", "label": "text", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 7, "char_len": 45}, {"uuid": "42653ead-acf4-4ba8-ba34-b92ab94cd157", "text": "Overall, among the chunking techniques, RSC achieves the highest Total Score across all datasets. The recursive breakdown mechanism in RSC ensures that large chunks do not negatively impact RAG tasks. Additionally, Contextual Relevancy improves significantly with RSC, as evident in datasets like BBC News (11.56) and NewsMatrix-", "label": "text", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 7, "char_len": 329}, {"uuid": "a6797cb1-4313-430f-9425-9fe7e0cf5987", "text": "71 (19.83), demonstrating its capability to maintain semantic coherence.", "label": "text", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 7, "char_len": 72}, {"uuid": "e114886d-120a-4812-ac54-d4b40b9cc376", "text": "These findings suggest the impact of the type and structure of data on the chunking techniques. However, in comparison, RSC is the most effective among the baseline chunking techniques.", "label": "text", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 7, "char_len": 185}, {"uuid": "c82003e1-eabd-441c-a223-0beb0e2bee3b", "text": "Figure 2: Performance Comparison of Chunking Techniques Across Datasets", "label": "caption", "section_path": ["6 Results and Analysis", "6.1 Results"], "section_refs": ["#/texts/132", "#/texts/133"], "page_no": 7, "char_len": 71}, {"uuid": "34d5bb48-a01c-4565-b6df-cfaee8344006", "text": "To evaluate the impact of Recursive Semantic Chunking on retrieval efficiency and chunk coherence, we conduct performance analysis across multiple datasets. The evaluation uses datasets of varying structures such as structured question-answering datasets (SQuAD, QuAC) and unstructured large-scale datasets (BBC News, NewsMatrix-71). It ensures that our findings are generalizable across multiple RAG tasks.", "label": "text", "section_path": ["6 Results and Analysis", "6.2 Analysis"], "section_refs": ["#/texts/132", "#/texts/147"], "page_no": 7, "char_len": 407}, {"uuid": "ebdb48f7-9713-4025-84b3-0aff8241504e", "text": "We conduct a study to analyze the effect of propositional segmentation incorporated in our proposed chunking technique. The hypothesis is that propositional segmentation enhances Contextual Relevancy.", "label": "text", "section_path": ["6 Results and Analysis", "6.2 Analysis"], "section_refs": ["#/texts/132", "#/texts/147"], "page_no": 7, "char_len": 200}, {"uuid": "801562af-f754-4c0d-921d-f8bde41d3fc5", "text": "To validate our hypothesis, we experiment by including propositional segmentation in RSC and compare the results. For this case study, we employ the BBC News dataset. The comparison of results is presented in Table 4", "label": "text", "section_path": ["6 Results and Analysis", "6.2 Analysis"], "section_refs": ["#/texts/132", "#/texts/147"], "page_no": 7, "char_len": 216}, {"uuid": "9ebf5d78-08cf-427d-89ca-9825533d53b4", "text": "The results confirm that propositional segmentation improves Contextual Relevancy (11.56 to 16.09). However, it is to be that improvement comes at the cost of increased retrieval time (from 0.716s to 0.8183s). In addition, it also has a computational overhead to convert all the sentences into propositions before they can be passed on for", "label": "text", "section_path": ["6 Results and Analysis", "6.2 Analysis"], "section_refs": ["#/texts/132", "#/texts/147"], "page_no": 7, "char_len": 339}, {"uuid": "4f6b40fd-ba75-41df-a1b8-9301e1551281", "text": "Table 4: Comparison of RSC with and without Propositional Segmentation on BBC News Dataset.", "label": "caption", "section_path": ["6 Results and Analysis", "6.2 Analysis"], "section_refs": ["#/texts/132", "#/texts/147"], "page_no": 8, "char_len": 91}, {"uuid": "761af8c0-0ae4-4745-adba-1eaade6e3ad3", "text": "chunking. However, it is an interesting area of study for the future.", "label": "text", "section_path": ["6 Results and Analysis", "6.2 Analysis"], "section_refs": ["#/texts/132", "#/texts/147"], "page_no": 8, "char_len": 69}, {"uuid": "d33cb3b8-46c5-4bdc-b4f7-fa98f775eb19", "text": "Although not included as a formal baseline, we initially explored Agentic Chunking to assess the viability of LLM-based chunking pipelines. However, due to its high computational demand, it is excluded from comparative evaluation. Details of Agentic Chunking are mentioned in Section 2. Since the Agentic approach operates at the propositional level, so for this technique, on average, each proposition requires 6 to 7 calls to the LLM for chunk assignment and metadata updates. To start with, we use this technique on the BBC dataset. The dataset contained more than 75,000 propositions, but after 8 hours of processing, only 1,500 propositions were successfully assigned to chunks. Due to the high computational overhead, we discontinued the experimentation. Hence, high computational cost makes this approach impractical for large-scale datasets.", "label": "text", "section_path": ["6 Results and Analysis", "6.2 Analysis"], "section_refs": ["#/texts/132", "#/texts/147"], "page_no": 8, "char_len": 849}, {"uuid": "2a60d76f-4d1b-45b9-821e-502882c1e15c", "text": "Despite its inefficiencies, Agentic Chunking may become viable in the future as LLMs improve in speed and affordability. However, for now, RSC provides a far more efficient and scalable solution.", "label": "text", "section_path": ["6 Results and Analysis", "6.2 Analysis"], "section_refs": ["#/texts/132", "#/texts/147"], "page_no": 8, "char_len": 195}, {"uuid": "bf80e676-73b9-48e7-ad2c-1899659bfa5f", "text": "The results and analysis confirm that RSC enhances retrieval efficiency and semantic coherence. Additionally, our findings highlight a new direction with propositional segmentation, which improves Contextual Relevancy. Overall, RSC consistently outperforms both Recursive Character Text Splitter and Semantic Chunking in Total Score and Contextual Relevancy, making it the preferred approach for RAG generation pipeline. Moving forward, future work will focus on optimizing propositional segmentation to reduce retrieval time, ensuring that the benefits of enhanced semantic coherence do not come at the expense of computational overhead.", "label": "text", "section_path": ["6 Results and Analysis", "6.2 Analysis"], "section_refs": ["#/texts/132", "#/texts/147"], "page_no": 8, "char_len": 638}, {"uuid": "a4b48ba1-00f9-42e9-b1da-cdf30d67986e", "text": "Our work offers a targeted contribution to optimizing the chunking process in RAG-based systems. The proposed technique, Recursive Semantic Chunking maintains a balance between retrieval efficiency and context relevancy. The novelty of RSC lies in the recursive nature of the proposed method dynamically adjusting the chunk size and going beyond the traditional approaches. The results, evaluated against the traditional techniques i.e. recursive character split, semantic and agentic techniques highlight the superiority of the proposed methodology. Additionally, its robustness is validated across structured question-answering datasets and unstructured large-scale datasets, with evaluation based on relevancy, retrieval quality, and time efficiency. The evaluation is based on relevancy, retrieval quality and time efficiency. These findings have significant implications for RAGbased applications such as medical, finance, legal, and education etc. Looking forward, the retrieval time will be further optimized with respect to Recursive Semantic Chunking on varied datasets.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 8, "char_len": 1079}, {"uuid": "120f1d03-4d89-476c-a333-7a99a1dc2afd", "text": "The scope of this study is limited to textual data, and it can be widened to more complex document types which may include tables, codes etc. In addition, Recursive Semantic which depends on propositions provides a new direction. However, its high computational cost, despite yielding improved results, highlights the need for a more efficient and scalable approach.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 8, "char_len": 366}, {"uuid": "8a32932d-d646-4094-8532-815208c783b8", "text": "LangChain AI. 2024. Langchain. GitHub repository.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 8, "char_len": 49}, {"uuid": "d669d272-d588-4ca3-b4c0-973f9d96196c", "text": "Mohammad Alkhalaf, Ping Yu, Mengyang Yin, and Chao Deng. 2024. Applying generative ai with retrieval augmented generation to summarize and extract key clinical information from electronic health records. Journal of Biomedical Informatics , 156:104662.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 8, "char_len": 251}, {"uuid": "8594d328-3c68-4b0a-a0ec-6ad1f04fb7f4", "text": "Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu. 2024. Dense X retrieval: What retrieval granularity should we use? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , pages 15159-15177, Miami, Florida, USA. Association for Computational Linguistics.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 8, "char_len": 342}, {"uuid": "0eb7d514-a642-4754-9f80-b941f8ddca59", "text": "Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wentau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. QuAC: Question answering in context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 2174-2184, Brussels, Belgium. Association for Computational Linguistics.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 319}, {"uuid": "09ce4e4b-4b6b-4cc3-9fb0-183b56c0e11f", "text": "Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazar'e, Maria Lomeli, Lucas Hosseini, and Herv'e J'egou. 2024. The faiss library. Preprint , arXiv:2401.08281.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 206}, {"uuid": "fb0c0b54-b982-4f08-aa58-0a4a0d5f2c50", "text": "Ruitao Feng, Xudong Hong, Mayank Jobanputra, Mattes Warning, and Vera Demberg. 2024. Retrievalaugmented modular prompt tuning for low-resource data-to-text generation. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) , pages 14053-14062, Torino, Italia. ELRA and ICCL.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 360}, {"uuid": "a6838901-c9bc-45c7-ab6c-dab798773031", "text": "FullStackRetrieval. 2024. Agentic chunker. Accessed: 2024-09-14.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 64}, {"uuid": "32102fb5-b298-4566-86f6-0238ed51ee11", "text": "Derek Greene and P'adraig Cunningham. 2006. Practical solutions to the problem of diagonal dominance in kernel document clustering. In Proceedings of the 23rd International Conference on Machine Learning , ICML '06, page 377-384, New York, NY, USA. Association for Computing Machinery.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 285}, {"uuid": "dbf6a771-2998-424c-9d86-8cda2118330e", "text": "Shiming He, Yu Hong, Shuai Yang, Jianmin Yao, and Guodong Zhou. 2024. Demonstration retrievalaugmented generative event argument extraction. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) , pages 4617-4625, Torino, Italia. ELRA and ICCL.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 331}, {"uuid": "e4e7af7c-14e8-4416-ae0c-b7d6dc2f2ce0", "text": "Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, and Bing Qin. 2025. GainRAG: Preference alignment in retrieval-augmented generation through gain signal synthesis. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 10746-10757, Vienna, Austria. Association for Computational Linguistics.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 359}, {"uuid": "02282007-6e6a-421c-b5d9-bf52a478ed5e", "text": "LangChain. 2023. Recursively split by character. Accessed: 2024-09-14.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 70}, {"uuid": "3b7efcc3-5ace-4346-9e72-aa924796f966", "text": "LangChain. 2024. Langchain documentation. Accessed: 2024-10-31.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 63}, {"uuid": "1763f50a-6f01-4c42-88eb-b9cd970bf01a", "text": "LangChain. 2024. Semantic chunker. Accessed: 202409-14.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 55}, {"uuid": "88cd5142-e327-4397-a8d4-461f2107f178", "text": "Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 195}, {"uuid": "e5a6e9f9-4324-4fd5-9c86-aba681de8915", "text": "Retrieval-augmented generation for knowledgeintensive nlp tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems , NIPS '20, Red Hook, NY, USA. Curran Associates Inc.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 211}, {"uuid": "23603f24-4735-486f-a8e1-3af8af7cf184", "text": "Jerry Liu. 2022. LlamaIndex.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 28}, {"uuid": "b74fac29-4448-4871-ac3b-d782c28792a1", "text": "Puneet Mathur, Zhe Liu, Ke Li, Yingyi Ma, Gil Karen, Zeeshan Ahmed, Dinesh Manocha, and Xuedong Zhang. 2024. DOC-RAG: ASR language model personalization with domain-distributed co-occurrence retrieval augmentation. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) , pages 5132-5139, Torino, Italia. ELRA and ICCL.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 405}, {"uuid": "2144c833-f814-44e4-8d4a-6fff3724dc44", "text": "Carlo Merola and Jaspersinder Singh. 2025. Reconstructing context: Evaluating advanced chunking strategies for retrieval-augmented generation. Preprint , arXiv:2504.19754.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 171}, {"uuid": "9bd0ce56-8226-4dbc-b45b-9bee7614c9f3", "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pages 2383-2392, Austin, Texas. Association for Computational Linguistics.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 300}, {"uuid": "078f630e-b4c0-45b3-93e4-2faa3a5a7d7f", "text": "WA Sahlman, AM Ciechanover, and E Grandjean. 2023. Khanmigo: Revolutionizing learning with genai. Harvard Business School Case , pages 824-059.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 143}, {"uuid": "484e3934-5f81-44b5-a3c4-0c36689281ce", "text": "Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2024. Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. Preprint , arXiv:2310.11324.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 233}, {"uuid": "a719e7bb-b2b6-4cf3-b39d-042e2fde4475", "text": "Spurti Styeth, Katherine Jijo, Eden Chung, and Natan Vidra. 2024. Improving retrieval for rag based question answering models on financial documents.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 149}, {"uuid": "02530df1-24e1-4f83-a9c6-2280a1e3d33b", "text": "L. Siddharth and Jianxi Luo. 2024. Retrieval augmented generation using engineering design knowledge. Knowledge-Based Systems , 303:112410.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 139}, {"uuid": "100ed47c-6c26-4cc6-93f5-0cc20fa3e5d1", "text": "R. Teja. 2023. Evaluating the ideal chunk size for a rag system using llamindex. Accessed: 2024-09-14.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 102}, {"uuid": "a9f378f0-5675-4ab8-b1cf-80a3c258b3f2", "text": "Antonio Jimeno Yepes, Yao You, Jan Milczek, Sebastian Laverde, and Renyu Li. 2024. Financial report chunking for effective retrieval augmented generation. Preprint , arXiv:2402.05131.", "label": "text", "section_path": ["7 Conclusion"], "section_refs": ["#/texts/159"], "page_no": 9, "char_len": 183}]
