{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79ac1af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "import qdrant_client\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.retrievers.fusion_retriever import QueryFusionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ee70c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "aclient = AsyncQdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    collection_name=\"my_collection\",\n",
    "    client=client,\n",
    "    aclient=aclient,\n",
    "    enable_hybrid=True,\n",
    "    batch_size=20,  # controls sparse batch processing\n",
    "    fastembed_sparse_model=\"prithivida/Splade_PP_en_v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4ac3f5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prithivida/Splade_PP_en_v1\n",
      "prithvida/Splade_PP_en_v1\n",
      "Qdrant/bm42-all-minilm-l6-v2-attentions\n",
      "Qdrant/bm25\n",
      "Qdrant/minicoil-v1\n"
     ]
    }
   ],
   "source": [
    "from fastembed import SparseTextEmbedding\n",
    "for i in SparseTextEmbedding.list_supported_models():\n",
    "    print(i['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1ab8f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_retriever = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    # Optional but recommended:\n",
    "    normalize=True,       # cosine similarity friendly\n",
    "    embed_batch_size=32,  # tune for your hardware\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5963b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    top_n=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dd896fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=dense_retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "163da000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerank score: 8.9718\n",
      "Similarity score: 0.6959\n",
      "Text: As shown in figure 2, the semantic chunking algorithm works by first splitting the input text into individual sentences, then encoding each sentence into a vector using a pre-trained language model. It calculates the cosine similarity between each sentence and the current chunk to determine semantic closeness. If the similarity is high, the sentence is grouped with the current chunk; otherwise, a new chunk is started. This results in contextually meaningful groups of sentences. These chunks can then be used for tasks like entity extraction, summarization, and building knowledge graphs, enabling structured understanding of long, unstructured text.\n",
      "--------------------------------------------------\n",
      "Rerank score: 0.3592\n",
      "Similarity score: 0.6069\n",
      "Text: The Semantic chunking implementation focuses on incorporating a semantic chunking methodology into a lightweight Graph RAG framework, due to its adaptability and reduced computational requirements. Efforts to integrate this approach into a more resource-intensive platform proved challenging, prompting a shift to a simpler system better suited for customization. By applying cosine similarity for chunking with buffer size, a parameter that determines the number of adjacent sentences combined around a central sentence to preserve contextual coherence-this process ensures semantic integrity within chunks. Additionally, by relying on locally hosted models, the process efficiently handles domain-specific texts while minimizing overhead. By integrating the robust chunking process, streamlined embeddings, and consistent evaluations, this setup achieves a balance between accuracy, scalability, and efficient computational power.\n",
      "--------------------------------------------------\n",
      "Rerank score: -0.1306\n",
      "Similarity score: 0.6622\n",
      "Text: The semantic chunking process aims to enhance the contextual understanding and retrieval capabilities of large language models (LLMs). By dynamically grouping sentences based on semantic similarity, the method ensures cohesive and contextually rich chunks that respect token limits.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1) Retrieve (embedding similarity)\n",
    "base_retriever = index.as_retriever(similarity_top_k=10)\n",
    "initial_nodes = base_retriever.retrieve(\"As shown in figure 2, the semantic chunking algorithm works by first splitting\")\n",
    "\n",
    "# 2) Preserve similarity scores\n",
    "for n in initial_nodes:\n",
    "    n.node.metadata[\"similarity_score\"] = n.score\n",
    "\n",
    "# 3) Rerank\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    top_n=3,\n",
    ")\n",
    "\n",
    "reranked_nodes = reranker.postprocess_nodes(initial_nodes, query_str=\"As shown in figure 2, the semantic chunking algorithm works by first splitting\")\n",
    "\n",
    "# 4) Print both scores\n",
    "for n in reranked_nodes:\n",
    "    print(f\"Rerank score: {n.score:.4f}\")\n",
    "    print(f\"Similarity score: {n.node.metadata['similarity_score']:.4f}\")\n",
    "    print(\"Text:\", n.text)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5259697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob=1.000  rerank_score=8.972\n",
      "prob=0.000  rerank_score=0.359\n",
      "prob=0.000  rerank_score=-0.131\n"
     ]
    }
   ],
   "source": [
    "# gives you relative relevance probabilities for a query\n",
    "\n",
    "\n",
    "scores = np.array([n.score for n in reranked_nodes])\n",
    "\n",
    "# temperature controls sharpness (lower = more confident)\n",
    "temperature = 1.0\n",
    "exp_scores = np.exp(scores / temperature)\n",
    "probs = exp_scores / exp_scores.sum()\n",
    "\n",
    "for n, p in zip(reranked_nodes, probs):\n",
    "    print(f\"prob={p:.3f}  rerank_score={n.score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fa9e1",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- https://developers.llamaindex.ai/python/framework/integrations/vector_stores/qdrant_hybrid/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
