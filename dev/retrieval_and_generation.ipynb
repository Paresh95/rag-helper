{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ac1af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.retrievers.fusion_retriever import QueryFusionRetriever\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2101a379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ea206",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5771f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_retriever = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    # Optional but recommended:\n",
    "    normalize=True,       # cosine similarity friendly\n",
    "    embed_batch_size=32,  # tune for your hardware\n",
    ")\n",
    "\n",
    "Settings.embed_model = dense_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee70c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "aclient = AsyncQdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    collection_name=\"my_collection\",\n",
    "    client=client,\n",
    "    aclient=aclient,\n",
    "    enable_hybrid=True,\n",
    "    batch_size=20,  # controls sparse batch processing\n",
    "    fastembed_sparse_model=\"prithivida/Splade_PP_en_v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ac3f5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prithivida/Splade_PP_en_v1\n",
      "prithvida/Splade_PP_en_v1\n",
      "Qdrant/bm42-all-minilm-l6-v2-attentions\n",
      "Qdrant/bm25\n",
      "Qdrant/minicoil-v1\n"
     ]
    }
   ],
   "source": [
    "from fastembed import SparseTextEmbedding\n",
    "for i in SparseTextEmbedding.list_supported_models():\n",
    "    print(i['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5963b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    top_n=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd896fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=dense_retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83c999b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "[NodeWithScore(node=TextNode(id_='6d735565-ae17-41e7-b949-ac0f0f5d761d', embedding=None, metadata={'uuid': 'ee95abd1-30ab-4f7e-be4c-3250076dcd69', 'file_path': 'data/processed/docling/2025.icnlsp-1.15.json', 'chunk_index': 0, 'section_path': ['4 Recursive Semantic Chunking'], 'section_refs': ['#/texts/35'], 'page_no': 3, 'tokens': 59, 'similarity_score': 0.7368821230294198}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='This section presents the Recursive Semantic Chunking framework in detail. The primary objective is to ensure the splitting of chunks is semantically coherent and maintains the integrity of the content. In addition, the size of the chunks should be optimal. The standard semantic chunking technique tends to generate large chunks, which', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7368821230294198), NodeWithScore(node=TextNode(id_='e368e613-851b-497b-bc19-2edf06ad89ae', embedding=None, metadata={'uuid': 'e21b0385-4a02-407a-bb12-1bd535572411', 'file_path': 'data/processed/docling/2025.icnlsp-1.15.json', 'chunk_index': 0, 'section_path': ['2 Related Work'], 'section_refs': ['#/texts/19'], 'page_no': 2, 'tokens': 105, 'similarity_score': 0.6032690290607552}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Although the recursive text split tends to keep the chunks semantically closed together, it does not directly account for semantic meaning. Conversely, semantic chunking (LangChain, 2024) groups the text that is semantically similar together. It first splits the text into sentences and groups them into three sentences, then merges similar groups in the embedding space. However, this technique does not ensure optimal chunk sizes. Since its mechanism is dependent on the similarity of the embedding vectors, it may lead to larger chunks and cause hallucinations.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6032690290607552), NodeWithScore(node=TextNode(id_='118b31c4-4b89-4d4d-a300-1627a5573154', embedding=None, metadata={'uuid': '0e2453e3-c73e-4b19-8409-8ebd37158677', 'file_path': 'data/processed/docling/2507.21110v1.json', 'chunk_index': 0, 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.3 Retrieval'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/89'], 'page_no': 6, 'tokens': 118, 'similarity_score': 0.5}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='As shown in figure 2, the semantic chunking algorithm works by first splitting the input text into individual sentences, then encoding each sentence into a vector using a pre-trained language model. It calculates the cosine similarity between each sentence and the current chunk to determine semantic closeness. If the similarity is high, the sentence is grouped with the current chunk; otherwise, a new chunk is started. This results in contextually meaningful groups of sentences. These chunks can then be used for tasks like entity extraction, summarization, and building knowledge graphs, enabling structured understanding of long, unstructured text.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.5), NodeWithScore(node=TextNode(id_='1ee78958-b546-4188-962a-31b8e6de3859', embedding=None, metadata={'uuid': '116557c4-8c53-4834-91e3-cd7f3f931047', 'file_path': 'data/processed/docling/2025.icnlsp-1.15.json', 'chunk_index': 0, 'section_path': ['4 Recursive Semantic Chunking'], 'section_refs': ['#/texts/35'], 'page_no': 3, 'tokens': 81, 'similarity_score': 0.45836383639349887}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Each segment t$_{j}$ undergoes an initial semantic chunking process (LangChain, 2024). In this step, the semantically similar texts are grouped in the embedding space, forming C$_{0}$ = { c$_{1}$, c$_{2}$, . . . , c$_{m}$ } , where c$_{k}$ represents an initial chunk.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.45836383639349887), NodeWithScore(node=TextNode(id_='e6218fbb-f344-4849-8973-45fc46cbaedf', embedding=None, metadata={'uuid': '44069895-f761-46e2-9ccb-f8a22872638e', 'file_path': 'data/processed/docling/2025.icnlsp-1.15.json', 'chunk_index': 0, 'section_path': ['4 Recursive Semantic Chunking'], 'section_refs': ['#/texts/35'], 'page_no': 3, 'tokens': 80, 'similarity_score': 0.4505402636286032}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='For each chunk c$_{k}$ ∈ C$_{0}$ , the semantic chunker is recursively applied if its length exceeds the threshold T$_{chunk}$ (1,500 characters). With each recursive iteration, the breakpoint threshold parameter is gradually reduced, ensuring that large chunks are broken into smaller, semantically meaningful segments. The recursive function R ( c, T ) operates as follows:', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.4505402636286032), NodeWithScore(node=TextNode(id_='78867a2e-a757-4068-a2b9-558a5295cceb', embedding=None, metadata={'uuid': '47be9844-f242-4091-8884-74b2ed4116d6', 'file_path': 'data/processed/docling/2025.icnlsp-1.15.json', 'chunk_index': 0, 'section_path': ['4 Recursive Semantic Chunking'], 'section_refs': ['#/texts/35'], 'page_no': 3, 'tokens': 30, 'similarity_score': 0.22491848100244152}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Algorithm 1 provides a detailed outline of the proposed chunking process. All predefined values are determined after extensive experimentation. The following steps describe the pipeline.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.22491848100244152), NodeWithScore(node=TextNode(id_='d278605f-3a39-424d-b5d6-62c95601e880', embedding=None, metadata={'uuid': 'bb19b056-3d7f-42b7-835b-f82138ee68cb', 'file_path': 'data/processed/docling/2025.icnlsp-1.15.json', 'chunk_index': 0, 'section_path': ['4 Recursive Semantic Chunking'], 'section_refs': ['#/texts/35'], 'page_no': 4, 'tokens': 55, 'similarity_score': 0.22314620536195717}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Finally, the algorithm checks whether any chunk exceeds the threshold T$_{final}$ (2,500 characters). If a chunk surpasses this limit, it undergoes a recursive character-based text split (LangChain, 2023). The final adjustment process is defined as:', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.22314620536195717)]\n"
     ]
    }
   ],
   "source": [
    "# 1) Retrieve (embedding similarity)\n",
    "base_retriever = index.as_retriever(similarity_top_k=10, sparse_top_k=10, vector_store_query_mode=\"hybrid\", hybrid_top_k=7)\n",
    "initial_nodes = base_retriever.retrieve(\"As shown in figure 2, the semantic chunking algorithm works by first splitting\")\n",
    "\n",
    "# 2) Preserve similarity scores\n",
    "for n in initial_nodes:\n",
    "    n.node.metadata[\"similarity_score\"] = n.score\n",
    "\n",
    "print(len(initial_nodes))\n",
    "print(initial_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "163da000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerank score: 7.7959\n",
      "Similarity score: 0.5000\n",
      "Text: As shown in figure 2, the semantic chunking algorithm works by first splitting the input text into individual sentences, then encoding each sentence into a vector using a pre-trained language model. It calculates the cosine similarity between each sentence and the current chunk to determine semantic closeness. If the similarity is high, the sentence is grouped with the current chunk; otherwise, a new chunk is started. This results in contextually meaningful groups of sentences. These chunks can then be used for tasks like entity extraction, summarization, and building knowledge graphs, enabling structured understanding of long, unstructured text.\n",
      "--------------------------------------------------\n",
      "Rerank score: 3.7247\n",
      "Similarity score: 0.6033\n",
      "Text: Although the recursive text split tends to keep the chunks semantically closed together, it does not directly account for semantic meaning. Conversely, semantic chunking (LangChain, 2024) groups the text that is semantically similar together. It first splits the text into sentences and groups them into three sentences, then merges similar groups in the embedding space. However, this technique does not ensure optimal chunk sizes. Since its mechanism is dependent on the similarity of the embedding vectors, it may lead to larger chunks and cause hallucinations.\n",
      "--------------------------------------------------\n",
      "Rerank score: 1.9209\n",
      "Similarity score: 0.7369\n",
      "Text: This section presents the Recursive Semantic Chunking framework in detail. The primary objective is to ensure the splitting of chunks is semantically coherent and maintains the integrity of the content. In addition, the size of the chunks should be optimal. The standard semantic chunking technique tends to generate large chunks, which\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3) Rerank\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    top_n=3,\n",
    ")\n",
    "\n",
    "reranked_nodes = reranker.postprocess_nodes(initial_nodes, query_str=\"As shown in figure 2, the semantic chunking algorithm works by first splitting\")\n",
    "\n",
    "# 4) Print both scores\n",
    "for n in reranked_nodes:\n",
    "    print(f\"Rerank score: {n.score:.4f}\")\n",
    "    print(f\"Similarity score: {n.node.metadata['similarity_score']:.4f}\")\n",
    "    print(\"Text:\", n.text)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5259697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob=1.000  rerank_score=8.972\n",
      "prob=0.000  rerank_score=0.359\n",
      "prob=0.000  rerank_score=-0.131\n"
     ]
    }
   ],
   "source": [
    "# gives you relative relevance probabilities for a query\n",
    "\n",
    "\n",
    "scores = np.array([n.score for n in reranked_nodes])\n",
    "\n",
    "# temperature controls sharpness (lower = more confident)\n",
    "temperature = 1.0\n",
    "exp_scores = np.exp(scores / temperature)\n",
    "probs = exp_scores / exp_scores.sum()\n",
    "\n",
    "for n, p in zip(reranked_nodes, probs):\n",
    "    print(f\"prob={p:.3f}  rerank_score={n.score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5637306",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4eb2a0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='59bbfa5d-3076-4b3a-834d-9bd9d0ad663a', embedding=None, metadata={'uuid': '0e2453e3-c73e-4b19-8409-8ebd37158677', 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.3 Retrieval'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/89'], 'page_no': 6, 'char_len': 654, 'similarity_score': 0.69592637}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e9ae230f-e254-458b-9b24-6ef3cc7de0af', node_type='4', metadata={'uuid': '0e2453e3-c73e-4b19-8409-8ebd37158677', 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.3 Retrieval'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/89'], 'page_no': 6, 'char_len': 654}, hash='1b5eae9341ef355ccd2be15a53a0b0250dcba53075986e63ce200055e934e34e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='As shown in figure 2, the semantic chunking algorithm works by first splitting the input text into individual sentences, then encoding each sentence into a vector using a pre-trained language model. It calculates the cosine similarity between each sentence and the current chunk to determine semantic closeness. If the similarity is high, the sentence is grouped with the current chunk; otherwise, a new chunk is started. This results in contextually meaningful groups of sentences. These chunks can then be used for tasks like entity extraction, summarization, and building knowledge graphs, enabling structured understanding of long, unstructured text.', mimetype='text/plain', start_char_idx=0, end_char_idx=654, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=np.float32(8.971758)),\n",
       " NodeWithScore(node=TextNode(id_='44a36ca6-ff49-4aab-b188-310510dc63e3', embedding=None, metadata={'uuid': '911caf17-0e7f-487b-93bd-c9b36fd2b0e0', 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.1 Semantic Chunking'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/42'], 'page_no': 4, 'char_len': 932, 'similarity_score': 0.60686845}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9d4166de-72fd-4d35-a36e-0fa0b3988cac', node_type='4', metadata={'uuid': '911caf17-0e7f-487b-93bd-c9b36fd2b0e0', 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.1 Semantic Chunking'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/42'], 'page_no': 4, 'char_len': 932}, hash='c4b13cb9784a779f60f4df675e68acda3a955ed246e96602b938d386c25621dc')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The Semantic chunking implementation focuses on incorporating a semantic chunking methodology into a lightweight Graph RAG framework, due to its adaptability and reduced computational requirements. Efforts to integrate this approach into a more resource-intensive platform proved challenging, prompting a shift to a simpler system better suited for customization. By applying cosine similarity for chunking with buffer size, a parameter that determines the number of adjacent sentences combined around a central sentence to preserve contextual coherence-this process ensures semantic integrity within chunks. Additionally, by relying on locally hosted models, the process efficiently handles domain-specific texts while minimizing overhead. By integrating the robust chunking process, streamlined embeddings, and consistent evaluations, this setup achieves a balance between accuracy, scalability, and efficient computational power.', mimetype='text/plain', start_char_idx=0, end_char_idx=932, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=np.float32(0.35915512)),\n",
       " NodeWithScore(node=TextNode(id_='f499e6a7-0971-4647-9a10-fa0dedcc95b5', embedding=None, metadata={'uuid': '997c7cef-b0b2-469f-9419-adf46e3e91f1', 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.2 Indexing'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/44'], 'page_no': 4, 'char_len': 282, 'similarity_score': 0.6621519}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='429dea3b-f643-4002-b6dc-9eb21bd598fa', node_type='4', metadata={'uuid': '997c7cef-b0b2-469f-9419-adf46e3e91f1', 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.2 Indexing'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/44'], 'page_no': 4, 'char_len': 282}, hash='f47867df196e5474ab7b201f0acafd4f25fb706d123f110e2c731fbcb2e91da8')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The semantic chunking process aims to enhance the contextual understanding and retrieval capabilities of large language models (LLMs). By dynamically grouping sentences based on semantic similarity, the method ensures cohesive and contextually rich chunks that respect token limits.', mimetype='text/plain', start_char_idx=0, end_char_idx=282, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=np.float32(-0.13059938))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e50900ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(nodes):\n",
    "    formatted = []\n",
    "    for i, node in enumerate(nodes, start=1):\n",
    "        formatted.append(\n",
    "            f\"\"\"Source {i}: {node.text}\"\"\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "07004d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chunk 1: As shown in figure 2, the semantic chunking algorithm works by first splitting the input text into individual sentences, then encoding each sentence into a vector using a pre-trained language model. It calculates the cosine similarity between each sentence and the current chunk to determine semantic closeness. If the similarity is high, the sentence is grouped with the current chunk; otherwise, a new chunk is started. This results in contextually meaningful groups of sentences. These chunks can then be used for tasks like entity extraction, summarization, and building knowledge graphs, enabling structured understanding of long, unstructured text.\\n\\nChunk 2: The Semantic chunking implementation focuses on incorporating a semantic chunking methodology into a lightweight Graph RAG framework, due to its adaptability and reduced computational requirements. Efforts to integrate this approach into a more resource-intensive platform proved challenging, prompting a shift to a simpler system better suited for customization. By applying cosine similarity for chunking with buffer size, a parameter that determines the number of adjacent sentences combined around a central sentence to preserve contextual coherence-this process ensures semantic integrity within chunks. Additionally, by relying on locally hosted models, the process efficiently handles domain-specific texts while minimizing overhead. By integrating the robust chunking process, streamlined embeddings, and consistent evaluations, this setup achieves a balance between accuracy, scalability, and efficient computational power.\\n\\nChunk 3: The semantic chunking process aims to enhance the contextual understanding and retrieval capabilities of large language models (LLMs). By dynamically grouping sentences based on semantic similarity, the method ensures cohesive and contextually rich chunks that respect token limits.'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_str = format_chunks(reranked_nodes)\n",
    "chunks_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "44a38f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a RAG-based assistant.\n",
    "Use ONLY the provided context to answer the question.\n",
    "If the answer is not contained in the context, say:\n",
    "\"I don’t know based on the provided information.\"\n",
    "Cite sources using [Source X] notation.\n",
    "Be concise and factual.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def generate_answer(\n",
    "    question: str,\n",
    "    retrieved_chunks: list[str],\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    user_prompt_template: str = USER_PROMPT_TEMPLATE,\n",
    ") -> str:\n",
    "    context = format_context(retrieved_chunks)\n",
    "\n",
    "    user_prompt = user_prompt_template.format(\n",
    "        context=context,\n",
    "        question=question,\n",
    "    )\n",
    "\n",
    "    response = completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2, # low temp for grounded answers\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e565eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What does figure 2 show?\"\n",
    "\n",
    "answer = generate_answer(query, reranked_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ac94bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 2 shows how the semantic chunking algorithm works by splitting input text into individual sentences, encoding each sentence into a vector using a pre-trained language model, and calculating cosine similarity to determine semantic closeness for grouping sentences into meaningful chunks. These chunks can be used for various tasks like entity extraction and summarization, enabling a structured understanding of long, unstructured text [Source 1].\n"
     ]
    }
   ],
   "source": [
    "print(answer.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fa4f43d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-D570HN9CFjYNvdLuTJipCwGCYOr3m', created=1770110933, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_6b23b3aa8a', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Figure 2 shows how the semantic chunking algorithm works, which involves splitting the input text into individual sentences, encoding each sentence into a vector using a pre-trained language model, and calculating the cosine similarity between each sentence and the current chunk to determine semantic closeness. It illustrates the process of grouping sentences into contextually meaningful chunks based on their semantic similarity. [Source 1]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=76, prompt_tokens=396, total_tokens=472, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4c9ad272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'request_time: 2026-02-03\\nuser_role: risk_analyst\\ntop_k: 3'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = {\n",
    "    \"request_time\": \"2026-02-03\",\n",
    "    \"user_role\": \"risk_analyst\",\n",
    "    \"top_k\": len(reranked_nodes)\n",
    "}\n",
    "\n",
    "metadata_str = \"\\n\".join(\n",
    "    f\"{k}: {v}\" for k, v in metadata.items()\n",
    ")\n",
    "\n",
    "metadata_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fa9e1",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- https://developers.llamaindex.ai/python/framework/integrations/vector_stores/qdrant_hybrid/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
