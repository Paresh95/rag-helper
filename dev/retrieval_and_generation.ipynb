{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79ac1af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, AsyncQdrantClient\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.retrievers.fusion_retriever import QueryFusionRetriever\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter\n",
    "\n",
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2101a379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ea206",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5771f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_retriever = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    # Optional but recommended:\n",
    "    normalize=True,       # cosine similarity friendly\n",
    "    embed_batch_size=32,  # tune for your hardware\n",
    ")\n",
    "\n",
    "Settings.embed_model = dense_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee70c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "aclient = AsyncQdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    collection_name=\"my_collection\",\n",
    "    client=client,\n",
    "    aclient=aclient,\n",
    "    enable_hybrid=True,\n",
    "    batch_size=20,  # controls sparse batch processing\n",
    "    fastembed_sparse_model=\"prithivida/Splade_PP_en_v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac3f5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prithivida/Splade_PP_en_v1\n",
      "prithvida/Splade_PP_en_v1\n",
      "Qdrant/bm42-all-minilm-l6-v2-attentions\n",
      "Qdrant/bm25\n",
      "Qdrant/minicoil-v1\n"
     ]
    }
   ],
   "source": [
    "from fastembed import SparseTextEmbedding\n",
    "for i in SparseTextEmbedding.list_supported_models():\n",
    "    print(i['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5963b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    top_n=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd896fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=dense_retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83c999b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[NodeWithScore(node=TextNode(id_='118b31c4-4b89-4d4d-a300-1627a5573154', embedding=None, metadata={'uuid': '0e2453e3-c73e-4b19-8409-8ebd37158677', 'file_path': 'data/processed/docling/2507.21110v1.json', 'chunk_index': 0, 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.3 Retrieval'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/89'], 'page_no': 6, 'tokens': 118, 'similarity_score': 0.5}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='As shown in figure 2, the semantic chunking algorithm works by first splitting the input text into individual sentences, then encoding each sentence into a vector using a pre-trained language model. It calculates the cosine similarity between each sentence and the current chunk to determine semantic closeness. If the similarity is high, the sentence is grouped with the current chunk; otherwise, a new chunk is started. This results in contextually meaningful groups of sentences. These chunks can then be used for tasks like entity extraction, summarization, and building knowledge graphs, enabling structured understanding of long, unstructured text.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.5), NodeWithScore(node=TextNode(id_='4ac80233-699a-4018-918a-1a1a6e52dd5c', embedding=None, metadata={'uuid': '5e7c1747-3efc-4414-9256-541e78fe3672', 'file_path': 'data/processed/docling/2507.21110v1.json', 'chunk_index': 0, 'section_path': ['1 Introduction'], 'section_refs': ['#/texts/11'], 'page_no': 2, 'tokens': 74, 'similarity_score': 0.5}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Figure 1: SemRAG framework leveraging semantic chunking and knowledge graphs for enhanced contextual understanding; (a) Semantic Indexing: Segment the document into smaller chunks indexed and stored in a database. (b) Context Retrieval: Retrieve the top k community based on semantic similarity. (c) Knowledge Graph: Information is extracted to build the knowledge graph hierarchy with communities.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.5), NodeWithScore(node=TextNode(id_='d11b8e4e-6b66-4eab-b963-37c5692b6aef', embedding=None, metadata={'uuid': '2bcb2c07-1cfe-47db-9914-2814cc97cbb2', 'file_path': 'data/processed/docling/2507.21110v1.json', 'chunk_index': 0, 'section_path': ['6 Future Work'], 'section_refs': ['#/texts/152'], 'page_no': 12, 'tokens': 43, 'similarity_score': 0.3319941538922412}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Another key direction is developing a ground-truth metric for evaluating chunk boundaries, enabling more precise and cohesive data segmentation. This would improve semantic chunking by minimizing noise and optimizing chunk size for higher answer relevance and correctness.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.3319941538922412), NodeWithScore(node=TextNode(id_='4aa5f5c5-dd9c-4e57-8e83-4a40d380fa62', embedding=None, metadata={'uuid': '997c7cef-b0b2-469f-9419-adf46e3e91f1', 'file_path': 'data/processed/docling/2507.21110v1.json', 'chunk_index': 0, 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.2 Indexing'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/44'], 'page_no': 4, 'tokens': 45, 'similarity_score': 0.26846686926227836}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The semantic chunking process aims to enhance the contextual understanding and retrieval capabilities of large language models (LLMs). By dynamically grouping sentences based on semantic similarity, the method ensures cohesive and contextually rich chunks that respect token limits.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.26846686926227836), NodeWithScore(node=TextNode(id_='4367bfcc-cdda-4edb-a2fb-34b540c7c150', embedding=None, metadata={'uuid': 'd03c2567-b83e-411a-aa4a-b8d7050f1f9d', 'file_path': 'data/processed/docling/2507.21110v1.json', 'chunk_index': 0, 'section_path': ['1 Introduction'], 'section_refs': ['#/texts/11'], 'page_no': 2, 'tokens': 164, 'similarity_score': 0.2514248719998007}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"Semantic chunking has emerged as a key trend for improving contextual performance in RAG pipelines. Research, including a study by Vectara, highlights the importance of optimizing chunking size during the indexing process to enhance RAG's effectiveness [5]. Semantic chunking stands out for its ability to maintain context and coherence, although it requires some computational resources. In contrast, methods like NLTK [6] and Spacy may fall short in accurately identifying sentence boundaries as they focus more on sophisticated rule base chunking that might be syntactically correct but semantically disjoint, on the other hand rule base chunking such as recursive chunking often struggles to preserve context and lead to fragmented and less meaningful chunks. Hence striking a balance between context preservation, semantic consistency, and computational efficiency is crucial for effective chunking strategies.\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.2514248719998007)]\n"
     ]
    }
   ],
   "source": [
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        ExactMatchFilter(key=\"file_path\", value=\"data/processed/docling/2507.21110v1.json\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# 1) Retrieve (embedding similarity)\n",
    "base_retriever = index.as_retriever(similarity_top_k=10, sparse_top_k=10, vector_store_query_mode=\"hybrid\", hybrid_top_k=7, filters=filters)\n",
    "initial_nodes = base_retriever.retrieve(\"As shown in figure 2, the semantic chunking algorithm works by first splitting\")\n",
    "\n",
    "# 2) Preserve similarity scores\n",
    "for n in initial_nodes:\n",
    "    n.node.metadata[\"similarity_score\"] = n.score\n",
    "\n",
    "print(len(initial_nodes))\n",
    "print(initial_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163da000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerank score: 7.7959\n",
      "Similarity score: 0.5000\n",
      "Text: As shown in figure 2, the semantic chunking algorithm works by first splitting the input text into individual sentences, then encoding each sentence into a vector using a pre-trained language model. It calculates the cosine similarity between each sentence and the current chunk to determine semantic closeness. If the similarity is high, the sentence is grouped with the current chunk; otherwise, a new chunk is started. This results in contextually meaningful groups of sentences. These chunks can then be used for tasks like entity extraction, summarization, and building knowledge graphs, enabling structured understanding of long, unstructured text.\n",
      "Metadata: {'uuid': '0e2453e3-c73e-4b19-8409-8ebd37158677', 'file_path': 'data/processed/docling/2507.21110v1.json', 'chunk_index': 0, 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.3 Retrieval'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/89'], 'page_no': 6, 'tokens': 118, 'similarity_score': 0.5}\n",
      "--------------------------------------------------\n",
      "Rerank score: -0.3935\n",
      "Similarity score: 0.2685\n",
      "Text: The semantic chunking process aims to enhance the contextual understanding and retrieval capabilities of large language models (LLMs). By dynamically grouping sentences based on semantic similarity, the method ensures cohesive and contextually rich chunks that respect token limits.\n",
      "Metadata: {'uuid': '997c7cef-b0b2-469f-9419-adf46e3e91f1', 'file_path': 'data/processed/docling/2507.21110v1.json', 'chunk_index': 0, 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.2 Indexing'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/44'], 'page_no': 4, 'tokens': 45, 'similarity_score': 0.26846686926227836}\n",
      "--------------------------------------------------\n",
      "Rerank score: -2.0121\n",
      "Similarity score: 0.2514\n",
      "Text: Semantic chunking has emerged as a key trend for improving contextual performance in RAG pipelines. Research, including a study by Vectara, highlights the importance of optimizing chunking size during the indexing process to enhance RAG's effectiveness [5]. Semantic chunking stands out for its ability to maintain context and coherence, although it requires some computational resources. In contrast, methods like NLTK [6] and Spacy may fall short in accurately identifying sentence boundaries as they focus more on sophisticated rule base chunking that might be syntactically correct but semantically disjoint, on the other hand rule base chunking such as recursive chunking often struggles to preserve context and lead to fragmented and less meaningful chunks. Hence striking a balance between context preservation, semantic consistency, and computational efficiency is crucial for effective chunking strategies.\n",
      "Metadata: {'uuid': 'd03c2567-b83e-411a-aa4a-b8d7050f1f9d', 'file_path': 'data/processed/docling/2507.21110v1.json', 'chunk_index': 0, 'section_path': ['1 Introduction'], 'section_refs': ['#/texts/11'], 'page_no': 2, 'tokens': 164, 'similarity_score': 0.2514248719998007}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3) Rerank\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    top_n=3,\n",
    ")\n",
    "\n",
    "reranked_nodes = reranker.postprocess_nodes(initial_nodes, query_str=\"As shown in figure 2, the semantic chunking algorithm works by first splitting\")\n",
    "\n",
    "# 4) Print both scores\n",
    "for n in reranked_nodes:\n",
    "    print(f\"Rerank score: {n.score:.4f}\")\n",
    "    print(f\"Similarity score: {n.node.metadata['similarity_score']:.4f}\")\n",
    "    print(\"Text:\", n.text)\n",
    "    print(\"Metadata:\", n.node.metadata)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e5259697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob=1.000  rerank_score=8.972\n",
      "prob=0.000  rerank_score=0.359\n",
      "prob=0.000  rerank_score=-0.131\n"
     ]
    }
   ],
   "source": [
    "# gives you relative relevance probabilities for a query\n",
    "\n",
    "\n",
    "scores = np.array([n.score for n in reranked_nodes])\n",
    "\n",
    "# temperature controls sharpness (lower = more confident)\n",
    "temperature = 1.0\n",
    "exp_scores = np.exp(scores / temperature)\n",
    "probs = exp_scores / exp_scores.sum()\n",
    "\n",
    "for n, p in zip(reranked_nodes, probs):\n",
    "    print(f\"prob={p:.3f}  rerank_score={n.score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5637306",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4eb2a0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='59bbfa5d-3076-4b3a-834d-9bd9d0ad663a', embedding=None, metadata={'uuid': '0e2453e3-c73e-4b19-8409-8ebd37158677', 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.3 Retrieval'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/89'], 'page_no': 6, 'char_len': 654, 'similarity_score': 0.69592637}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e9ae230f-e254-458b-9b24-6ef3cc7de0af', node_type='4', metadata={'uuid': '0e2453e3-c73e-4b19-8409-8ebd37158677', 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.3 Retrieval'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/89'], 'page_no': 6, 'char_len': 654}, hash='1b5eae9341ef355ccd2be15a53a0b0250dcba53075986e63ce200055e934e34e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='As shown in figure 2, the semantic chunking algorithm works by first splitting the input text into individual sentences, then encoding each sentence into a vector using a pre-trained language model. It calculates the cosine similarity between each sentence and the current chunk to determine semantic closeness. If the similarity is high, the sentence is grouped with the current chunk; otherwise, a new chunk is started. This results in contextually meaningful groups of sentences. These chunks can then be used for tasks like entity extraction, summarization, and building knowledge graphs, enabling structured understanding of long, unstructured text.', mimetype='text/plain', start_char_idx=0, end_char_idx=654, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=np.float32(8.971758)),\n",
       " NodeWithScore(node=TextNode(id_='44a36ca6-ff49-4aab-b188-310510dc63e3', embedding=None, metadata={'uuid': '911caf17-0e7f-487b-93bd-c9b36fd2b0e0', 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.1 Semantic Chunking'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/42'], 'page_no': 4, 'char_len': 932, 'similarity_score': 0.60686845}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9d4166de-72fd-4d35-a36e-0fa0b3988cac', node_type='4', metadata={'uuid': '911caf17-0e7f-487b-93bd-c9b36fd2b0e0', 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.1 Semantic Chunking'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/42'], 'page_no': 4, 'char_len': 932}, hash='c4b13cb9784a779f60f4df675e68acda3a955ed246e96602b938d386c25621dc')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The Semantic chunking implementation focuses on incorporating a semantic chunking methodology into a lightweight Graph RAG framework, due to its adaptability and reduced computational requirements. Efforts to integrate this approach into a more resource-intensive platform proved challenging, prompting a shift to a simpler system better suited for customization. By applying cosine similarity for chunking with buffer size, a parameter that determines the number of adjacent sentences combined around a central sentence to preserve contextual coherence-this process ensures semantic integrity within chunks. Additionally, by relying on locally hosted models, the process efficiently handles domain-specific texts while minimizing overhead. By integrating the robust chunking process, streamlined embeddings, and consistent evaluations, this setup achieves a balance between accuracy, scalability, and efficient computational power.', mimetype='text/plain', start_char_idx=0, end_char_idx=932, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=np.float32(0.35915512)),\n",
       " NodeWithScore(node=TextNode(id_='f499e6a7-0971-4647-9a10-fa0dedcc95b5', embedding=None, metadata={'uuid': '997c7cef-b0b2-469f-9419-adf46e3e91f1', 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.2 Indexing'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/44'], 'page_no': 4, 'char_len': 282, 'similarity_score': 0.6621519}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='429dea3b-f643-4002-b6dc-9eb21bd598fa', node_type='4', metadata={'uuid': '997c7cef-b0b2-469f-9419-adf46e3e91f1', 'section_path': ['3 Methodology', '3.2 SemRAG', '3.2.2 Indexing'], 'section_refs': ['#/texts/37', '#/texts/40', '#/texts/44'], 'page_no': 4, 'char_len': 282}, hash='f47867df196e5474ab7b201f0acafd4f25fb706d123f110e2c731fbcb2e91da8')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The semantic chunking process aims to enhance the contextual understanding and retrieval capabilities of large language models (LLMs). By dynamically grouping sentences based on semantic similarity, the method ensures cohesive and contextually rich chunks that respect token limits.', mimetype='text/plain', start_char_idx=0, end_char_idx=282, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=np.float32(-0.13059938))]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e50900ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context(nodes):\n",
    "    formatted = []\n",
    "    for i, node in enumerate(nodes, start=1):\n",
    "        formatted.append(\n",
    "            f\"\"\"Source {i}: {node.text}\"\"\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "07004d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chunk 1: As shown in figure 2, the semantic chunking algorithm works by first splitting the input text into individual sentences, then encoding each sentence into a vector using a pre-trained language model. It calculates the cosine similarity between each sentence and the current chunk to determine semantic closeness. If the similarity is high, the sentence is grouped with the current chunk; otherwise, a new chunk is started. This results in contextually meaningful groups of sentences. These chunks can then be used for tasks like entity extraction, summarization, and building knowledge graphs, enabling structured understanding of long, unstructured text.\\n\\nChunk 2: The Semantic chunking implementation focuses on incorporating a semantic chunking methodology into a lightweight Graph RAG framework, due to its adaptability and reduced computational requirements. Efforts to integrate this approach into a more resource-intensive platform proved challenging, prompting a shift to a simpler system better suited for customization. By applying cosine similarity for chunking with buffer size, a parameter that determines the number of adjacent sentences combined around a central sentence to preserve contextual coherence-this process ensures semantic integrity within chunks. Additionally, by relying on locally hosted models, the process efficiently handles domain-specific texts while minimizing overhead. By integrating the robust chunking process, streamlined embeddings, and consistent evaluations, this setup achieves a balance between accuracy, scalability, and efficient computational power.\\n\\nChunk 3: The semantic chunking process aims to enhance the contextual understanding and retrieval capabilities of large language models (LLMs). By dynamically grouping sentences based on semantic similarity, the method ensures cohesive and contextually rich chunks that respect token limits.'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_str = format_chunks(reranked_nodes)\n",
    "chunks_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "44a38f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a RAG-based assistant.\n",
    "Use ONLY the provided context to answer the question.\n",
    "If the answer is not contained in the context, say:\n",
    "\"I donâ€™t know based on the provided information.\"\n",
    "Cite sources using [Source X] notation.\n",
    "Be concise and factual.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def generate_answer(\n",
    "    question: str,\n",
    "    retrieved_chunks: list[str],\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    user_prompt_template: str = USER_PROMPT_TEMPLATE,\n",
    ") -> str:\n",
    "    context = format_context(retrieved_chunks)\n",
    "\n",
    "    user_prompt = user_prompt_template.format(\n",
    "        context=context,\n",
    "        question=question,\n",
    "    )\n",
    "\n",
    "    response = completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2, # low temp for grounded answers\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e565eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What does figure 2 show?\"\n",
    "\n",
    "answer = generate_answer(query, reranked_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ac94bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 2 shows how the semantic chunking algorithm works by splitting input text into individual sentences, encoding each sentence into a vector using a pre-trained language model, and calculating cosine similarity to determine semantic closeness for grouping sentences into meaningful chunks. These chunks can be used for various tasks like entity extraction and summarization, enabling a structured understanding of long, unstructured text [Source 1].\n"
     ]
    }
   ],
   "source": [
    "print(answer.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fa4f43d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-D570HN9CFjYNvdLuTJipCwGCYOr3m', created=1770110933, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_6b23b3aa8a', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Figure 2 shows how the semantic chunking algorithm works, which involves splitting the input text into individual sentences, encoding each sentence into a vector using a pre-trained language model, and calculating the cosine similarity between each sentence and the current chunk to determine semantic closeness. It illustrates the process of grouping sentences into contextually meaningful chunks based on their semantic similarity. [Source 1]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=76, prompt_tokens=396, total_tokens=472, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4c9ad272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'request_time: 2026-02-03\\nuser_role: risk_analyst\\ntop_k: 3'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = {\n",
    "    \"request_time\": \"2026-02-03\",\n",
    "    \"user_role\": \"risk_analyst\",\n",
    "    \"top_k\": len(reranked_nodes)\n",
    "}\n",
    "\n",
    "metadata_str = \"\\n\".join(\n",
    "    f\"{k}: {v}\" for k, v in metadata.items()\n",
    ")\n",
    "\n",
    "metadata_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fa9e1",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- https://developers.llamaindex.ai/python/framework/integrations/vector_stores/qdrant_hybrid/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
